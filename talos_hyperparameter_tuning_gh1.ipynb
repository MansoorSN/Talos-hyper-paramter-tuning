{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "talos_hyperparameter_tuning_gh1.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "wzDat4V3eHRI",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 488
        },
        "outputId": "8515c1d3-203f-4675-c30f-ddb2ae36a0a3"
      },
      "source": [
        "#importing the required libraries\n",
        "\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "housing=fetch_california_housing()\n",
        "import keras\n",
        "import numpy as np"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading Cal. housing from https://ndownloader.figshare.com/files/5976036 to /root/scikit_learn_data\n",
            "Using TensorFlow backend.\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "MaihW05vinfP",
        "outputId": "9bff1c32-8a8b-4225-bf97-cd325cfe4a20",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "#installing the talos package\n",
        "!pip install talos"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting talos\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/73/64/65bee9b585f6a196ff3119a60a7170fe0b2a18225a2e1e67115510b46a65/talos-0.6.6-py3-none-any.whl (53kB)\n",
            "\r\u001b[K     |██████▏                         | 10kB 26.5MB/s eta 0:00:01\r\u001b[K     |████████████▎                   | 20kB 1.7MB/s eta 0:00:01\r\u001b[K     |██████████████████▍             | 30kB 2.2MB/s eta 0:00:01\r\u001b[K     |████████████████████████▌       | 40kB 2.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▋ | 51kB 2.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 61kB 2.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from talos) (4.41.1)\n",
            "Collecting wrangle\n",
            "  Downloading https://files.pythonhosted.org/packages/85/35/bc729e377417613f2d062a890faea5d649ef1a554df21499e9c3a4a5501a/wrangle-0.6.7.tar.gz\n",
            "Collecting keras==2.3.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1b/18/2e1ef121e5560ac24c7ac9e363aa5fa7006c40563c989e7211aba95b793a/Keras-2.3.0-py2.py3-none-any.whl (377kB)\n",
            "\u001b[K     |████████████████████████████████| 378kB 6.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from talos) (1.0.4)\n",
            "Collecting statsmodels>=0.11.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/cb/83/540fd83238a18abe6c2d280fa8e489ac5fcefa1f370f0ca1acd16ae1b860/statsmodels-0.11.1-cp36-cp36m-manylinux1_x86_64.whl (8.7MB)\n",
            "\u001b[K     |████████████████████████████████| 8.7MB 17.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: sklearn in /usr/local/lib/python3.6/dist-packages (from talos) (0.0)\n",
            "Collecting chances\n",
            "  Downloading https://files.pythonhosted.org/packages/fa/d8/d61112d7476dc3074b855f1edd8556cde9b49b7106853f0b060109dd4c82/chances-0.1.9.tar.gz\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from talos) (1.18.5)\n",
            "Collecting kerasplotlib\n",
            "  Downloading https://files.pythonhosted.org/packages/7b/b7/31663d3b5ea9afd8c2c6ffa06d3c4e118ef363e12dc75b7c49fb6a2d22aa/kerasplotlib-0.1.6.tar.gz\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from talos) (2.23.0)\n",
            "Collecting astetik\n",
            "  Downloading https://files.pythonhosted.org/packages/3c/ba/f8622951da73d9b47b45bb847112c388651f9c6e413e712954f260301d9f/astetik-1.9.9.tar.gz\n",
            "Collecting tensorflow==1.14.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/de/f0/96fb2e0412ae9692dbf400e5b04432885f677ad6241c088ccc5fe7724d69/tensorflow-1.14.0-cp36-cp36m-manylinux1_x86_64.whl (109.2MB)\n",
            "\u001b[K     |████████████████████████████████| 109.2MB 39kB/s \n",
            "\u001b[?25hCollecting scipy==1.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/67/e6/6d4edaceee6a110ecf6f318482f5229792f143e468b34a631f5a0899f56d/scipy-1.2.0-cp36-cp36m-manylinux1_x86_64.whl (26.6MB)\n",
            "\u001b[K     |████████████████████████████████| 26.6MB 116kB/s \n",
            "\u001b[?25hRequirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from keras==2.3.0->talos) (1.0.8)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras==2.3.0->talos) (2.10.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras==2.3.0->talos) (3.13)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from keras==2.3.0->talos) (1.1.2)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from keras==2.3.0->talos) (1.12.0)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas->talos) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from pandas->talos) (2.8.1)\n",
            "Requirement already satisfied: patsy>=0.5 in /usr/local/lib/python3.6/dist-packages (from statsmodels>=0.11.0->talos) (0.5.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from sklearn->talos) (0.22.2.post1)\n",
            "Requirement already satisfied: ipython in /usr/local/lib/python3.6/dist-packages (from kerasplotlib->talos) (5.5.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->talos) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->talos) (2020.4.5.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->talos) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->talos) (2.9)\n",
            "Collecting geonamescache\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/32/c1/efb823270c8526b2f4f3eb8c804c5a0a55277267ad2312f5eb47bd9cc370/geonamescache-1.1.0-py3-none-any.whl (830kB)\n",
            "\u001b[K     |████████████████████████████████| 839kB 53.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14.0->talos) (1.29.0)\n",
            "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14.0->talos) (0.3.3)\n",
            "Collecting tensorboard<1.15.0,>=1.14.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/91/2d/2ed263449a078cd9c8a9ba50ebd50123adf1f8cfbea1492f9084169b89d9/tensorboard-1.14.0-py3-none-any.whl (3.1MB)\n",
            "\u001b[K     |████████████████████████████████| 3.2MB 51.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14.0->talos) (1.12.1)\n",
            "Collecting tensorflow-estimator<1.15.0rc0,>=1.14.0rc0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3c/d5/21860a5b11caf0678fbc8319341b0ae21a07156911132e0e71bffed0510d/tensorflow_estimator-1.14.0-py2.py3-none-any.whl (488kB)\n",
            "\u001b[K     |████████████████████████████████| 491kB 51.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14.0->talos) (1.1.0)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14.0->talos) (0.9.0)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14.0->talos) (0.8.1)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14.0->talos) (0.2.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14.0->talos) (0.34.2)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14.0->talos) (3.10.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->sklearn->talos) (0.15.1)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.6/dist-packages (from ipython->kerasplotlib->talos) (47.1.1)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.6/dist-packages (from ipython->kerasplotlib->talos) (2.1.3)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.6/dist-packages (from ipython->kerasplotlib->talos) (4.3.3)\n",
            "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.6/dist-packages (from ipython->kerasplotlib->talos) (0.8.1)\n",
            "Requirement already satisfied: pexpect; sys_platform != \"win32\" in /usr/local/lib/python3.6/dist-packages (from ipython->kerasplotlib->talos) (4.8.0)\n",
            "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.6/dist-packages (from ipython->kerasplotlib->talos) (1.0.18)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.6/dist-packages (from ipython->kerasplotlib->talos) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.6/dist-packages (from ipython->kerasplotlib->talos) (0.7.5)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow==1.14.0->talos) (1.0.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow==1.14.0->talos) (3.2.2)\n",
            "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.6/dist-packages (from traitlets>=4.2->ipython->kerasplotlib->talos) (0.2.0)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.6/dist-packages (from pexpect; sys_platform != \"win32\"->ipython->kerasplotlib->talos) (0.6.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.6/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython->kerasplotlib->talos) (0.2.3)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard<1.15.0,>=1.14.0->tensorflow==1.14.0->talos) (1.6.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.15.0,>=1.14.0->tensorflow==1.14.0->talos) (3.1.0)\n",
            "Building wheels for collected packages: wrangle, chances, kerasplotlib, astetik\n",
            "  Building wheel for wrangle (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wrangle: filename=wrangle-0.6.7-cp36-none-any.whl size=49894 sha256=df9fe52155c1512152ffae4a781c304b3e6d750760e5092ce752dbe47d0ed7cc\n",
            "  Stored in directory: /root/.cache/pip/wheels/bf/1b/50/d0403ce6ef269e364894da7b50db68db14c4ac62c577561e2d\n",
            "  Building wheel for chances (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for chances: filename=chances-0.1.9-cp36-none-any.whl size=41609 sha256=af1cef3d7baffa06e05070a3c6afecc7a4c65d9dddcdee08c1be0f3b1a16b3e4\n",
            "  Stored in directory: /root/.cache/pip/wheels/75/33/46/c871b94249bd57d17797d049b3dff8e3a09c315afb67eb14c6\n",
            "  Building wheel for kerasplotlib (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for kerasplotlib: filename=kerasplotlib-0.1.6-cp36-none-any.whl size=3601 sha256=bc3c3370711fbc8bbca40a74758cb3b61b65622b10358ee672cef9ea106f7760\n",
            "  Stored in directory: /root/.cache/pip/wheels/9d/d3/8c/9503a22b0a38e8b21c70ad834e4606d209193443e5c709305d\n",
            "  Building wheel for astetik (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for astetik: filename=astetik-1.9.9-cp36-none-any.whl size=56960 sha256=b28f7bf61a39c75da297ba99ba14685add26689ba92c7b54337c70f09e0b468d\n",
            "  Stored in directory: /root/.cache/pip/wheels/ae/70/21/c475cd079ec401dd6e1b9b1d42b4c38554ce12679bfb214aad\n",
            "Successfully built wrangle chances kerasplotlib astetik\n",
            "\u001b[31mERROR: umap-learn 0.4.4 has requirement scipy>=1.3.1, but you'll have scipy 1.2.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: albumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Installing collected packages: scipy, statsmodels, keras, wrangle, chances, kerasplotlib, geonamescache, astetik, tensorboard, tensorflow-estimator, tensorflow, talos\n",
            "  Found existing installation: scipy 1.4.1\n",
            "    Uninstalling scipy-1.4.1:\n",
            "      Successfully uninstalled scipy-1.4.1\n",
            "  Found existing installation: statsmodels 0.10.2\n",
            "    Uninstalling statsmodels-0.10.2:\n",
            "      Successfully uninstalled statsmodels-0.10.2\n",
            "  Found existing installation: Keras 2.3.1\n",
            "    Uninstalling Keras-2.3.1:\n",
            "      Successfully uninstalled Keras-2.3.1\n",
            "  Found existing installation: tensorboard 2.2.2\n",
            "    Uninstalling tensorboard-2.2.2:\n",
            "      Successfully uninstalled tensorboard-2.2.2\n",
            "  Found existing installation: tensorflow-estimator 2.2.0\n",
            "    Uninstalling tensorflow-estimator-2.2.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.2.0\n",
            "  Found existing installation: tensorflow 2.2.0\n",
            "    Uninstalling tensorflow-2.2.0:\n",
            "      Successfully uninstalled tensorflow-2.2.0\n",
            "Successfully installed astetik-1.9.9 chances-0.1.9 geonamescache-1.1.0 keras-2.3.0 kerasplotlib-0.1.6 scipy-1.2.0 statsmodels-0.11.1 talos-0.6.6 tensorboard-1.14.0 tensorflow-1.14.0 tensorflow-estimator-1.14.0 wrangle-0.6.7\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "9WQFxdxleHRL",
        "colab": {}
      },
      "source": [
        "#data preprocesssing\n",
        "\n",
        "\n",
        "x_train_full,x_test, y_train_full,y_test=train_test_split(housing.data, housing.target)\n",
        "x_train,x_valid,y_train,y_valid=train_test_split(x_train_full,y_train_full)\n",
        "\n",
        "scaler=StandardScaler()\n",
        "\n",
        "x_train=scaler.fit_transform(x_train)\n",
        "x_valid=scaler.transform(x_valid)\n",
        "x_test=scaler.transform(x_test)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "rzgoZcM5eHRP",
        "colab": {}
      },
      "source": [
        "#creating a neural network with 2 hidden  layers\n",
        "model=keras.models.Sequential()\n",
        "model.add(keras.layers.Dense(30,activation='relu', input_shape=x_train.shape[1:]))\n",
        "model.add(keras.layers.Dense(15,activation='relu'))\n",
        "model.add(keras.layers.Dense(1))\n",
        "\n",
        "model.compile(loss='mean_squared_error', optimizer='sgd')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "OBRWWobPeHRV",
        "outputId": "369ba3cf-6f07-4568-e055-abd0c8ca512e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "#training the model\n",
        "history=model.fit(x_train,y_train, epochs=40, validation_data=(x_valid,y_valid))\n",
        "mse_test=model.evaluate(x_test,y_test)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "Train on 11610 samples, validate on 3870 samples\n",
            "Epoch 1/40\n",
            "11610/11610 [==============================] - 0s 41us/step - loss: 0.7992 - val_loss: 0.5191\n",
            "Epoch 2/40\n",
            "11610/11610 [==============================] - 0s 34us/step - loss: 1.2757 - val_loss: 0.5238\n",
            "Epoch 3/40\n",
            "11610/11610 [==============================] - 0s 34us/step - loss: 0.5084 - val_loss: 0.4564\n",
            "Epoch 4/40\n",
            "11610/11610 [==============================] - 0s 33us/step - loss: 0.4573 - val_loss: 0.4211\n",
            "Epoch 5/40\n",
            "11610/11610 [==============================] - 0s 35us/step - loss: 0.4342 - val_loss: 0.4092\n",
            "Epoch 6/40\n",
            "11610/11610 [==============================] - 0s 34us/step - loss: 0.4202 - val_loss: 0.4080\n",
            "Epoch 7/40\n",
            "11610/11610 [==============================] - 0s 34us/step - loss: 0.4101 - val_loss: 0.4030\n",
            "Epoch 8/40\n",
            "11610/11610 [==============================] - 0s 35us/step - loss: 0.4031 - val_loss: 0.3846\n",
            "Epoch 9/40\n",
            "11610/11610 [==============================] - 0s 34us/step - loss: 0.3954 - val_loss: 0.3738\n",
            "Epoch 10/40\n",
            "11610/11610 [==============================] - 0s 32us/step - loss: 0.3881 - val_loss: 0.3823\n",
            "Epoch 11/40\n",
            "11610/11610 [==============================] - 0s 33us/step - loss: 0.3823 - val_loss: 0.3675\n",
            "Epoch 12/40\n",
            "11610/11610 [==============================] - 0s 32us/step - loss: 0.3785 - val_loss: 0.3630\n",
            "Epoch 13/40\n",
            "11610/11610 [==============================] - 0s 32us/step - loss: 0.3742 - val_loss: 0.3593\n",
            "Epoch 14/40\n",
            "11610/11610 [==============================] - 0s 31us/step - loss: 0.3705 - val_loss: 0.3713\n",
            "Epoch 15/40\n",
            "11610/11610 [==============================] - 0s 31us/step - loss: 0.3682 - val_loss: 0.3590\n",
            "Epoch 16/40\n",
            "11610/11610 [==============================] - 0s 32us/step - loss: 0.3654 - val_loss: 0.3576\n",
            "Epoch 17/40\n",
            "11610/11610 [==============================] - 0s 31us/step - loss: 0.3623 - val_loss: 0.3614\n",
            "Epoch 18/40\n",
            "11610/11610 [==============================] - 0s 33us/step - loss: 0.3593 - val_loss: 0.3518\n",
            "Epoch 19/40\n",
            "11610/11610 [==============================] - 0s 31us/step - loss: 0.3565 - val_loss: 0.3569\n",
            "Epoch 20/40\n",
            "11610/11610 [==============================] - 0s 31us/step - loss: 0.3547 - val_loss: 0.3449\n",
            "Epoch 21/40\n",
            "11610/11610 [==============================] - 0s 33us/step - loss: 0.3534 - val_loss: 0.3477\n",
            "Epoch 22/40\n",
            "11610/11610 [==============================] - 0s 31us/step - loss: 0.3510 - val_loss: 0.3425\n",
            "Epoch 23/40\n",
            "11610/11610 [==============================] - 0s 33us/step - loss: 0.3494 - val_loss: 0.3405\n",
            "Epoch 24/40\n",
            "11610/11610 [==============================] - 0s 32us/step - loss: 0.3477 - val_loss: 0.3578\n",
            "Epoch 25/40\n",
            "11610/11610 [==============================] - 0s 31us/step - loss: 0.3466 - val_loss: 0.3404\n",
            "Epoch 26/40\n",
            "11610/11610 [==============================] - 0s 31us/step - loss: 0.3427 - val_loss: 0.3381\n",
            "Epoch 27/40\n",
            "11610/11610 [==============================] - 0s 32us/step - loss: 0.3421 - val_loss: 0.3382\n",
            "Epoch 28/40\n",
            "11610/11610 [==============================] - 0s 31us/step - loss: 0.3394 - val_loss: 0.3426\n",
            "Epoch 29/40\n",
            "11610/11610 [==============================] - 0s 32us/step - loss: 0.3368 - val_loss: 0.3311\n",
            "Epoch 30/40\n",
            "11610/11610 [==============================] - 0s 31us/step - loss: 0.3351 - val_loss: 0.3573\n",
            "Epoch 31/40\n",
            "11610/11610 [==============================] - 0s 32us/step - loss: 0.3325 - val_loss: 0.3316\n",
            "Epoch 32/40\n",
            "11610/11610 [==============================] - 0s 32us/step - loss: 0.3300 - val_loss: 0.3235\n",
            "Epoch 33/40\n",
            "11610/11610 [==============================] - 0s 32us/step - loss: 0.3284 - val_loss: 0.3276\n",
            "Epoch 34/40\n",
            "11610/11610 [==============================] - 0s 32us/step - loss: 0.3303 - val_loss: 0.3245\n",
            "Epoch 35/40\n",
            "11610/11610 [==============================] - 0s 33us/step - loss: 0.3250 - val_loss: 0.3193\n",
            "Epoch 36/40\n",
            "11610/11610 [==============================] - 0s 32us/step - loss: 0.3242 - val_loss: 0.3495\n",
            "Epoch 37/40\n",
            "11610/11610 [==============================] - 0s 31us/step - loss: 0.3275 - val_loss: 0.3190\n",
            "Epoch 38/40\n",
            "11610/11610 [==============================] - 0s 32us/step - loss: 0.3227 - val_loss: 0.3194\n",
            "Epoch 39/40\n",
            "11610/11610 [==============================] - 0s 31us/step - loss: 0.3233 - val_loss: 0.3177\n",
            "Epoch 40/40\n",
            "11610/11610 [==============================] - 0s 32us/step - loss: 0.3192 - val_loss: 0.3141\n",
            "5160/5160 [==============================] - 0s 14us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "h53bYiJJeHRZ",
        "colab": {}
      },
      "source": [
        "# Hyper Parameter Tuning"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Rhqjf17aeHRb",
        "colab": {}
      },
      "source": [
        "#defining the hyperparameters in a dictionary \n",
        "\n",
        "\n",
        "from tensorflow.keras.activations import relu, elu\n",
        "\n",
        "p={'activation1':[relu,elu],\n",
        "   'optimizer':['sgd','adam'],\n",
        "   'first_hidden_layer': [30, 16, 8],\n",
        "   'second_hidden_layer': [16, 12, 8]}\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "2gU0mBrugEbs",
        "colab": {}
      },
      "source": [
        "#Building a Neural Network model \n",
        "def model_1(x_train,y_train,x_val,y_val,params):\n",
        "\n",
        "  model1=keras.models.Sequential()\n",
        "  model1.add(keras.layers.Dense(params['first_hidden_layer'],activation=params['activation1'], input_shape=x_train.shape[1:]))\n",
        "  model1.add(keras.layers.Dense(params['second_hidden_layer'],activation=params['activation1']))\n",
        "  model1.add(keras.layers.Dense(1))\n",
        "\n",
        "  model1.compile(loss='mean_squared_error', optimizer=params['optimizer'])\n",
        "\n",
        "  history=model1.fit(x_train,y_train, epochs=40, validation_data=(x_valid,y_valid))\n",
        "  return history,model1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "31DA8FPFhKKA",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "302563f1-feac-4d39-dfd1-7c78ff7a2291"
      },
      "source": [
        "import talos\n",
        "from talos import Scan\n",
        "\n",
        "h=Scan(x_train,y_train, \n",
        "       model=model_1,\n",
        "       params=p,\n",
        "       print_params=True,\n",
        "       reduction_metric=\"val_loss\",\n",
        "       experiment_name='e1')\n",
        "\n",
        "#as the search space is small(36), I am applying this to the entire search space. Alternatively you can searcha percent of the search space\n",
        "#using the grid_downsample parameter in the Scan function above."
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/36 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "{'activation1': <function relu at 0x7f4d33213598>, 'first_hidden_layer': 30, 'optimizer': 'sgd', 'second_hidden_layer': 16}\n",
            "Train on 8126 samples, validate on 3870 samples\n",
            "Epoch 1/40\n",
            "8126/8126 [==============================] - 0s 40us/step - loss: 2.8236 - val_loss: 1.7792\n",
            "Epoch 2/40\n",
            "8126/8126 [==============================] - 0s 34us/step - loss: 2.1409 - val_loss: 0.5290\n",
            "Epoch 3/40\n",
            "8126/8126 [==============================] - 0s 36us/step - loss: 0.5155 - val_loss: 0.4657\n",
            "Epoch 4/40\n",
            "8126/8126 [==============================] - 0s 35us/step - loss: 0.4619 - val_loss: 0.4327\n",
            "Epoch 5/40\n",
            "8126/8126 [==============================] - 0s 34us/step - loss: 0.4330 - val_loss: 0.4023\n",
            "Epoch 6/40\n",
            "8126/8126 [==============================] - 0s 33us/step - loss: 0.4165 - val_loss: 0.3927\n",
            "Epoch 7/40\n",
            "8126/8126 [==============================] - 0s 33us/step - loss: 0.4025 - val_loss: 0.3851\n",
            "Epoch 8/40\n",
            "8126/8126 [==============================] - 0s 35us/step - loss: 0.3945 - val_loss: 0.3794\n",
            "Epoch 9/40\n",
            "8126/8126 [==============================] - 0s 35us/step - loss: 0.3863 - val_loss: 0.3772\n",
            "Epoch 10/40\n",
            "8126/8126 [==============================] - 0s 33us/step - loss: 0.3793 - val_loss: 0.3707\n",
            "Epoch 11/40\n",
            "8126/8126 [==============================] - 0s 34us/step - loss: 0.3728 - val_loss: 0.3726\n",
            "Epoch 12/40\n",
            "8126/8126 [==============================] - 0s 34us/step - loss: 0.3683 - val_loss: 0.3649\n",
            "Epoch 13/40\n",
            "8126/8126 [==============================] - 0s 34us/step - loss: 0.3639 - val_loss: 0.3594\n",
            "Epoch 14/40\n",
            "8126/8126 [==============================] - 0s 33us/step - loss: 0.3594 - val_loss: 0.3572\n",
            "Epoch 15/40\n",
            "8126/8126 [==============================] - 0s 35us/step - loss: 0.3558 - val_loss: 0.3504\n",
            "Epoch 16/40\n",
            "8126/8126 [==============================] - 0s 36us/step - loss: 0.3517 - val_loss: 0.3484\n",
            "Epoch 17/40\n",
            "8126/8126 [==============================] - 0s 34us/step - loss: 0.3494 - val_loss: 0.3516\n",
            "Epoch 18/40\n",
            "8126/8126 [==============================] - 0s 35us/step - loss: 0.3446 - val_loss: 0.3433\n",
            "Epoch 19/40\n",
            "8126/8126 [==============================] - 0s 34us/step - loss: 0.3408 - val_loss: 0.3432\n",
            "Epoch 20/40\n",
            "8126/8126 [==============================] - 0s 33us/step - loss: 0.3371 - val_loss: 0.3425\n",
            "Epoch 21/40\n",
            "8126/8126 [==============================] - 0s 33us/step - loss: 0.3357 - val_loss: 0.3387\n",
            "Epoch 22/40\n",
            "8126/8126 [==============================] - 0s 34us/step - loss: 0.3328 - val_loss: 0.3325\n",
            "Epoch 23/40\n",
            "8126/8126 [==============================] - 0s 37us/step - loss: 0.3303 - val_loss: 0.3329\n",
            "Epoch 24/40\n",
            "8126/8126 [==============================] - 0s 33us/step - loss: 0.3271 - val_loss: 0.3407\n",
            "Epoch 25/40\n",
            "8126/8126 [==============================] - 0s 38us/step - loss: 0.3273 - val_loss: 0.3298\n",
            "Epoch 26/40\n",
            "8126/8126 [==============================] - 0s 37us/step - loss: 0.3245 - val_loss: 0.3357\n",
            "Epoch 27/40\n",
            "8126/8126 [==============================] - 0s 35us/step - loss: 0.3235 - val_loss: 0.3280\n",
            "Epoch 28/40\n",
            "8126/8126 [==============================] - 0s 33us/step - loss: 0.3204 - val_loss: 0.3264\n",
            "Epoch 29/40\n",
            "8126/8126 [==============================] - 0s 36us/step - loss: 0.3180 - val_loss: 0.3341\n",
            "Epoch 30/40\n",
            "8126/8126 [==============================] - 0s 34us/step - loss: 0.3170 - val_loss: 0.3282\n",
            "Epoch 31/40\n",
            "8126/8126 [==============================] - 0s 35us/step - loss: 0.3166 - val_loss: 0.3286\n",
            "Epoch 32/40\n",
            "8126/8126 [==============================] - 0s 33us/step - loss: 0.3143 - val_loss: 0.3222\n",
            "Epoch 33/40\n",
            "8126/8126 [==============================] - 0s 34us/step - loss: 0.3124 - val_loss: 0.3230\n",
            "Epoch 34/40\n",
            "8126/8126 [==============================] - 0s 34us/step - loss: 0.3122 - val_loss: 0.3322\n",
            "Epoch 35/40\n",
            "8126/8126 [==============================] - 0s 33us/step - loss: 0.3137 - val_loss: 0.3274\n",
            "Epoch 36/40\n",
            "8126/8126 [==============================] - 0s 34us/step - loss: 0.3106 - val_loss: 0.3263\n",
            "Epoch 37/40\n",
            "8126/8126 [==============================] - 0s 34us/step - loss: 0.3106 - val_loss: 0.3187\n",
            "Epoch 38/40\n",
            "8126/8126 [==============================] - 0s 34us/step - loss: 0.3080 - val_loss: 0.3330\n",
            "Epoch 39/40\n",
            "8126/8126 [==============================] - 0s 37us/step - loss: 0.3073 - val_loss: 0.3162\n",
            "Epoch 40/40\n",
            "8126/8126 [==============================] - 0s 34us/step - loss: 0.3071 - val_loss: 0.3382\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  3%|▎         | 1/36 [00:11<06:49, 11.69s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "{'activation1': <function relu at 0x7f4d33213598>, 'first_hidden_layer': 30, 'optimizer': 'sgd', 'second_hidden_layer': 12}\n",
            "Train on 8126 samples, validate on 3870 samples\n",
            "Epoch 1/40\n",
            "8126/8126 [==============================] - 0s 39us/step - loss: 1.2942 - val_loss: 0.6642\n",
            "Epoch 2/40\n",
            "8126/8126 [==============================] - 0s 34us/step - loss: 1.0466 - val_loss: 0.5010\n",
            "Epoch 3/40\n",
            "8126/8126 [==============================] - 0s 34us/step - loss: 0.4876 - val_loss: 0.4530\n",
            "Epoch 4/40\n",
            "8126/8126 [==============================] - 0s 35us/step - loss: 0.4542 - val_loss: 0.4295\n",
            "Epoch 5/40\n",
            "8126/8126 [==============================] - 0s 33us/step - loss: 0.4350 - val_loss: 0.4145\n",
            "Epoch 6/40\n",
            "8126/8126 [==============================] - 0s 33us/step - loss: 0.4215 - val_loss: 0.4019\n",
            "Epoch 7/40\n",
            "8126/8126 [==============================] - 0s 33us/step - loss: 0.4080 - val_loss: 0.4047\n",
            "Epoch 8/40\n",
            "8126/8126 [==============================] - 0s 32us/step - loss: 0.3996 - val_loss: 0.3921\n",
            "Epoch 9/40\n",
            "8126/8126 [==============================] - 0s 33us/step - loss: 0.3917 - val_loss: 0.3863\n",
            "Epoch 10/40\n",
            "8126/8126 [==============================] - 0s 36us/step - loss: 0.3880 - val_loss: 0.3762\n",
            "Epoch 11/40\n",
            "8126/8126 [==============================] - 0s 34us/step - loss: 0.3811 - val_loss: 0.3762\n",
            "Epoch 12/40\n",
            "8126/8126 [==============================] - 0s 33us/step - loss: 0.3751 - val_loss: 0.3926\n",
            "Epoch 13/40\n",
            "8126/8126 [==============================] - 0s 35us/step - loss: 0.3694 - val_loss: 0.3675\n",
            "Epoch 14/40\n",
            "8126/8126 [==============================] - 0s 34us/step - loss: 0.3640 - val_loss: 0.3715\n",
            "Epoch 15/40\n",
            "8126/8126 [==============================] - 0s 35us/step - loss: 0.3635 - val_loss: 0.3598\n",
            "Epoch 16/40\n",
            "8126/8126 [==============================] - 0s 34us/step - loss: 0.3601 - val_loss: 0.3588\n",
            "Epoch 17/40\n",
            "8126/8126 [==============================] - 0s 35us/step - loss: 0.3514 - val_loss: 0.3501\n",
            "Epoch 18/40\n",
            "8126/8126 [==============================] - 0s 35us/step - loss: 0.3476 - val_loss: 0.3500\n",
            "Epoch 19/40\n",
            "8126/8126 [==============================] - 0s 36us/step - loss: 0.3424 - val_loss: 0.3438\n",
            "Epoch 20/40\n",
            "8126/8126 [==============================] - 0s 35us/step - loss: 0.3410 - val_loss: 0.3480\n",
            "Epoch 21/40\n",
            "8126/8126 [==============================] - 0s 36us/step - loss: 0.3387 - val_loss: 0.3397\n",
            "Epoch 22/40\n",
            "8126/8126 [==============================] - 0s 35us/step - loss: 0.3363 - val_loss: 0.3547\n",
            "Epoch 23/40\n",
            "8126/8126 [==============================] - 0s 34us/step - loss: 0.3325 - val_loss: 0.3401\n",
            "Epoch 24/40\n",
            "8126/8126 [==============================] - 0s 36us/step - loss: 0.3314 - val_loss: 0.3343\n",
            "Epoch 25/40\n",
            "8126/8126 [==============================] - 0s 34us/step - loss: 0.3295 - val_loss: 0.3370\n",
            "Epoch 26/40\n",
            "8126/8126 [==============================] - 0s 34us/step - loss: 0.3250 - val_loss: 0.3429\n",
            "Epoch 27/40\n",
            "8126/8126 [==============================] - 0s 34us/step - loss: 0.3272 - val_loss: 0.3288\n",
            "Epoch 28/40\n",
            "8126/8126 [==============================] - 0s 35us/step - loss: 0.3233 - val_loss: 0.3306\n",
            "Epoch 29/40\n",
            "8126/8126 [==============================] - 0s 35us/step - loss: 0.3224 - val_loss: 0.3303\n",
            "Epoch 30/40\n",
            "8126/8126 [==============================] - 0s 34us/step - loss: 0.3212 - val_loss: 0.3284\n",
            "Epoch 31/40\n",
            "8126/8126 [==============================] - 0s 35us/step - loss: 0.3180 - val_loss: 0.3248\n",
            "Epoch 32/40\n",
            "8126/8126 [==============================] - 0s 34us/step - loss: 0.3171 - val_loss: 0.3272\n",
            "Epoch 33/40\n",
            "8126/8126 [==============================] - 0s 34us/step - loss: 0.3154 - val_loss: 0.3276\n",
            "Epoch 34/40\n",
            "8126/8126 [==============================] - 0s 36us/step - loss: 0.3152 - val_loss: 0.3395\n",
            "Epoch 35/40\n",
            "8126/8126 [==============================] - 0s 35us/step - loss: 0.3147 - val_loss: 0.3326\n",
            "Epoch 36/40\n",
            "8126/8126 [==============================] - 0s 34us/step - loss: 0.3144 - val_loss: 0.3227\n",
            "Epoch 37/40\n",
            "8126/8126 [==============================] - 0s 35us/step - loss: 0.3131 - val_loss: 0.3293\n",
            "Epoch 38/40\n",
            "8126/8126 [==============================] - 0s 34us/step - loss: 0.3109 - val_loss: 0.3247\n",
            "Epoch 39/40\n",
            "8126/8126 [==============================] - 0s 34us/step - loss: 0.3105 - val_loss: 0.3287\n",
            "Epoch 40/40\n",
            "8126/8126 [==============================] - 0s 35us/step - loss: 0.3112 - val_loss: 0.3230\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  6%|▌         | 2/36 [00:23<06:36, 11.66s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "{'activation1': <function relu at 0x7f4d33213598>, 'first_hidden_layer': 30, 'optimizer': 'sgd', 'second_hidden_layer': 8}\n",
            "Train on 8126 samples, validate on 3870 samples\n",
            "Epoch 1/40\n",
            "8126/8126 [==============================] - 0s 39us/step - loss: 1.5919 - val_loss: 0.5727\n",
            "Epoch 2/40\n",
            "8126/8126 [==============================] - 0s 33us/step - loss: 0.5288 - val_loss: 0.4642\n",
            "Epoch 3/40\n",
            "8126/8126 [==============================] - 0s 31us/step - loss: 0.4611 - val_loss: 0.4277\n",
            "Epoch 4/40\n",
            "8126/8126 [==============================] - 0s 35us/step - loss: 0.4373 - val_loss: 0.4107\n",
            "Epoch 5/40\n",
            "8126/8126 [==============================] - 0s 33us/step - loss: 0.4194 - val_loss: 0.4050\n",
            "Epoch 6/40\n",
            "8126/8126 [==============================] - 0s 33us/step - loss: 0.4083 - val_loss: 0.3954\n",
            "Epoch 7/40\n",
            "8126/8126 [==============================] - 0s 32us/step - loss: 0.4019 - val_loss: 0.3939\n",
            "Epoch 8/40\n",
            "8126/8126 [==============================] - 0s 33us/step - loss: 0.3931 - val_loss: 0.4239\n",
            "Epoch 9/40\n",
            "8126/8126 [==============================] - 0s 32us/step - loss: 0.3857 - val_loss: 0.3833\n",
            "Epoch 10/40\n",
            "8126/8126 [==============================] - 0s 33us/step - loss: 0.3818 - val_loss: 0.3727\n",
            "Epoch 11/40\n",
            "8126/8126 [==============================] - 0s 33us/step - loss: 0.3772 - val_loss: 0.3778\n",
            "Epoch 12/40\n",
            "8126/8126 [==============================] - 0s 33us/step - loss: 0.3733 - val_loss: 0.3738\n",
            "Epoch 13/40\n",
            "8126/8126 [==============================] - 0s 35us/step - loss: 0.3696 - val_loss: 0.3685\n",
            "Epoch 14/40\n",
            "8126/8126 [==============================] - 0s 33us/step - loss: 0.3672 - val_loss: 0.3653\n",
            "Epoch 15/40\n",
            "8126/8126 [==============================] - 0s 33us/step - loss: 0.3627 - val_loss: 0.3718\n",
            "Epoch 16/40\n",
            "8126/8126 [==============================] - 0s 35us/step - loss: 0.3558 - val_loss: 0.3856\n",
            "Epoch 17/40\n",
            "8126/8126 [==============================] - 0s 32us/step - loss: 0.3531 - val_loss: 0.3667\n",
            "Epoch 18/40\n",
            "8126/8126 [==============================] - 0s 33us/step - loss: 0.3545 - val_loss: 0.3534\n",
            "Epoch 19/40\n",
            "8126/8126 [==============================] - 0s 33us/step - loss: 0.3471 - val_loss: 0.3543\n",
            "Epoch 20/40\n",
            "8126/8126 [==============================] - 0s 35us/step - loss: 0.3459 - val_loss: 0.3578\n",
            "Epoch 21/40\n",
            "8126/8126 [==============================] - 0s 33us/step - loss: 0.3442 - val_loss: 0.3525\n",
            "Epoch 22/40\n",
            "8126/8126 [==============================] - 0s 34us/step - loss: 0.3423 - val_loss: 0.3408\n",
            "Epoch 23/40\n",
            "8126/8126 [==============================] - 0s 35us/step - loss: 0.3406 - val_loss: 0.3446\n",
            "Epoch 24/40\n",
            "8126/8126 [==============================] - 0s 32us/step - loss: 0.3385 - val_loss: 0.3475\n",
            "Epoch 25/40\n",
            "8126/8126 [==============================] - 0s 36us/step - loss: 0.3359 - val_loss: 0.3377\n",
            "Epoch 26/40\n",
            "8126/8126 [==============================] - 0s 33us/step - loss: 0.3366 - val_loss: 0.3386\n",
            "Epoch 27/40\n",
            "8126/8126 [==============================] - 0s 34us/step - loss: 0.3344 - val_loss: 0.3390\n",
            "Epoch 28/40\n",
            "8126/8126 [==============================] - 0s 33us/step - loss: 0.3304 - val_loss: 0.3424\n",
            "Epoch 29/40\n",
            "8126/8126 [==============================] - 0s 35us/step - loss: 0.3314 - val_loss: 0.3408\n",
            "Epoch 30/40\n",
            "8126/8126 [==============================] - 0s 33us/step - loss: 0.3264 - val_loss: 0.3335\n",
            "Epoch 31/40\n",
            "8126/8126 [==============================] - 0s 34us/step - loss: 0.3287 - val_loss: 0.3414\n",
            "Epoch 32/40\n",
            "8126/8126 [==============================] - 0s 34us/step - loss: 0.3259 - val_loss: 0.3528\n",
            "Epoch 33/40\n",
            "8126/8126 [==============================] - 0s 34us/step - loss: 0.3241 - val_loss: 0.3280\n",
            "Epoch 34/40\n",
            "8126/8126 [==============================] - 0s 33us/step - loss: 0.3233 - val_loss: 0.3307\n",
            "Epoch 35/40\n",
            "8126/8126 [==============================] - 0s 33us/step - loss: 0.3206 - val_loss: 0.3286\n",
            "Epoch 36/40\n",
            "8126/8126 [==============================] - 0s 34us/step - loss: 0.3206 - val_loss: 0.3284\n",
            "Epoch 37/40\n",
            "8126/8126 [==============================] - 0s 33us/step - loss: 0.3183 - val_loss: 0.3261\n",
            "Epoch 38/40\n",
            "8126/8126 [==============================] - 0s 34us/step - loss: 0.3184 - val_loss: 0.3252\n",
            "Epoch 39/40\n",
            "8126/8126 [==============================] - 0s 33us/step - loss: 0.3161 - val_loss: 0.3237\n",
            "Epoch 40/40\n",
            "8126/8126 [==============================] - 0s 32us/step - loss: 0.3150 - val_loss: 0.3212\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  8%|▊         | 3/36 [00:34<06:20, 11.53s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "{'activation1': <function relu at 0x7f4d33213598>, 'first_hidden_layer': 30, 'optimizer': 'adam', 'second_hidden_layer': 16}\n",
            "Train on 8126 samples, validate on 3870 samples\n",
            "Epoch 1/40\n",
            "8126/8126 [==============================] - 0s 46us/step - loss: 1.7898 - val_loss: 0.6750\n",
            "Epoch 2/40\n",
            "8126/8126 [==============================] - 0s 36us/step - loss: 0.6009 - val_loss: 0.5083\n",
            "Epoch 3/40\n",
            "8126/8126 [==============================] - 0s 36us/step - loss: 0.4852 - val_loss: 0.4576\n",
            "Epoch 4/40\n",
            "8126/8126 [==============================] - 0s 34us/step - loss: 0.4426 - val_loss: 0.4265\n",
            "Epoch 5/40\n",
            "8126/8126 [==============================] - 0s 37us/step - loss: 0.4220 - val_loss: 0.4119\n",
            "Epoch 6/40\n",
            "8126/8126 [==============================] - 0s 36us/step - loss: 0.4081 - val_loss: 0.3967\n",
            "Epoch 7/40\n",
            "8126/8126 [==============================] - 0s 36us/step - loss: 0.3963 - val_loss: 0.3886\n",
            "Epoch 8/40\n",
            "8126/8126 [==============================] - 0s 37us/step - loss: 0.3889 - val_loss: 0.3825\n",
            "Epoch 9/40\n",
            "8126/8126 [==============================] - 0s 36us/step - loss: 0.3817 - val_loss: 0.3828\n",
            "Epoch 10/40\n",
            "8126/8126 [==============================] - 0s 37us/step - loss: 0.3776 - val_loss: 0.3774\n",
            "Epoch 11/40\n",
            "8126/8126 [==============================] - 0s 35us/step - loss: 0.3731 - val_loss: 0.3774\n",
            "Epoch 12/40\n",
            "8126/8126 [==============================] - 0s 36us/step - loss: 0.3666 - val_loss: 0.3650\n",
            "Epoch 13/40\n",
            "8126/8126 [==============================] - 0s 41us/step - loss: 0.3624 - val_loss: 0.3624\n",
            "Epoch 14/40\n",
            "8126/8126 [==============================] - 0s 36us/step - loss: 0.3571 - val_loss: 0.3550\n",
            "Epoch 15/40\n",
            "8126/8126 [==============================] - 0s 36us/step - loss: 0.3524 - val_loss: 0.3538\n",
            "Epoch 16/40\n",
            "8126/8126 [==============================] - 0s 39us/step - loss: 0.3498 - val_loss: 0.3503\n",
            "Epoch 17/40\n",
            "8126/8126 [==============================] - 0s 37us/step - loss: 0.3450 - val_loss: 0.3530\n",
            "Epoch 18/40\n",
            "8126/8126 [==============================] - 0s 36us/step - loss: 0.3422 - val_loss: 0.3440\n",
            "Epoch 19/40\n",
            "8126/8126 [==============================] - 0s 38us/step - loss: 0.3372 - val_loss: 0.3426\n",
            "Epoch 20/40\n",
            "8126/8126 [==============================] - 0s 36us/step - loss: 0.3325 - val_loss: 0.3334\n",
            "Epoch 21/40\n",
            "8126/8126 [==============================] - 0s 36us/step - loss: 0.3283 - val_loss: 0.3346\n",
            "Epoch 22/40\n",
            "8126/8126 [==============================] - 0s 38us/step - loss: 0.3239 - val_loss: 0.3291\n",
            "Epoch 23/40\n",
            "8126/8126 [==============================] - 0s 37us/step - loss: 0.3192 - val_loss: 0.3270\n",
            "Epoch 24/40\n",
            "8126/8126 [==============================] - 0s 37us/step - loss: 0.3187 - val_loss: 0.3242\n",
            "Epoch 25/40\n",
            "8126/8126 [==============================] - 0s 38us/step - loss: 0.3124 - val_loss: 0.3302\n",
            "Epoch 26/40\n",
            "8126/8126 [==============================] - 0s 37us/step - loss: 0.3085 - val_loss: 0.3227\n",
            "Epoch 27/40\n",
            "8126/8126 [==============================] - 0s 39us/step - loss: 0.3100 - val_loss: 0.3175\n",
            "Epoch 28/40\n",
            "8126/8126 [==============================] - 0s 39us/step - loss: 0.3054 - val_loss: 0.3129\n",
            "Epoch 29/40\n",
            "8126/8126 [==============================] - 0s 37us/step - loss: 0.3022 - val_loss: 0.3178\n",
            "Epoch 30/40\n",
            "8126/8126 [==============================] - 0s 38us/step - loss: 0.3032 - val_loss: 0.3239\n",
            "Epoch 31/40\n",
            "8126/8126 [==============================] - 0s 36us/step - loss: 0.2981 - val_loss: 0.3099\n",
            "Epoch 32/40\n",
            "8126/8126 [==============================] - 0s 36us/step - loss: 0.2953 - val_loss: 0.3130\n",
            "Epoch 33/40\n",
            "8126/8126 [==============================] - 0s 37us/step - loss: 0.2951 - val_loss: 0.3107\n",
            "Epoch 34/40\n",
            "8126/8126 [==============================] - 0s 37us/step - loss: 0.2929 - val_loss: 0.3083\n",
            "Epoch 35/40\n",
            "8126/8126 [==============================] - 0s 36us/step - loss: 0.2896 - val_loss: 0.3134\n",
            "Epoch 36/40\n",
            "8126/8126 [==============================] - 0s 35us/step - loss: 0.2884 - val_loss: 0.3141\n",
            "Epoch 37/40\n",
            "8126/8126 [==============================] - 0s 36us/step - loss: 0.2882 - val_loss: 0.3121\n",
            "Epoch 38/40\n",
            "8126/8126 [==============================] - 0s 36us/step - loss: 0.2881 - val_loss: 0.3074\n",
            "Epoch 39/40\n",
            "8126/8126 [==============================] - 0s 38us/step - loss: 0.2862 - val_loss: 0.3121\n",
            "Epoch 40/40\n",
            "8126/8126 [==============================] - 0s 36us/step - loss: 0.2855 - val_loss: 0.3090\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 11%|█         | 4/36 [00:47<06:18, 11.83s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "{'activation1': <function relu at 0x7f4d33213598>, 'first_hidden_layer': 30, 'optimizer': 'adam', 'second_hidden_layer': 12}\n",
            "Train on 8126 samples, validate on 3870 samples\n",
            "Epoch 1/40\n",
            "8126/8126 [==============================] - 0s 47us/step - loss: 1.5320 - val_loss: 0.6721\n",
            "Epoch 2/40\n",
            "8126/8126 [==============================] - 0s 36us/step - loss: 0.7110 - val_loss: 0.5114\n",
            "Epoch 3/40\n",
            "8126/8126 [==============================] - 0s 36us/step - loss: 0.5139 - val_loss: 0.4325\n",
            "Epoch 4/40\n",
            "8126/8126 [==============================] - 0s 37us/step - loss: 0.4307 - val_loss: 0.4063\n",
            "Epoch 5/40\n",
            "8126/8126 [==============================] - 0s 37us/step - loss: 0.4091 - val_loss: 0.3966\n",
            "Epoch 6/40\n",
            "8126/8126 [==============================] - 0s 36us/step - loss: 0.3974 - val_loss: 0.3978\n",
            "Epoch 7/40\n",
            "8126/8126 [==============================] - 0s 38us/step - loss: 0.3887 - val_loss: 0.3827\n",
            "Epoch 8/40\n",
            "8126/8126 [==============================] - 0s 36us/step - loss: 0.3801 - val_loss: 0.3757\n",
            "Epoch 9/40\n",
            "8126/8126 [==============================] - 0s 37us/step - loss: 0.3756 - val_loss: 0.3706\n",
            "Epoch 10/40\n",
            "8126/8126 [==============================] - 0s 35us/step - loss: 0.3688 - val_loss: 0.3692\n",
            "Epoch 11/40\n",
            "8126/8126 [==============================] - 0s 37us/step - loss: 0.3660 - val_loss: 0.3633\n",
            "Epoch 12/40\n",
            "8126/8126 [==============================] - 0s 38us/step - loss: 0.3615 - val_loss: 0.3605\n",
            "Epoch 13/40\n",
            "8126/8126 [==============================] - 0s 35us/step - loss: 0.3560 - val_loss: 0.3598\n",
            "Epoch 14/40\n",
            "8126/8126 [==============================] - 0s 38us/step - loss: 0.3548 - val_loss: 0.3571\n",
            "Epoch 15/40\n",
            "8126/8126 [==============================] - 0s 38us/step - loss: 0.3513 - val_loss: 0.3539\n",
            "Epoch 16/40\n",
            "8126/8126 [==============================] - 0s 36us/step - loss: 0.3447 - val_loss: 0.3685\n",
            "Epoch 17/40\n",
            "8126/8126 [==============================] - 0s 39us/step - loss: 0.3465 - val_loss: 0.3490\n",
            "Epoch 18/40\n",
            "8126/8126 [==============================] - 0s 37us/step - loss: 0.3443 - val_loss: 0.3439\n",
            "Epoch 19/40\n",
            "8126/8126 [==============================] - 0s 38us/step - loss: 0.3407 - val_loss: 0.3416\n",
            "Epoch 20/40\n",
            "8126/8126 [==============================] - 0s 38us/step - loss: 0.3349 - val_loss: 0.3376\n",
            "Epoch 21/40\n",
            "8126/8126 [==============================] - 0s 38us/step - loss: 0.3391 - val_loss: 0.3493\n",
            "Epoch 22/40\n",
            "8126/8126 [==============================] - 0s 38us/step - loss: 0.3283 - val_loss: 0.3383\n",
            "Epoch 23/40\n",
            "8126/8126 [==============================] - 0s 38us/step - loss: 0.3257 - val_loss: 0.3421\n",
            "Epoch 24/40\n",
            "8126/8126 [==============================] - 0s 39us/step - loss: 0.3198 - val_loss: 0.3339\n",
            "Epoch 25/40\n",
            "8126/8126 [==============================] - 0s 38us/step - loss: 0.3218 - val_loss: 0.3415\n",
            "Epoch 26/40\n",
            "8126/8126 [==============================] - 0s 38us/step - loss: 0.3293 - val_loss: 0.3311\n",
            "Epoch 27/40\n",
            "8126/8126 [==============================] - 0s 39us/step - loss: 0.3166 - val_loss: 0.3283\n",
            "Epoch 28/40\n",
            "8126/8126 [==============================] - 0s 38us/step - loss: 0.3183 - val_loss: 0.3239\n",
            "Epoch 29/40\n",
            "8126/8126 [==============================] - 0s 38us/step - loss: 0.3117 - val_loss: 0.3251\n",
            "Epoch 30/40\n",
            "8126/8126 [==============================] - 0s 40us/step - loss: 0.3115 - val_loss: 0.3248\n",
            "Epoch 31/40\n",
            "8126/8126 [==============================] - 0s 38us/step - loss: 0.3110 - val_loss: 0.3233\n",
            "Epoch 32/40\n",
            "8126/8126 [==============================] - 0s 38us/step - loss: 0.3150 - val_loss: 0.3309\n",
            "Epoch 33/40\n",
            "8126/8126 [==============================] - 0s 40us/step - loss: 0.3343 - val_loss: 0.3194\n",
            "Epoch 34/40\n",
            "8126/8126 [==============================] - 0s 38us/step - loss: 0.3090 - val_loss: 0.3234\n",
            "Epoch 35/40\n",
            "8126/8126 [==============================] - 0s 38us/step - loss: 0.3044 - val_loss: 0.3275\n",
            "Epoch 36/40\n",
            "8126/8126 [==============================] - 0s 37us/step - loss: 0.3098 - val_loss: 0.3206\n",
            "Epoch 37/40\n",
            "8126/8126 [==============================] - 0s 39us/step - loss: 0.3046 - val_loss: 0.3176\n",
            "Epoch 38/40\n",
            "8126/8126 [==============================] - 0s 38us/step - loss: 0.3005 - val_loss: 0.3230\n",
            "Epoch 39/40\n",
            "8126/8126 [==============================] - 0s 37us/step - loss: 0.3003 - val_loss: 0.3163\n",
            "Epoch 40/40\n",
            "8126/8126 [==============================] - 0s 39us/step - loss: 0.3008 - val_loss: 0.3252\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 14%|█▍        | 5/36 [00:59<06:15, 12.12s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "{'activation1': <function relu at 0x7f4d33213598>, 'first_hidden_layer': 30, 'optimizer': 'adam', 'second_hidden_layer': 8}\n",
            "Train on 8126 samples, validate on 3870 samples\n",
            "Epoch 1/40\n",
            "8126/8126 [==============================] - 0s 47us/step - loss: 2.2082 - val_loss: 0.8244\n",
            "Epoch 2/40\n",
            "8126/8126 [==============================] - 0s 37us/step - loss: 0.7748 - val_loss: 0.6388\n",
            "Epoch 3/40\n",
            "8126/8126 [==============================] - 0s 37us/step - loss: 0.5854 - val_loss: 0.5038\n",
            "Epoch 4/40\n",
            "8126/8126 [==============================] - 0s 37us/step - loss: 0.4875 - val_loss: 0.4437\n",
            "Epoch 5/40\n",
            "8126/8126 [==============================] - 0s 37us/step - loss: 0.4410 - val_loss: 0.4124\n",
            "Epoch 6/40\n",
            "8126/8126 [==============================] - 0s 39us/step - loss: 0.4174 - val_loss: 0.4002\n",
            "Epoch 7/40\n",
            "8126/8126 [==============================] - 0s 37us/step - loss: 0.4010 - val_loss: 0.3828\n",
            "Epoch 8/40\n",
            "8126/8126 [==============================] - 0s 38us/step - loss: 0.3888 - val_loss: 0.3711\n",
            "Epoch 9/40\n",
            "8126/8126 [==============================] - 0s 38us/step - loss: 0.3806 - val_loss: 0.3646\n",
            "Epoch 10/40\n",
            "8126/8126 [==============================] - 0s 37us/step - loss: 0.3725 - val_loss: 0.3578\n",
            "Epoch 11/40\n",
            "8126/8126 [==============================] - 0s 37us/step - loss: 0.3670 - val_loss: 0.3592\n",
            "Epoch 12/40\n",
            "8126/8126 [==============================] - 0s 37us/step - loss: 0.3638 - val_loss: 0.3505\n",
            "Epoch 13/40\n",
            "8126/8126 [==============================] - 0s 36us/step - loss: 0.3566 - val_loss: 0.3532\n",
            "Epoch 14/40\n",
            "8126/8126 [==============================] - 0s 37us/step - loss: 0.3524 - val_loss: 0.3440\n",
            "Epoch 15/40\n",
            "8126/8126 [==============================] - 0s 38us/step - loss: 0.3488 - val_loss: 0.3369\n",
            "Epoch 16/40\n",
            "8126/8126 [==============================] - 0s 37us/step - loss: 0.3425 - val_loss: 0.3398\n",
            "Epoch 17/40\n",
            "8126/8126 [==============================] - 0s 36us/step - loss: 0.3411 - val_loss: 0.3353\n",
            "Epoch 18/40\n",
            "8126/8126 [==============================] - 0s 38us/step - loss: 0.3339 - val_loss: 0.3293\n",
            "Epoch 19/40\n",
            "8126/8126 [==============================] - 0s 35us/step - loss: 0.3329 - val_loss: 0.3292\n",
            "Epoch 20/40\n",
            "8126/8126 [==============================] - 0s 37us/step - loss: 0.3307 - val_loss: 0.3266\n",
            "Epoch 21/40\n",
            "8126/8126 [==============================] - 0s 37us/step - loss: 0.3272 - val_loss: 0.3212\n",
            "Epoch 22/40\n",
            "8126/8126 [==============================] - 0s 36us/step - loss: 0.3225 - val_loss: 0.3215\n",
            "Epoch 23/40\n",
            "8126/8126 [==============================] - 0s 37us/step - loss: 0.3210 - val_loss: 0.3223\n",
            "Epoch 24/40\n",
            "8126/8126 [==============================] - 0s 36us/step - loss: 0.3166 - val_loss: 0.3317\n",
            "Epoch 25/40\n",
            "8126/8126 [==============================] - 0s 36us/step - loss: 0.3212 - val_loss: 0.3243\n",
            "Epoch 26/40\n",
            "8126/8126 [==============================] - 0s 36us/step - loss: 0.3179 - val_loss: 0.3330\n",
            "Epoch 27/40\n",
            "8126/8126 [==============================] - 0s 36us/step - loss: 0.3137 - val_loss: 0.3148\n",
            "Epoch 28/40\n",
            "8126/8126 [==============================] - 0s 36us/step - loss: 0.3111 - val_loss: 0.3160\n",
            "Epoch 29/40\n",
            "8126/8126 [==============================] - 0s 37us/step - loss: 0.3109 - val_loss: 0.3124\n",
            "Epoch 30/40\n",
            "8126/8126 [==============================] - 0s 37us/step - loss: 0.3181 - val_loss: 0.3119\n",
            "Epoch 31/40\n",
            "8126/8126 [==============================] - 0s 36us/step - loss: 0.3072 - val_loss: 0.3116\n",
            "Epoch 32/40\n",
            "8126/8126 [==============================] - 0s 36us/step - loss: 0.3060 - val_loss: 0.3208\n",
            "Epoch 33/40\n",
            "8126/8126 [==============================] - 0s 38us/step - loss: 0.3022 - val_loss: 0.3121\n",
            "Epoch 34/40\n",
            "8126/8126 [==============================] - 0s 39us/step - loss: 0.3011 - val_loss: 0.3138\n",
            "Epoch 35/40\n",
            "8126/8126 [==============================] - 0s 37us/step - loss: 0.3012 - val_loss: 0.3071\n",
            "Epoch 36/40\n",
            "8126/8126 [==============================] - 0s 37us/step - loss: 0.2984 - val_loss: 0.3141\n",
            "Epoch 37/40\n",
            "8126/8126 [==============================] - 0s 43us/step - loss: 0.2982 - val_loss: 0.3103\n",
            "Epoch 38/40\n",
            "8126/8126 [==============================] - 0s 40us/step - loss: 0.3015 - val_loss: 0.3220\n",
            "Epoch 39/40\n",
            "8126/8126 [==============================] - 0s 41us/step - loss: 0.2993 - val_loss: 0.3213\n",
            "Epoch 40/40\n",
            "8126/8126 [==============================] - 0s 40us/step - loss: 0.3315 - val_loss: 0.3069\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 17%|█▋        | 6/36 [01:12<06:09, 12.31s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "{'activation1': <function relu at 0x7f4d33213598>, 'first_hidden_layer': 16, 'optimizer': 'sgd', 'second_hidden_layer': 16}\n",
            "Train on 8126 samples, validate on 3870 samples\n",
            "Epoch 1/40\n",
            "8126/8126 [==============================] - 0s 45us/step - loss: 0.9947 - val_loss: 0.6560\n",
            "Epoch 2/40\n",
            "8126/8126 [==============================] - 0s 37us/step - loss: 0.8242 - val_loss: 0.5385\n",
            "Epoch 3/40\n",
            "8126/8126 [==============================] - 0s 37us/step - loss: 0.5139 - val_loss: 0.4743\n",
            "Epoch 4/40\n",
            "8126/8126 [==============================] - 0s 37us/step - loss: 0.4714 - val_loss: 0.4543\n",
            "Epoch 5/40\n",
            "8126/8126 [==============================] - 0s 37us/step - loss: 0.4506 - val_loss: 0.4376\n",
            "Epoch 6/40\n",
            "8126/8126 [==============================] - 0s 37us/step - loss: 0.4397 - val_loss: 0.4254\n",
            "Epoch 7/40\n",
            "8126/8126 [==============================] - 0s 37us/step - loss: 0.4308 - val_loss: 0.4248\n",
            "Epoch 8/40\n",
            "8126/8126 [==============================] - 0s 36us/step - loss: 0.4251 - val_loss: 0.4147\n",
            "Epoch 9/40\n",
            "8126/8126 [==============================] - 0s 38us/step - loss: 0.4198 - val_loss: 0.4089\n",
            "Epoch 10/40\n",
            "8126/8126 [==============================] - 0s 37us/step - loss: 0.4139 - val_loss: 0.4087\n",
            "Epoch 11/40\n",
            "8126/8126 [==============================] - 0s 38us/step - loss: 0.4095 - val_loss: 0.4089\n",
            "Epoch 12/40\n",
            "8126/8126 [==============================] - 0s 38us/step - loss: 0.4064 - val_loss: 0.3975\n",
            "Epoch 13/40\n",
            "8126/8126 [==============================] - 0s 36us/step - loss: 0.4017 - val_loss: 0.3921\n",
            "Epoch 14/40\n",
            "8126/8126 [==============================] - 0s 38us/step - loss: 0.3965 - val_loss: 0.3873\n",
            "Epoch 15/40\n",
            "8126/8126 [==============================] - 0s 37us/step - loss: 0.3939 - val_loss: 0.3852\n",
            "Epoch 16/40\n",
            "8126/8126 [==============================] - 0s 38us/step - loss: 0.3882 - val_loss: 0.3920\n",
            "Epoch 17/40\n",
            "8126/8126 [==============================] - 0s 37us/step - loss: 0.3863 - val_loss: 0.3791\n",
            "Epoch 18/40\n",
            "8126/8126 [==============================] - 0s 38us/step - loss: 0.3826 - val_loss: 0.3774\n",
            "Epoch 19/40\n",
            "8126/8126 [==============================] - 0s 37us/step - loss: 0.3781 - val_loss: 0.3904\n",
            "Epoch 20/40\n",
            "8126/8126 [==============================] - 0s 38us/step - loss: 0.3766 - val_loss: 0.3714\n",
            "Epoch 21/40\n",
            "8126/8126 [==============================] - 0s 36us/step - loss: 0.3734 - val_loss: 0.3713\n",
            "Epoch 22/40\n",
            "8126/8126 [==============================] - 0s 38us/step - loss: 0.3704 - val_loss: 0.3687\n",
            "Epoch 23/40\n",
            "8126/8126 [==============================] - 0s 36us/step - loss: 0.3688 - val_loss: 0.3654\n",
            "Epoch 24/40\n",
            "8126/8126 [==============================] - 0s 38us/step - loss: 0.3679 - val_loss: 0.4046\n",
            "Epoch 25/40\n",
            "8126/8126 [==============================] - 0s 37us/step - loss: 0.3664 - val_loss: 0.3676\n",
            "Epoch 26/40\n",
            "8126/8126 [==============================] - 0s 37us/step - loss: 0.3614 - val_loss: 0.3643\n",
            "Epoch 27/40\n",
            "8126/8126 [==============================] - 0s 37us/step - loss: 0.3624 - val_loss: 0.3683\n",
            "Epoch 28/40\n",
            "8126/8126 [==============================] - 0s 35us/step - loss: 0.3600 - val_loss: 0.3583\n",
            "Epoch 29/40\n",
            "8126/8126 [==============================] - 0s 38us/step - loss: 0.3564 - val_loss: 0.3557\n",
            "Epoch 30/40\n",
            "8126/8126 [==============================] - 0s 32us/step - loss: 0.3552 - val_loss: 0.3545\n",
            "Epoch 31/40\n",
            "8126/8126 [==============================] - 0s 35us/step - loss: 0.3538 - val_loss: 0.3571\n",
            "Epoch 32/40\n",
            "8126/8126 [==============================] - 0s 34us/step - loss: 0.3534 - val_loss: 0.3498\n",
            "Epoch 33/40\n",
            "8126/8126 [==============================] - 0s 33us/step - loss: 0.3518 - val_loss: 0.3550\n",
            "Epoch 34/40\n",
            "8126/8126 [==============================] - 0s 33us/step - loss: 0.3472 - val_loss: 0.3488\n",
            "Epoch 35/40\n",
            "8126/8126 [==============================] - 0s 32us/step - loss: 0.3472 - val_loss: 0.3671\n",
            "Epoch 36/40\n",
            "8126/8126 [==============================] - 0s 34us/step - loss: 0.3460 - val_loss: 0.3435\n",
            "Epoch 37/40\n",
            "8126/8126 [==============================] - 0s 32us/step - loss: 0.3431 - val_loss: 0.3499\n",
            "Epoch 38/40\n",
            "8126/8126 [==============================] - 0s 33us/step - loss: 0.3420 - val_loss: 0.3665\n",
            "Epoch 39/40\n",
            "8126/8126 [==============================] - 0s 33us/step - loss: 0.3429 - val_loss: 0.3459\n",
            "Epoch 40/40\n",
            "8126/8126 [==============================] - 0s 34us/step - loss: 0.3412 - val_loss: 0.3454\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 19%|█▉        | 7/36 [01:24<05:55, 12.26s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "{'activation1': <function relu at 0x7f4d33213598>, 'first_hidden_layer': 16, 'optimizer': 'sgd', 'second_hidden_layer': 12}\n",
            "Train on 8126 samples, validate on 3870 samples\n",
            "Epoch 1/40\n",
            "8126/8126 [==============================] - 0s 39us/step - loss: 1.0205 - val_loss: 0.6229\n",
            "Epoch 2/40\n",
            "8126/8126 [==============================] - 0s 33us/step - loss: 0.5853 - val_loss: 0.5242\n",
            "Epoch 3/40\n",
            "8126/8126 [==============================] - 0s 33us/step - loss: 0.5276 - val_loss: 0.4969\n",
            "Epoch 4/40\n",
            "8126/8126 [==============================] - 0s 32us/step - loss: 0.5012 - val_loss: 0.4845\n",
            "Epoch 5/40\n",
            "8126/8126 [==============================] - 0s 32us/step - loss: 0.4840 - val_loss: 0.4623\n",
            "Epoch 6/40\n",
            "8126/8126 [==============================] - 0s 34us/step - loss: 0.4707 - val_loss: 0.4537\n",
            "Epoch 7/40\n",
            "8126/8126 [==============================] - 0s 32us/step - loss: 0.4585 - val_loss: 0.4406\n",
            "Epoch 8/40\n",
            "8126/8126 [==============================] - 0s 33us/step - loss: 0.4491 - val_loss: 0.4359\n",
            "Epoch 9/40\n",
            "8126/8126 [==============================] - 0s 33us/step - loss: 0.4409 - val_loss: 0.4273\n",
            "Epoch 10/40\n",
            "8126/8126 [==============================] - 0s 33us/step - loss: 0.4326 - val_loss: 0.4315\n",
            "Epoch 11/40\n",
            "8126/8126 [==============================] - 0s 33us/step - loss: 0.4272 - val_loss: 0.4115\n",
            "Epoch 12/40\n",
            "8126/8126 [==============================] - 0s 33us/step - loss: 0.4213 - val_loss: 0.4090\n",
            "Epoch 13/40\n",
            "8126/8126 [==============================] - 0s 33us/step - loss: 0.4165 - val_loss: 0.4044\n",
            "Epoch 14/40\n",
            "8126/8126 [==============================] - 0s 33us/step - loss: 0.4106 - val_loss: 0.4073\n",
            "Epoch 15/40\n",
            "8126/8126 [==============================] - 0s 32us/step - loss: 0.4069 - val_loss: 0.3952\n",
            "Epoch 16/40\n",
            "8126/8126 [==============================] - 0s 34us/step - loss: 0.4038 - val_loss: 0.3905\n",
            "Epoch 17/40\n",
            "8126/8126 [==============================] - 0s 33us/step - loss: 0.3995 - val_loss: 0.3862\n",
            "Epoch 18/40\n",
            "8126/8126 [==============================] - 0s 32us/step - loss: 0.3940 - val_loss: 0.3843\n",
            "Epoch 19/40\n",
            "8126/8126 [==============================] - 0s 33us/step - loss: 0.3933 - val_loss: 0.3810\n",
            "Epoch 20/40\n",
            "8126/8126 [==============================] - 0s 33us/step - loss: 0.3886 - val_loss: 0.3834\n",
            "Epoch 21/40\n",
            "8126/8126 [==============================] - 0s 33us/step - loss: 0.3862 - val_loss: 0.3909\n",
            "Epoch 22/40\n",
            "8126/8126 [==============================] - 0s 32us/step - loss: 0.3816 - val_loss: 0.3760\n",
            "Epoch 23/40\n",
            "8126/8126 [==============================] - 0s 33us/step - loss: 0.3788 - val_loss: 0.3688\n",
            "Epoch 24/40\n",
            "8126/8126 [==============================] - 0s 33us/step - loss: 0.3744 - val_loss: 0.3710\n",
            "Epoch 25/40\n",
            "8126/8126 [==============================] - 0s 32us/step - loss: 0.3708 - val_loss: 0.3694\n",
            "Epoch 26/40\n",
            "8126/8126 [==============================] - 0s 32us/step - loss: 0.3681 - val_loss: 0.3616\n",
            "Epoch 27/40\n",
            "8126/8126 [==============================] - 0s 34us/step - loss: 0.3640 - val_loss: 0.3682\n",
            "Epoch 28/40\n",
            "8126/8126 [==============================] - 0s 36us/step - loss: 0.3611 - val_loss: 0.3572\n",
            "Epoch 29/40\n",
            "8126/8126 [==============================] - 0s 33us/step - loss: 0.3619 - val_loss: 0.3526\n",
            "Epoch 30/40\n",
            "8126/8126 [==============================] - 0s 32us/step - loss: 0.3549 - val_loss: 0.3643\n",
            "Epoch 31/40\n",
            "8126/8126 [==============================] - 0s 34us/step - loss: 0.3525 - val_loss: 0.3562\n",
            "Epoch 32/40\n",
            "8126/8126 [==============================] - 0s 34us/step - loss: 0.3526 - val_loss: 0.3473\n",
            "Epoch 33/40\n",
            "8126/8126 [==============================] - 0s 33us/step - loss: 0.3502 - val_loss: 0.3532\n",
            "Epoch 34/40\n",
            "8126/8126 [==============================] - 0s 31us/step - loss: 0.3492 - val_loss: 0.3425\n",
            "Epoch 35/40\n",
            "8126/8126 [==============================] - 0s 35us/step - loss: 0.3445 - val_loss: 0.3410\n",
            "Epoch 36/40\n",
            "8126/8126 [==============================] - 0s 33us/step - loss: 0.3438 - val_loss: 0.3446\n",
            "Epoch 37/40\n",
            "8126/8126 [==============================] - 0s 34us/step - loss: 0.3416 - val_loss: 0.3512\n",
            "Epoch 38/40\n",
            "8126/8126 [==============================] - 0s 32us/step - loss: 0.3396 - val_loss: 0.3366\n",
            "Epoch 39/40\n",
            "8126/8126 [==============================] - 0s 34us/step - loss: 0.3366 - val_loss: 0.3402\n",
            "Epoch 40/40\n",
            "8126/8126 [==============================] - 0s 33us/step - loss: 0.3376 - val_loss: 0.3410\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 22%|██▏       | 8/36 [01:35<05:33, 11.92s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "{'activation1': <function relu at 0x7f4d33213598>, 'first_hidden_layer': 16, 'optimizer': 'sgd', 'second_hidden_layer': 8}\n",
            "Train on 8126 samples, validate on 3870 samples\n",
            "Epoch 1/40\n",
            "8126/8126 [==============================] - 0s 39us/step - loss: 0.9751 - val_loss: 0.5380\n",
            "Epoch 2/40\n",
            "8126/8126 [==============================] - 0s 34us/step - loss: 0.5168 - val_loss: 0.4603\n",
            "Epoch 3/40\n",
            "8126/8126 [==============================] - 0s 33us/step - loss: 0.4673 - val_loss: 0.4258\n",
            "Epoch 4/40\n",
            "8126/8126 [==============================] - 0s 33us/step - loss: 0.4405 - val_loss: 0.4170\n",
            "Epoch 5/40\n",
            "8126/8126 [==============================] - 0s 33us/step - loss: 0.4279 - val_loss: 0.4181\n",
            "Epoch 6/40\n",
            "8126/8126 [==============================] - 0s 33us/step - loss: 0.4226 - val_loss: 0.4017\n",
            "Epoch 7/40\n",
            "8126/8126 [==============================] - 0s 33us/step - loss: 0.4156 - val_loss: 0.3976\n",
            "Epoch 8/40\n",
            "8126/8126 [==============================] - 0s 33us/step - loss: 0.4121 - val_loss: 0.3965\n",
            "Epoch 9/40\n",
            "8126/8126 [==============================] - 0s 33us/step - loss: 0.4066 - val_loss: 0.3922\n",
            "Epoch 10/40\n",
            "8126/8126 [==============================] - 0s 33us/step - loss: 0.4044 - val_loss: 0.3969\n",
            "Epoch 11/40\n",
            "8126/8126 [==============================] - 0s 32us/step - loss: 0.3987 - val_loss: 0.3870\n",
            "Epoch 12/40\n",
            "8126/8126 [==============================] - 0s 35us/step - loss: 0.3943 - val_loss: 0.3819\n",
            "Epoch 13/40\n",
            "8126/8126 [==============================] - 0s 33us/step - loss: 0.3911 - val_loss: 0.3789\n",
            "Epoch 14/40\n",
            "8126/8126 [==============================] - 0s 32us/step - loss: 0.3885 - val_loss: 0.3787\n",
            "Epoch 15/40\n",
            "8126/8126 [==============================] - 0s 33us/step - loss: 0.3835 - val_loss: 0.3814\n",
            "Epoch 16/40\n",
            "8126/8126 [==============================] - 0s 33us/step - loss: 0.3810 - val_loss: 0.3772\n",
            "Epoch 17/40\n",
            "8126/8126 [==============================] - 0s 33us/step - loss: 0.3786 - val_loss: 0.3766\n",
            "Epoch 18/40\n",
            "8126/8126 [==============================] - 0s 33us/step - loss: 0.3769 - val_loss: 0.3685\n",
            "Epoch 19/40\n",
            "8126/8126 [==============================] - 0s 32us/step - loss: 0.3750 - val_loss: 0.3664\n",
            "Epoch 20/40\n",
            "8126/8126 [==============================] - 0s 34us/step - loss: 0.3737 - val_loss: 0.3637\n",
            "Epoch 21/40\n",
            "8126/8126 [==============================] - 0s 34us/step - loss: 0.3685 - val_loss: 0.3623\n",
            "Epoch 22/40\n",
            "8126/8126 [==============================] - 0s 34us/step - loss: 0.3688 - val_loss: 0.3583\n",
            "Epoch 23/40\n",
            "8126/8126 [==============================] - 0s 35us/step - loss: 0.3643 - val_loss: 0.3549\n",
            "Epoch 24/40\n",
            "8126/8126 [==============================] - 0s 36us/step - loss: 0.3638 - val_loss: 0.3598\n",
            "Epoch 25/40\n",
            "8126/8126 [==============================] - 0s 35us/step - loss: 0.3635 - val_loss: 0.3520\n",
            "Epoch 26/40\n",
            "8126/8126 [==============================] - 0s 33us/step - loss: 0.3613 - val_loss: 0.3516\n",
            "Epoch 27/40\n",
            "8126/8126 [==============================] - 0s 34us/step - loss: 0.3585 - val_loss: 0.3600\n",
            "Epoch 28/40\n",
            "8126/8126 [==============================] - 0s 34us/step - loss: 0.3553 - val_loss: 0.4083\n",
            "Epoch 29/40\n",
            "8126/8126 [==============================] - 0s 33us/step - loss: 0.3555 - val_loss: 0.3479\n",
            "Epoch 30/40\n",
            "8126/8126 [==============================] - 0s 34us/step - loss: 0.3536 - val_loss: 0.3451\n",
            "Epoch 31/40\n",
            "8126/8126 [==============================] - 0s 35us/step - loss: 0.3524 - val_loss: 0.3554\n",
            "Epoch 32/40\n",
            "8126/8126 [==============================] - 0s 33us/step - loss: 0.3490 - val_loss: 0.3441\n",
            "Epoch 33/40\n",
            "8126/8126 [==============================] - 0s 33us/step - loss: 0.3466 - val_loss: 0.3454\n",
            "Epoch 34/40\n",
            "8126/8126 [==============================] - 0s 33us/step - loss: 0.3440 - val_loss: 0.3450\n",
            "Epoch 35/40\n",
            "8126/8126 [==============================] - 0s 37us/step - loss: 0.3419 - val_loss: 0.3508\n",
            "Epoch 36/40\n",
            "8126/8126 [==============================] - 0s 33us/step - loss: 0.3388 - val_loss: 0.3403\n",
            "Epoch 37/40\n",
            "8126/8126 [==============================] - 0s 33us/step - loss: 0.3387 - val_loss: 0.3432\n",
            "Epoch 38/40\n",
            "8126/8126 [==============================] - 0s 36us/step - loss: 0.3373 - val_loss: 0.3442\n",
            "Epoch 39/40\n",
            "8126/8126 [==============================] - 0s 34us/step - loss: 0.3348 - val_loss: 0.3380\n",
            "Epoch 40/40\n",
            "8126/8126 [==============================] - 0s 33us/step - loss: 0.3351 - val_loss: 0.3359\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 25%|██▌       | 9/36 [01:47<05:17, 11.74s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "{'activation1': <function relu at 0x7f4d33213598>, 'first_hidden_layer': 16, 'optimizer': 'adam', 'second_hidden_layer': 16}\n",
            "Train on 8126 samples, validate on 3870 samples\n",
            "Epoch 1/40\n",
            "8126/8126 [==============================] - 0s 48us/step - loss: 1.8487 - val_loss: 0.7949\n",
            "Epoch 2/40\n",
            "8126/8126 [==============================] - 0s 36us/step - loss: 0.7052 - val_loss: 0.6007\n",
            "Epoch 3/40\n",
            "8126/8126 [==============================] - 0s 38us/step - loss: 0.5631 - val_loss: 0.5008\n",
            "Epoch 4/40\n",
            "8126/8126 [==============================] - 0s 38us/step - loss: 0.4866 - val_loss: 0.4461\n",
            "Epoch 5/40\n",
            "8126/8126 [==============================] - 0s 37us/step - loss: 0.4501 - val_loss: 0.4275\n",
            "Epoch 6/40\n",
            "8126/8126 [==============================] - 0s 36us/step - loss: 0.4289 - val_loss: 0.4081\n",
            "Epoch 7/40\n",
            "8126/8126 [==============================] - 0s 38us/step - loss: 0.4174 - val_loss: 0.3982\n",
            "Epoch 8/40\n",
            "8126/8126 [==============================] - 0s 35us/step - loss: 0.4051 - val_loss: 0.3874\n",
            "Epoch 9/40\n",
            "8126/8126 [==============================] - 0s 36us/step - loss: 0.3943 - val_loss: 0.3842\n",
            "Epoch 10/40\n",
            "8126/8126 [==============================] - 0s 37us/step - loss: 0.3874 - val_loss: 0.3707\n",
            "Epoch 11/40\n",
            "8126/8126 [==============================] - 0s 36us/step - loss: 0.3776 - val_loss: 0.3684\n",
            "Epoch 12/40\n",
            "8126/8126 [==============================] - 0s 37us/step - loss: 0.3748 - val_loss: 0.3601\n",
            "Epoch 13/40\n",
            "8126/8126 [==============================] - 0s 37us/step - loss: 0.3672 - val_loss: 0.3542\n",
            "Epoch 14/40\n",
            "8126/8126 [==============================] - 0s 37us/step - loss: 0.3616 - val_loss: 0.3506\n",
            "Epoch 15/40\n",
            "8126/8126 [==============================] - 0s 36us/step - loss: 0.3558 - val_loss: 0.3469\n",
            "Epoch 16/40\n",
            "8126/8126 [==============================] - 0s 36us/step - loss: 0.3534 - val_loss: 0.3467\n",
            "Epoch 17/40\n",
            "8126/8126 [==============================] - 0s 38us/step - loss: 0.3486 - val_loss: 0.3411\n",
            "Epoch 18/40\n",
            "8126/8126 [==============================] - 0s 36us/step - loss: 0.3498 - val_loss: 0.3383\n",
            "Epoch 19/40\n",
            "8126/8126 [==============================] - 0s 35us/step - loss: 0.3395 - val_loss: 0.3364\n",
            "Epoch 20/40\n",
            "8126/8126 [==============================] - 0s 37us/step - loss: 0.3353 - val_loss: 0.3342\n",
            "Epoch 21/40\n",
            "8126/8126 [==============================] - 0s 38us/step - loss: 0.3360 - val_loss: 0.3297\n",
            "Epoch 22/40\n",
            "8126/8126 [==============================] - 0s 37us/step - loss: 0.3325 - val_loss: 0.3254\n",
            "Epoch 23/40\n",
            "8126/8126 [==============================] - 0s 38us/step - loss: 0.3253 - val_loss: 0.3265\n",
            "Epoch 24/40\n",
            "8126/8126 [==============================] - 0s 37us/step - loss: 0.3257 - val_loss: 0.3283\n",
            "Epoch 25/40\n",
            "8126/8126 [==============================] - 0s 36us/step - loss: 0.3212 - val_loss: 0.3230\n",
            "Epoch 26/40\n",
            "8126/8126 [==============================] - 0s 37us/step - loss: 0.3199 - val_loss: 0.3252\n",
            "Epoch 27/40\n",
            "8126/8126 [==============================] - 0s 37us/step - loss: 0.3184 - val_loss: 0.3246\n",
            "Epoch 28/40\n",
            "8126/8126 [==============================] - 0s 36us/step - loss: 0.3160 - val_loss: 0.3190\n",
            "Epoch 29/40\n",
            "8126/8126 [==============================] - 0s 36us/step - loss: 0.3187 - val_loss: 0.3177\n",
            "Epoch 30/40\n",
            "8126/8126 [==============================] - 0s 36us/step - loss: 0.3118 - val_loss: 0.3207\n",
            "Epoch 31/40\n",
            "8126/8126 [==============================] - 0s 36us/step - loss: 0.3111 - val_loss: 0.3149\n",
            "Epoch 32/40\n",
            "8126/8126 [==============================] - 0s 36us/step - loss: 0.3081 - val_loss: 0.3144\n",
            "Epoch 33/40\n",
            "8126/8126 [==============================] - 0s 36us/step - loss: 0.3063 - val_loss: 0.3132\n",
            "Epoch 34/40\n",
            "8126/8126 [==============================] - 0s 36us/step - loss: 0.3059 - val_loss: 0.3153\n",
            "Epoch 35/40\n",
            "8126/8126 [==============================] - 0s 36us/step - loss: 0.3042 - val_loss: 0.3177\n",
            "Epoch 36/40\n",
            "8126/8126 [==============================] - 0s 35us/step - loss: 0.3046 - val_loss: 0.3114\n",
            "Epoch 37/40\n",
            "8126/8126 [==============================] - 0s 36us/step - loss: 0.3055 - val_loss: 0.3111\n",
            "Epoch 38/40\n",
            "8126/8126 [==============================] - 0s 38us/step - loss: 0.3103 - val_loss: 0.3157\n",
            "Epoch 39/40\n",
            "8126/8126 [==============================] - 0s 35us/step - loss: 0.3035 - val_loss: 0.3155\n",
            "Epoch 40/40\n",
            "8126/8126 [==============================] - 0s 39us/step - loss: 0.3030 - val_loss: 0.3089\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 28%|██▊       | 10/36 [01:59<05:11, 11.97s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "{'activation1': <function relu at 0x7f4d33213598>, 'first_hidden_layer': 16, 'optimizer': 'adam', 'second_hidden_layer': 12}\n",
            "Train on 8126 samples, validate on 3870 samples\n",
            "Epoch 1/40\n",
            "8126/8126 [==============================] - 0s 45us/step - loss: 1.6508 - val_loss: 0.7981\n",
            "Epoch 2/40\n",
            "8126/8126 [==============================] - 0s 38us/step - loss: 0.7779 - val_loss: 0.6123\n",
            "Epoch 3/40\n",
            "8126/8126 [==============================] - 0s 37us/step - loss: 0.5715 - val_loss: 0.4926\n",
            "Epoch 4/40\n",
            "8126/8126 [==============================] - 0s 36us/step - loss: 0.4835 - val_loss: 0.4433\n",
            "Epoch 5/40\n",
            "8126/8126 [==============================] - 0s 37us/step - loss: 0.4458 - val_loss: 0.4173\n",
            "Epoch 6/40\n",
            "8126/8126 [==============================] - 0s 36us/step - loss: 0.4276 - val_loss: 0.4060\n",
            "Epoch 7/40\n",
            "8126/8126 [==============================] - 0s 35us/step - loss: 0.4124 - val_loss: 0.4081\n",
            "Epoch 8/40\n",
            "8126/8126 [==============================] - 0s 36us/step - loss: 0.4053 - val_loss: 0.3886\n",
            "Epoch 9/40\n",
            "8126/8126 [==============================] - 0s 37us/step - loss: 0.3995 - val_loss: 0.3845\n",
            "Epoch 10/40\n",
            "8126/8126 [==============================] - 0s 38us/step - loss: 0.3939 - val_loss: 0.3764\n",
            "Epoch 11/40\n",
            "8126/8126 [==============================] - 0s 36us/step - loss: 0.3886 - val_loss: 0.3726\n",
            "Epoch 12/40\n",
            "8126/8126 [==============================] - 0s 39us/step - loss: 0.3870 - val_loss: 0.3701\n",
            "Epoch 13/40\n",
            "8126/8126 [==============================] - 0s 36us/step - loss: 0.3808 - val_loss: 0.3653\n",
            "Epoch 14/40\n",
            "8126/8126 [==============================] - 0s 36us/step - loss: 0.3812 - val_loss: 0.3634\n",
            "Epoch 15/40\n",
            "8126/8126 [==============================] - 0s 36us/step - loss: 0.3714 - val_loss: 0.3610\n",
            "Epoch 16/40\n",
            "8126/8126 [==============================] - 0s 40us/step - loss: 0.3709 - val_loss: 0.3613\n",
            "Epoch 17/40\n",
            "8126/8126 [==============================] - 0s 36us/step - loss: 0.3695 - val_loss: 0.3573\n",
            "Epoch 18/40\n",
            "8126/8126 [==============================] - 0s 37us/step - loss: 0.3628 - val_loss: 0.3622\n",
            "Epoch 19/40\n",
            "8126/8126 [==============================] - 0s 38us/step - loss: 0.3604 - val_loss: 0.3540\n",
            "Epoch 20/40\n",
            "8126/8126 [==============================] - 0s 36us/step - loss: 0.3568 - val_loss: 0.3497\n",
            "Epoch 21/40\n",
            "8126/8126 [==============================] - 0s 36us/step - loss: 0.3693 - val_loss: 0.3563\n",
            "Epoch 22/40\n",
            "8126/8126 [==============================] - 0s 37us/step - loss: 0.3561 - val_loss: 0.3476\n",
            "Epoch 23/40\n",
            "8126/8126 [==============================] - 0s 36us/step - loss: 0.3471 - val_loss: 0.3447\n",
            "Epoch 24/40\n",
            "8126/8126 [==============================] - 0s 36us/step - loss: 0.3462 - val_loss: 0.3490\n",
            "Epoch 25/40\n",
            "8126/8126 [==============================] - 0s 36us/step - loss: 0.3450 - val_loss: 0.3394\n",
            "Epoch 26/40\n",
            "8126/8126 [==============================] - 0s 37us/step - loss: 0.3426 - val_loss: 0.3356\n",
            "Epoch 27/40\n",
            "8126/8126 [==============================] - 0s 37us/step - loss: 0.3406 - val_loss: 0.3350\n",
            "Epoch 28/40\n",
            "8126/8126 [==============================] - 0s 36us/step - loss: 0.3471 - val_loss: 0.3356\n",
            "Epoch 29/40\n",
            "8126/8126 [==============================] - 0s 38us/step - loss: 0.3365 - val_loss: 0.3409\n",
            "Epoch 30/40\n",
            "8126/8126 [==============================] - 0s 36us/step - loss: 0.3403 - val_loss: 0.3294\n",
            "Epoch 31/40\n",
            "8126/8126 [==============================] - 0s 36us/step - loss: 0.3362 - val_loss: 0.3326\n",
            "Epoch 32/40\n",
            "8126/8126 [==============================] - 0s 36us/step - loss: 0.3302 - val_loss: 0.3449\n",
            "Epoch 33/40\n",
            "8126/8126 [==============================] - 0s 36us/step - loss: 0.3301 - val_loss: 0.3314\n",
            "Epoch 34/40\n",
            "8126/8126 [==============================] - 0s 36us/step - loss: 0.3279 - val_loss: 0.3251\n",
            "Epoch 35/40\n",
            "8126/8126 [==============================] - 0s 36us/step - loss: 0.3262 - val_loss: 0.3234\n",
            "Epoch 36/40\n",
            "8126/8126 [==============================] - 0s 37us/step - loss: 0.3354 - val_loss: 0.3288\n",
            "Epoch 37/40\n",
            "8126/8126 [==============================] - 0s 35us/step - loss: 0.3300 - val_loss: 0.3242\n",
            "Epoch 38/40\n",
            "8126/8126 [==============================] - 0s 36us/step - loss: 0.3233 - val_loss: 0.3315\n",
            "Epoch 39/40\n",
            "8126/8126 [==============================] - 0s 37us/step - loss: 0.3228 - val_loss: 0.3226\n",
            "Epoch 40/40\n",
            "8126/8126 [==============================] - 0s 36us/step - loss: 0.3222 - val_loss: 0.3227\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 31%|███       | 11/36 [02:12<05:03, 12.12s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "{'activation1': <function relu at 0x7f4d33213598>, 'first_hidden_layer': 16, 'optimizer': 'adam', 'second_hidden_layer': 8}\n",
            "Train on 8126 samples, validate on 3870 samples\n",
            "Epoch 1/40\n",
            "8126/8126 [==============================] - 0s 48us/step - loss: 2.4032 - val_loss: 0.9047\n",
            "Epoch 2/40\n",
            "8126/8126 [==============================] - 0s 37us/step - loss: 0.7174 - val_loss: 0.6255\n",
            "Epoch 3/40\n",
            "8126/8126 [==============================] - 0s 38us/step - loss: 0.5890 - val_loss: 0.5446\n",
            "Epoch 4/40\n",
            "8126/8126 [==============================] - 0s 38us/step - loss: 0.5303 - val_loss: 0.4982\n",
            "Epoch 5/40\n",
            "8126/8126 [==============================] - 0s 37us/step - loss: 0.4877 - val_loss: 0.4629\n",
            "Epoch 6/40\n",
            "8126/8126 [==============================] - 0s 37us/step - loss: 0.4559 - val_loss: 0.4382\n",
            "Epoch 7/40\n",
            "8126/8126 [==============================] - 0s 37us/step - loss: 0.4337 - val_loss: 0.4165\n",
            "Epoch 8/40\n",
            "8126/8126 [==============================] - 0s 39us/step - loss: 0.4189 - val_loss: 0.4024\n",
            "Epoch 9/40\n",
            "8126/8126 [==============================] - 0s 36us/step - loss: 0.4077 - val_loss: 0.3939\n",
            "Epoch 10/40\n",
            "8126/8126 [==============================] - 0s 37us/step - loss: 0.3984 - val_loss: 0.3935\n",
            "Epoch 11/40\n",
            "8126/8126 [==============================] - 0s 39us/step - loss: 0.3909 - val_loss: 0.3827\n",
            "Epoch 12/40\n",
            "8126/8126 [==============================] - 0s 36us/step - loss: 0.3849 - val_loss: 0.3803\n",
            "Epoch 13/40\n",
            "8126/8126 [==============================] - 0s 38us/step - loss: 0.3809 - val_loss: 0.3715\n",
            "Epoch 14/40\n",
            "8126/8126 [==============================] - 0s 38us/step - loss: 0.3733 - val_loss: 0.3756\n",
            "Epoch 15/40\n",
            "8126/8126 [==============================] - 0s 37us/step - loss: 0.3697 - val_loss: 0.3647\n",
            "Epoch 16/40\n",
            "8126/8126 [==============================] - 0s 38us/step - loss: 0.3644 - val_loss: 0.3599\n",
            "Epoch 17/40\n",
            "8126/8126 [==============================] - 0s 36us/step - loss: 0.3625 - val_loss: 0.3560\n",
            "Epoch 18/40\n",
            "8126/8126 [==============================] - 0s 38us/step - loss: 0.3579 - val_loss: 0.3558\n",
            "Epoch 19/40\n",
            "8126/8126 [==============================] - 0s 37us/step - loss: 0.3552 - val_loss: 0.3513\n",
            "Epoch 20/40\n",
            "8126/8126 [==============================] - 0s 39us/step - loss: 0.3515 - val_loss: 0.3491\n",
            "Epoch 21/40\n",
            "8126/8126 [==============================] - 0s 39us/step - loss: 0.3491 - val_loss: 0.3448\n",
            "Epoch 22/40\n",
            "8126/8126 [==============================] - 0s 37us/step - loss: 0.3460 - val_loss: 0.3476\n",
            "Epoch 23/40\n",
            "8126/8126 [==============================] - 0s 37us/step - loss: 0.3426 - val_loss: 0.3463\n",
            "Epoch 24/40\n",
            "8126/8126 [==============================] - 0s 37us/step - loss: 0.3400 - val_loss: 0.3376\n",
            "Epoch 25/40\n",
            "8126/8126 [==============================] - 0s 37us/step - loss: 0.3367 - val_loss: 0.3365\n",
            "Epoch 26/40\n",
            "8126/8126 [==============================] - 0s 36us/step - loss: 0.3347 - val_loss: 0.3336\n",
            "Epoch 27/40\n",
            "8126/8126 [==============================] - 0s 36us/step - loss: 0.3325 - val_loss: 0.3336\n",
            "Epoch 28/40\n",
            "8126/8126 [==============================] - 0s 37us/step - loss: 0.3312 - val_loss: 0.3309\n",
            "Epoch 29/40\n",
            "8126/8126 [==============================] - 0s 35us/step - loss: 0.3309 - val_loss: 0.3327\n",
            "Epoch 30/40\n",
            "8126/8126 [==============================] - 0s 37us/step - loss: 0.3270 - val_loss: 0.3290\n",
            "Epoch 31/40\n",
            "8126/8126 [==============================] - 0s 37us/step - loss: 0.3252 - val_loss: 0.3283\n",
            "Epoch 32/40\n",
            "8126/8126 [==============================] - 0s 36us/step - loss: 0.3256 - val_loss: 0.3273\n",
            "Epoch 33/40\n",
            "8126/8126 [==============================] - 0s 37us/step - loss: 0.3228 - val_loss: 0.3238\n",
            "Epoch 34/40\n",
            "8126/8126 [==============================] - 0s 36us/step - loss: 0.3209 - val_loss: 0.3246\n",
            "Epoch 35/40\n",
            "8126/8126 [==============================] - 0s 39us/step - loss: 0.3192 - val_loss: 0.3220\n",
            "Epoch 36/40\n",
            "8126/8126 [==============================] - 0s 36us/step - loss: 0.3184 - val_loss: 0.3221\n",
            "Epoch 37/40\n",
            "8126/8126 [==============================] - 0s 36us/step - loss: 0.3192 - val_loss: 0.3211\n",
            "Epoch 38/40\n",
            "8126/8126 [==============================] - 0s 37us/step - loss: 0.3166 - val_loss: 0.3224\n",
            "Epoch 39/40\n",
            "8126/8126 [==============================] - 0s 35us/step - loss: 0.3160 - val_loss: 0.3197\n",
            "Epoch 40/40\n",
            "8126/8126 [==============================] - 0s 36us/step - loss: 0.3146 - val_loss: 0.3196\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 33%|███▎      | 12/36 [02:24<04:54, 12.28s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "{'activation1': <function relu at 0x7f4d33213598>, 'first_hidden_layer': 8, 'optimizer': 'sgd', 'second_hidden_layer': 16}\n",
            "Train on 8126 samples, validate on 3870 samples\n",
            "Epoch 1/40\n",
            "8126/8126 [==============================] - 0s 38us/step - loss: 0.8354 - val_loss: 0.5305\n",
            "Epoch 2/40\n",
            "8126/8126 [==============================] - 0s 32us/step - loss: 0.5186 - val_loss: 0.4812\n",
            "Epoch 3/40\n",
            "8126/8126 [==============================] - 0s 31us/step - loss: 0.4920 - val_loss: 0.4645\n",
            "Epoch 4/40\n",
            "8126/8126 [==============================] - 0s 33us/step - loss: 0.4757 - val_loss: 0.4656\n",
            "Epoch 5/40\n",
            "8126/8126 [==============================] - 0s 32us/step - loss: 0.4688 - val_loss: 0.4582\n",
            "Epoch 6/40\n",
            "8126/8126 [==============================] - 0s 32us/step - loss: 0.4594 - val_loss: 0.4531\n",
            "Epoch 7/40\n",
            "8126/8126 [==============================] - 0s 31us/step - loss: 0.4552 - val_loss: 0.4382\n",
            "Epoch 8/40\n",
            "8126/8126 [==============================] - 0s 34us/step - loss: 0.4508 - val_loss: 0.4353\n",
            "Epoch 9/40\n",
            "8126/8126 [==============================] - 0s 32us/step - loss: 0.4478 - val_loss: 0.4516\n",
            "Epoch 10/40\n",
            "8126/8126 [==============================] - 0s 31us/step - loss: 0.4433 - val_loss: 0.4287\n",
            "Epoch 11/40\n",
            "8126/8126 [==============================] - 0s 32us/step - loss: 0.4403 - val_loss: 0.4238\n",
            "Epoch 12/40\n",
            "8126/8126 [==============================] - 0s 33us/step - loss: 0.4363 - val_loss: 0.4231\n",
            "Epoch 13/40\n",
            "8126/8126 [==============================] - 0s 35us/step - loss: 0.4298 - val_loss: 0.4262\n",
            "Epoch 14/40\n",
            "8126/8126 [==============================] - 0s 31us/step - loss: 0.4271 - val_loss: 0.4168\n",
            "Epoch 15/40\n",
            "8126/8126 [==============================] - 0s 31us/step - loss: 0.4209 - val_loss: 0.4109\n",
            "Epoch 16/40\n",
            "8126/8126 [==============================] - 0s 33us/step - loss: 0.4183 - val_loss: 0.4076\n",
            "Epoch 17/40\n",
            "8126/8126 [==============================] - 0s 32us/step - loss: 0.4157 - val_loss: 0.4056\n",
            "Epoch 18/40\n",
            "8126/8126 [==============================] - 0s 31us/step - loss: 0.4124 - val_loss: 0.4066\n",
            "Epoch 19/40\n",
            "8126/8126 [==============================] - 0s 33us/step - loss: 0.4067 - val_loss: 0.4053\n",
            "Epoch 20/40\n",
            "8126/8126 [==============================] - 0s 33us/step - loss: 0.4044 - val_loss: 0.3999\n",
            "Epoch 21/40\n",
            "8126/8126 [==============================] - 0s 32us/step - loss: 0.4014 - val_loss: 0.4230\n",
            "Epoch 22/40\n",
            "8126/8126 [==============================] - 0s 31us/step - loss: 0.3991 - val_loss: 0.3930\n",
            "Epoch 23/40\n",
            "8126/8126 [==============================] - 0s 34us/step - loss: 0.3981 - val_loss: 0.3884\n",
            "Epoch 24/40\n",
            "8126/8126 [==============================] - 0s 31us/step - loss: 0.3944 - val_loss: 0.3859\n",
            "Epoch 25/40\n",
            "8126/8126 [==============================] - 0s 32us/step - loss: 0.3929 - val_loss: 0.3924\n",
            "Epoch 26/40\n",
            "8126/8126 [==============================] - 0s 33us/step - loss: 0.3921 - val_loss: 0.3863\n",
            "Epoch 27/40\n",
            "8126/8126 [==============================] - 0s 32us/step - loss: 0.3893 - val_loss: 0.3876\n",
            "Epoch 28/40\n",
            "8126/8126 [==============================] - 0s 31us/step - loss: 0.3878 - val_loss: 0.3844\n",
            "Epoch 29/40\n",
            "8126/8126 [==============================] - 0s 32us/step - loss: 0.3872 - val_loss: 0.3994\n",
            "Epoch 30/40\n",
            "8126/8126 [==============================] - 0s 31us/step - loss: 0.3833 - val_loss: 0.3898\n",
            "Epoch 31/40\n",
            "8126/8126 [==============================] - 0s 32us/step - loss: 0.3827 - val_loss: 0.3890\n",
            "Epoch 32/40\n",
            "8126/8126 [==============================] - 0s 34us/step - loss: 0.3817 - val_loss: 0.3856\n",
            "Epoch 33/40\n",
            "8126/8126 [==============================] - 0s 30us/step - loss: 0.3817 - val_loss: 0.3759\n",
            "Epoch 34/40\n",
            "8126/8126 [==============================] - 0s 30us/step - loss: 0.3804 - val_loss: 0.3861\n",
            "Epoch 35/40\n",
            "8126/8126 [==============================] - 0s 32us/step - loss: 0.3757 - val_loss: 0.3781\n",
            "Epoch 36/40\n",
            "8126/8126 [==============================] - 0s 32us/step - loss: 0.3774 - val_loss: 0.3731\n",
            "Epoch 37/40\n",
            "8126/8126 [==============================] - 0s 31us/step - loss: 0.3734 - val_loss: 0.3729\n",
            "Epoch 38/40\n",
            "8126/8126 [==============================] - 0s 31us/step - loss: 0.3730 - val_loss: 0.3760\n",
            "Epoch 39/40\n",
            "8126/8126 [==============================] - 0s 33us/step - loss: 0.3723 - val_loss: 0.3713\n",
            "Epoch 40/40\n",
            "8126/8126 [==============================] - 0s 32us/step - loss: 0.3709 - val_loss: 0.3741\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 36%|███▌      | 13/36 [02:35<04:32, 11.85s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "{'activation1': <function relu at 0x7f4d33213598>, 'first_hidden_layer': 8, 'optimizer': 'sgd', 'second_hidden_layer': 12}\n",
            "Train on 8126 samples, validate on 3870 samples\n",
            "Epoch 1/40\n",
            "8126/8126 [==============================] - 0s 39us/step - loss: 1.9353 - val_loss: 1.0620\n",
            "Epoch 2/40\n",
            "8126/8126 [==============================] - 0s 32us/step - loss: 0.7450 - val_loss: 0.5718\n",
            "Epoch 3/40\n",
            "8126/8126 [==============================] - 0s 32us/step - loss: 0.5539 - val_loss: 0.5154\n",
            "Epoch 4/40\n",
            "8126/8126 [==============================] - 0s 32us/step - loss: 0.5144 - val_loss: 0.4939\n",
            "Epoch 5/40\n",
            "8126/8126 [==============================] - 0s 34us/step - loss: 0.4928 - val_loss: 0.4773\n",
            "Epoch 6/40\n",
            "8126/8126 [==============================] - 0s 31us/step - loss: 0.4797 - val_loss: 0.4646\n",
            "Epoch 7/40\n",
            "8126/8126 [==============================] - 0s 31us/step - loss: 0.4682 - val_loss: 0.4525\n",
            "Epoch 8/40\n",
            "8126/8126 [==============================] - 0s 35us/step - loss: 0.4579 - val_loss: 0.4439\n",
            "Epoch 9/40\n",
            "8126/8126 [==============================] - 0s 43us/step - loss: 0.4487 - val_loss: 0.4370\n",
            "Epoch 10/40\n",
            "8126/8126 [==============================] - 0s 33us/step - loss: 0.4390 - val_loss: 0.4284\n",
            "Epoch 11/40\n",
            "8126/8126 [==============================] - 0s 31us/step - loss: 0.4286 - val_loss: 0.4208\n",
            "Epoch 12/40\n",
            "8126/8126 [==============================] - 0s 31us/step - loss: 0.4202 - val_loss: 0.4093\n",
            "Epoch 13/40\n",
            "8126/8126 [==============================] - 0s 34us/step - loss: 0.4118 - val_loss: 0.3990\n",
            "Epoch 14/40\n",
            "8126/8126 [==============================] - 0s 32us/step - loss: 0.4043 - val_loss: 0.4107\n",
            "Epoch 15/40\n",
            "8126/8126 [==============================] - 0s 32us/step - loss: 0.3993 - val_loss: 0.3909\n",
            "Epoch 16/40\n",
            "8126/8126 [==============================] - 0s 34us/step - loss: 0.3933 - val_loss: 0.3938\n",
            "Epoch 17/40\n",
            "8126/8126 [==============================] - 0s 32us/step - loss: 0.3873 - val_loss: 0.3861\n",
            "Epoch 18/40\n",
            "8126/8126 [==============================] - 0s 31us/step - loss: 0.3851 - val_loss: 0.3784\n",
            "Epoch 19/40\n",
            "8126/8126 [==============================] - 0s 32us/step - loss: 0.3811 - val_loss: 0.3731\n",
            "Epoch 20/40\n",
            "8126/8126 [==============================] - 0s 33us/step - loss: 0.3768 - val_loss: 0.3757\n",
            "Epoch 21/40\n",
            "8126/8126 [==============================] - 0s 32us/step - loss: 0.3740 - val_loss: 0.3760\n",
            "Epoch 22/40\n",
            "8126/8126 [==============================] - 0s 31us/step - loss: 0.3702 - val_loss: 0.3743\n",
            "Epoch 23/40\n",
            "8126/8126 [==============================] - 0s 31us/step - loss: 0.3672 - val_loss: 0.3691\n",
            "Epoch 24/40\n",
            "8126/8126 [==============================] - 0s 32us/step - loss: 0.3662 - val_loss: 0.3589\n",
            "Epoch 25/40\n",
            "8126/8126 [==============================] - 0s 31us/step - loss: 0.3636 - val_loss: 0.3738\n",
            "Epoch 26/40\n",
            "8126/8126 [==============================] - 0s 32us/step - loss: 0.3609 - val_loss: 0.3578\n",
            "Epoch 27/40\n",
            "8126/8126 [==============================] - 0s 31us/step - loss: 0.3578 - val_loss: 0.3580\n",
            "Epoch 28/40\n",
            "8126/8126 [==============================] - 0s 33us/step - loss: 0.3579 - val_loss: 0.3540\n",
            "Epoch 29/40\n",
            "8126/8126 [==============================] - 0s 34us/step - loss: 0.3547 - val_loss: 0.3531\n",
            "Epoch 30/40\n",
            "8126/8126 [==============================] - 0s 32us/step - loss: 0.3532 - val_loss: 0.3700\n",
            "Epoch 31/40\n",
            "8126/8126 [==============================] - 0s 32us/step - loss: 0.3537 - val_loss: 0.3500\n",
            "Epoch 32/40\n",
            "8126/8126 [==============================] - 0s 32us/step - loss: 0.3509 - val_loss: 0.3540\n",
            "Epoch 33/40\n",
            "8126/8126 [==============================] - 0s 32us/step - loss: 0.3510 - val_loss: 0.3527\n",
            "Epoch 34/40\n",
            "8126/8126 [==============================] - 0s 36us/step - loss: 0.3493 - val_loss: 0.3523\n",
            "Epoch 35/40\n",
            "8126/8126 [==============================] - 0s 31us/step - loss: 0.3470 - val_loss: 0.3575\n",
            "Epoch 36/40\n",
            "8126/8126 [==============================] - 0s 33us/step - loss: 0.3464 - val_loss: 0.3456\n",
            "Epoch 37/40\n",
            "8126/8126 [==============================] - 0s 31us/step - loss: 0.3458 - val_loss: 0.3782\n",
            "Epoch 38/40\n",
            "8126/8126 [==============================] - 0s 31us/step - loss: 0.3461 - val_loss: 0.3510\n",
            "Epoch 39/40\n",
            "8126/8126 [==============================] - 0s 32us/step - loss: 0.3435 - val_loss: 0.3509\n",
            "Epoch 40/40\n",
            "8126/8126 [==============================] - 0s 34us/step - loss: 0.3439 - val_loss: 0.3465\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 39%|███▉      | 14/36 [02:46<04:15, 11.59s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "{'activation1': <function relu at 0x7f4d33213598>, 'first_hidden_layer': 8, 'optimizer': 'sgd', 'second_hidden_layer': 8}\n",
            "Train on 8126 samples, validate on 3870 samples\n",
            "Epoch 1/40\n",
            "8126/8126 [==============================] - 0s 38us/step - loss: 2.0778 - val_loss: 0.8399\n",
            "Epoch 2/40\n",
            "8126/8126 [==============================] - 0s 33us/step - loss: 1.5666 - val_loss: 0.5867\n",
            "Epoch 3/40\n",
            "8126/8126 [==============================] - 0s 31us/step - loss: 1.2226 - val_loss: 0.5682\n",
            "Epoch 4/40\n",
            "8126/8126 [==============================] - 0s 32us/step - loss: 0.5432 - val_loss: 0.4947\n",
            "Epoch 5/40\n",
            "8126/8126 [==============================] - 0s 31us/step - loss: 0.4962 - val_loss: 0.4655\n",
            "Epoch 6/40\n",
            "8126/8126 [==============================] - 0s 36us/step - loss: 0.4726 - val_loss: 0.4484\n",
            "Epoch 7/40\n",
            "8126/8126 [==============================] - 0s 34us/step - loss: 0.4569 - val_loss: 0.4414\n",
            "Epoch 8/40\n",
            "8126/8126 [==============================] - 0s 31us/step - loss: 0.4484 - val_loss: 0.4330\n",
            "Epoch 9/40\n",
            "8126/8126 [==============================] - 0s 33us/step - loss: 0.4419 - val_loss: 0.4270\n",
            "Epoch 10/40\n",
            "8126/8126 [==============================] - 0s 33us/step - loss: 0.4359 - val_loss: 0.4199\n",
            "Epoch 11/40\n",
            "8126/8126 [==============================] - 0s 32us/step - loss: 0.4314 - val_loss: 0.4218\n",
            "Epoch 12/40\n",
            "8126/8126 [==============================] - 0s 32us/step - loss: 0.4273 - val_loss: 0.4144\n",
            "Epoch 13/40\n",
            "8126/8126 [==============================] - 0s 32us/step - loss: 0.4235 - val_loss: 0.4118\n",
            "Epoch 14/40\n",
            "8126/8126 [==============================] - 0s 33us/step - loss: 0.4192 - val_loss: 0.4091\n",
            "Epoch 15/40\n",
            "8126/8126 [==============================] - 0s 32us/step - loss: 0.4157 - val_loss: 0.4048\n",
            "Epoch 16/40\n",
            "8126/8126 [==============================] - 0s 32us/step - loss: 0.4120 - val_loss: 0.4043\n",
            "Epoch 17/40\n",
            "8126/8126 [==============================] - 0s 32us/step - loss: 0.4089 - val_loss: 0.4023\n",
            "Epoch 18/40\n",
            "8126/8126 [==============================] - 0s 32us/step - loss: 0.4051 - val_loss: 0.3948\n",
            "Epoch 19/40\n",
            "8126/8126 [==============================] - 0s 31us/step - loss: 0.4010 - val_loss: 0.4080\n",
            "Epoch 20/40\n",
            "8126/8126 [==============================] - 0s 31us/step - loss: 0.3973 - val_loss: 0.3955\n",
            "Epoch 21/40\n",
            "8126/8126 [==============================] - 0s 34us/step - loss: 0.3942 - val_loss: 0.3854\n",
            "Epoch 22/40\n",
            "8126/8126 [==============================] - 0s 31us/step - loss: 0.3904 - val_loss: 0.3885\n",
            "Epoch 23/40\n",
            "8126/8126 [==============================] - 0s 32us/step - loss: 0.3874 - val_loss: 0.3833\n",
            "Epoch 24/40\n",
            "8126/8126 [==============================] - 0s 31us/step - loss: 0.3849 - val_loss: 0.3915\n",
            "Epoch 25/40\n",
            "8126/8126 [==============================] - 0s 32us/step - loss: 0.3830 - val_loss: 0.3827\n",
            "Epoch 26/40\n",
            "8126/8126 [==============================] - 0s 30us/step - loss: 0.3818 - val_loss: 0.3804\n",
            "Epoch 27/40\n",
            "8126/8126 [==============================] - 0s 33us/step - loss: 0.3804 - val_loss: 0.3733\n",
            "Epoch 28/40\n",
            "8126/8126 [==============================] - 0s 32us/step - loss: 0.3788 - val_loss: 0.3737\n",
            "Epoch 29/40\n",
            "8126/8126 [==============================] - 0s 32us/step - loss: 0.3775 - val_loss: 0.3811\n",
            "Epoch 30/40\n",
            "8126/8126 [==============================] - 0s 32us/step - loss: 0.3761 - val_loss: 0.3721\n",
            "Epoch 31/40\n",
            "8126/8126 [==============================] - 0s 31us/step - loss: 0.3727 - val_loss: 0.3753\n",
            "Epoch 32/40\n",
            "8126/8126 [==============================] - 0s 32us/step - loss: 0.3715 - val_loss: 0.3780\n",
            "Epoch 33/40\n",
            "8126/8126 [==============================] - 0s 32us/step - loss: 0.3714 - val_loss: 0.3738\n",
            "Epoch 34/40\n",
            "8126/8126 [==============================] - 0s 32us/step - loss: 0.3696 - val_loss: 0.3734\n",
            "Epoch 35/40\n",
            "8126/8126 [==============================] - 0s 31us/step - loss: 0.3675 - val_loss: 0.3638\n",
            "Epoch 36/40\n",
            "8126/8126 [==============================] - 0s 33us/step - loss: 0.3669 - val_loss: 0.3726\n",
            "Epoch 37/40\n",
            "8126/8126 [==============================] - 0s 34us/step - loss: 0.3629 - val_loss: 0.3733\n",
            "Epoch 38/40\n",
            "8126/8126 [==============================] - 0s 32us/step - loss: 0.3621 - val_loss: 0.3627\n",
            "Epoch 39/40\n",
            "8126/8126 [==============================] - 0s 32us/step - loss: 0.3608 - val_loss: 0.3613\n",
            "Epoch 40/40\n",
            "8126/8126 [==============================] - 0s 33us/step - loss: 0.3588 - val_loss: 0.3649\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 42%|████▏     | 15/36 [02:57<03:58, 11.38s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "{'activation1': <function relu at 0x7f4d33213598>, 'first_hidden_layer': 8, 'optimizer': 'adam', 'second_hidden_layer': 16}\n",
            "Train on 8126 samples, validate on 3870 samples\n",
            "Epoch 1/40\n",
            "8126/8126 [==============================] - 0s 45us/step - loss: 2.0383 - val_loss: 1.0212\n",
            "Epoch 2/40\n",
            "8126/8126 [==============================] - 0s 35us/step - loss: 0.8155 - val_loss: 0.7157\n",
            "Epoch 3/40\n",
            "8126/8126 [==============================] - 0s 34us/step - loss: 0.6508 - val_loss: 0.5968\n",
            "Epoch 4/40\n",
            "8126/8126 [==============================] - 0s 34us/step - loss: 0.5811 - val_loss: 0.5326\n",
            "Epoch 5/40\n",
            "8126/8126 [==============================] - 0s 34us/step - loss: 0.5261 - val_loss: 0.4828\n",
            "Epoch 6/40\n",
            "8126/8126 [==============================] - 0s 37us/step - loss: 0.4814 - val_loss: 0.4486\n",
            "Epoch 7/40\n",
            "8126/8126 [==============================] - 0s 35us/step - loss: 0.4510 - val_loss: 0.4295\n",
            "Epoch 8/40\n",
            "8126/8126 [==============================] - 0s 34us/step - loss: 0.4330 - val_loss: 0.4131\n",
            "Epoch 9/40\n",
            "8126/8126 [==============================] - 0s 37us/step - loss: 0.4204 - val_loss: 0.4039\n",
            "Epoch 10/40\n",
            "8126/8126 [==============================] - 0s 35us/step - loss: 0.4113 - val_loss: 0.3960\n",
            "Epoch 11/40\n",
            "8126/8126 [==============================] - 0s 34us/step - loss: 0.4059 - val_loss: 0.3930\n",
            "Epoch 12/40\n",
            "8126/8126 [==============================] - 0s 35us/step - loss: 0.3999 - val_loss: 0.3865\n",
            "Epoch 13/40\n",
            "8126/8126 [==============================] - 0s 35us/step - loss: 0.3954 - val_loss: 0.3875\n",
            "Epoch 14/40\n",
            "8126/8126 [==============================] - 0s 33us/step - loss: 0.3922 - val_loss: 0.3788\n",
            "Epoch 15/40\n",
            "8126/8126 [==============================] - 0s 35us/step - loss: 0.3884 - val_loss: 0.3829\n",
            "Epoch 16/40\n",
            "8126/8126 [==============================] - 0s 34us/step - loss: 0.3842 - val_loss: 0.3761\n",
            "Epoch 17/40\n",
            "8126/8126 [==============================] - 0s 35us/step - loss: 0.3802 - val_loss: 0.3731\n",
            "Epoch 18/40\n",
            "8126/8126 [==============================] - 0s 35us/step - loss: 0.3764 - val_loss: 0.3696\n",
            "Epoch 19/40\n",
            "8126/8126 [==============================] - 0s 34us/step - loss: 0.3743 - val_loss: 0.3714\n",
            "Epoch 20/40\n",
            "8126/8126 [==============================] - 0s 36us/step - loss: 0.3710 - val_loss: 0.3652\n",
            "Epoch 21/40\n",
            "8126/8126 [==============================] - 0s 35us/step - loss: 0.3681 - val_loss: 0.3637\n",
            "Epoch 22/40\n",
            "8126/8126 [==============================] - 0s 36us/step - loss: 0.3670 - val_loss: 0.3590\n",
            "Epoch 23/40\n",
            "8126/8126 [==============================] - 0s 35us/step - loss: 0.3635 - val_loss: 0.3616\n",
            "Epoch 24/40\n",
            "8126/8126 [==============================] - 0s 38us/step - loss: 0.3611 - val_loss: 0.3533\n",
            "Epoch 25/40\n",
            "8126/8126 [==============================] - 0s 36us/step - loss: 0.3590 - val_loss: 0.3535\n",
            "Epoch 26/40\n",
            "8126/8126 [==============================] - 0s 36us/step - loss: 0.3563 - val_loss: 0.3485\n",
            "Epoch 27/40\n",
            "8126/8126 [==============================] - 0s 36us/step - loss: 0.3529 - val_loss: 0.3464\n",
            "Epoch 28/40\n",
            "8126/8126 [==============================] - 0s 34us/step - loss: 0.3502 - val_loss: 0.3517\n",
            "Epoch 29/40\n",
            "8126/8126 [==============================] - 0s 36us/step - loss: 0.3475 - val_loss: 0.3437\n",
            "Epoch 30/40\n",
            "8126/8126 [==============================] - 0s 35us/step - loss: 0.3459 - val_loss: 0.3403\n",
            "Epoch 31/40\n",
            "8126/8126 [==============================] - 0s 36us/step - loss: 0.3436 - val_loss: 0.3458\n",
            "Epoch 32/40\n",
            "8126/8126 [==============================] - 0s 35us/step - loss: 0.3430 - val_loss: 0.3403\n",
            "Epoch 33/40\n",
            "8126/8126 [==============================] - 0s 36us/step - loss: 0.3401 - val_loss: 0.3350\n",
            "Epoch 34/40\n",
            "8126/8126 [==============================] - 0s 36us/step - loss: 0.3376 - val_loss: 0.3389\n",
            "Epoch 35/40\n",
            "8126/8126 [==============================] - 0s 36us/step - loss: 0.3379 - val_loss: 0.3334\n",
            "Epoch 36/40\n",
            "8126/8126 [==============================] - 0s 35us/step - loss: 0.3365 - val_loss: 0.3412\n",
            "Epoch 37/40\n",
            "8126/8126 [==============================] - 0s 37us/step - loss: 0.3358 - val_loss: 0.3318\n",
            "Epoch 38/40\n",
            "8126/8126 [==============================] - 0s 36us/step - loss: 0.3331 - val_loss: 0.3304\n",
            "Epoch 39/40\n",
            "8126/8126 [==============================] - 0s 34us/step - loss: 0.3312 - val_loss: 0.3328\n",
            "Epoch 40/40\n",
            "8126/8126 [==============================] - 0s 35us/step - loss: 0.3330 - val_loss: 0.3273\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 44%|████▍     | 16/36 [03:09<03:51, 11.57s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "{'activation1': <function relu at 0x7f4d33213598>, 'first_hidden_layer': 8, 'optimizer': 'adam', 'second_hidden_layer': 12}\n",
            "Train on 8126 samples, validate on 3870 samples\n",
            "Epoch 1/40\n",
            "8126/8126 [==============================] - 0s 44us/step - loss: 3.1211 - val_loss: 1.0874\n",
            "Epoch 2/40\n",
            "8126/8126 [==============================] - 0s 36us/step - loss: 1.7739 - val_loss: 0.7992\n",
            "Epoch 3/40\n",
            "8126/8126 [==============================] - 0s 35us/step - loss: 1.2215 - val_loss: 0.5860\n",
            "Epoch 4/40\n",
            "8126/8126 [==============================] - 0s 35us/step - loss: 0.7801 - val_loss: 0.4929\n",
            "Epoch 5/40\n",
            "8126/8126 [==============================] - 0s 35us/step - loss: 0.5759 - val_loss: 0.4538\n",
            "Epoch 6/40\n",
            "8126/8126 [==============================] - 0s 36us/step - loss: 0.4952 - val_loss: 0.4407\n",
            "Epoch 7/40\n",
            "8126/8126 [==============================] - 0s 36us/step - loss: 0.4574 - val_loss: 0.4331\n",
            "Epoch 8/40\n",
            "8126/8126 [==============================] - 0s 35us/step - loss: 0.4439 - val_loss: 0.4256\n",
            "Epoch 9/40\n",
            "8126/8126 [==============================] - 0s 36us/step - loss: 0.4360 - val_loss: 0.4193\n",
            "Epoch 10/40\n",
            "8126/8126 [==============================] - 0s 35us/step - loss: 0.4304 - val_loss: 0.4174\n",
            "Epoch 11/40\n",
            "8126/8126 [==============================] - 0s 34us/step - loss: 0.4264 - val_loss: 0.4136\n",
            "Epoch 12/40\n",
            "8126/8126 [==============================] - 0s 36us/step - loss: 0.4229 - val_loss: 0.4099\n",
            "Epoch 13/40\n",
            "8126/8126 [==============================] - 0s 35us/step - loss: 0.4194 - val_loss: 0.4062\n",
            "Epoch 14/40\n",
            "8126/8126 [==============================] - 0s 35us/step - loss: 0.4165 - val_loss: 0.4042\n",
            "Epoch 15/40\n",
            "8126/8126 [==============================] - 0s 36us/step - loss: 0.4132 - val_loss: 0.4025\n",
            "Epoch 16/40\n",
            "8126/8126 [==============================] - 0s 35us/step - loss: 0.4105 - val_loss: 0.4000\n",
            "Epoch 17/40\n",
            "8126/8126 [==============================] - 0s 36us/step - loss: 0.4084 - val_loss: 0.3995\n",
            "Epoch 18/40\n",
            "8126/8126 [==============================] - 0s 35us/step - loss: 0.4061 - val_loss: 0.3963\n",
            "Epoch 19/40\n",
            "8126/8126 [==============================] - 0s 34us/step - loss: 0.4033 - val_loss: 0.3970\n",
            "Epoch 20/40\n",
            "8126/8126 [==============================] - 0s 34us/step - loss: 0.4005 - val_loss: 0.3941\n",
            "Epoch 21/40\n",
            "8126/8126 [==============================] - 0s 36us/step - loss: 0.3992 - val_loss: 0.3906\n",
            "Epoch 22/40\n",
            "8126/8126 [==============================] - 0s 35us/step - loss: 0.3987 - val_loss: 0.3871\n",
            "Epoch 23/40\n",
            "8126/8126 [==============================] - 0s 38us/step - loss: 0.3953 - val_loss: 0.3865\n",
            "Epoch 24/40\n",
            "8126/8126 [==============================] - 0s 37us/step - loss: 0.3926 - val_loss: 0.3853\n",
            "Epoch 25/40\n",
            "8126/8126 [==============================] - 0s 34us/step - loss: 0.3914 - val_loss: 0.3853\n",
            "Epoch 26/40\n",
            "8126/8126 [==============================] - 0s 36us/step - loss: 0.3953 - val_loss: 0.3847\n",
            "Epoch 27/40\n",
            "8126/8126 [==============================] - 0s 35us/step - loss: 0.3868 - val_loss: 0.3811\n",
            "Epoch 28/40\n",
            "8126/8126 [==============================] - 0s 35us/step - loss: 0.3857 - val_loss: 0.3783\n",
            "Epoch 29/40\n",
            "8126/8126 [==============================] - 0s 35us/step - loss: 0.3826 - val_loss: 0.3766\n",
            "Epoch 30/40\n",
            "8126/8126 [==============================] - 0s 35us/step - loss: 0.3823 - val_loss: 0.3781\n",
            "Epoch 31/40\n",
            "8126/8126 [==============================] - 0s 37us/step - loss: 0.3799 - val_loss: 0.3740\n",
            "Epoch 32/40\n",
            "8126/8126 [==============================] - 0s 35us/step - loss: 0.3807 - val_loss: 0.3764\n",
            "Epoch 33/40\n",
            "8126/8126 [==============================] - 0s 34us/step - loss: 0.3761 - val_loss: 0.3718\n",
            "Epoch 34/40\n",
            "8126/8126 [==============================] - 0s 35us/step - loss: 0.3746 - val_loss: 0.3744\n",
            "Epoch 35/40\n",
            "8126/8126 [==============================] - 0s 34us/step - loss: 0.3733 - val_loss: 0.3733\n",
            "Epoch 36/40\n",
            "8126/8126 [==============================] - 0s 35us/step - loss: 0.3730 - val_loss: 0.3694\n",
            "Epoch 37/40\n",
            "8126/8126 [==============================] - 0s 35us/step - loss: 0.3691 - val_loss: 0.3722\n",
            "Epoch 38/40\n",
            "8126/8126 [==============================] - 0s 36us/step - loss: 0.3681 - val_loss: 0.3683\n",
            "Epoch 39/40\n",
            "8126/8126 [==============================] - 0s 34us/step - loss: 0.3662 - val_loss: 0.3630\n",
            "Epoch 40/40\n",
            "8126/8126 [==============================] - 0s 33us/step - loss: 0.3631 - val_loss: 0.3640\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 47%|████▋     | 17/36 [03:21<03:42, 11.71s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "{'activation1': <function relu at 0x7f4d33213598>, 'first_hidden_layer': 8, 'optimizer': 'adam', 'second_hidden_layer': 8}\n",
            "Train on 8126 samples, validate on 3870 samples\n",
            "Epoch 1/40\n",
            "8126/8126 [==============================] - 0s 46us/step - loss: 1.2703 - val_loss: 0.8303\n",
            "Epoch 2/40\n",
            "8126/8126 [==============================] - 0s 37us/step - loss: 0.7323 - val_loss: 0.6522\n",
            "Epoch 3/40\n",
            "8126/8126 [==============================] - 0s 37us/step - loss: 0.6133 - val_loss: 0.5743\n",
            "Epoch 4/40\n",
            "8126/8126 [==============================] - 0s 34us/step - loss: 0.5523 - val_loss: 0.5267\n",
            "Epoch 5/40\n",
            "8126/8126 [==============================] - 0s 35us/step - loss: 0.5122 - val_loss: 0.4909\n",
            "Epoch 6/40\n",
            "8126/8126 [==============================] - 0s 37us/step - loss: 0.4817 - val_loss: 0.4707\n",
            "Epoch 7/40\n",
            "8126/8126 [==============================] - 0s 37us/step - loss: 0.4605 - val_loss: 0.4475\n",
            "Epoch 8/40\n",
            "8126/8126 [==============================] - 0s 37us/step - loss: 0.4450 - val_loss: 0.4332\n",
            "Epoch 9/40\n",
            "8126/8126 [==============================] - 0s 35us/step - loss: 0.4342 - val_loss: 0.4221\n",
            "Epoch 10/40\n",
            "8126/8126 [==============================] - 0s 35us/step - loss: 0.4238 - val_loss: 0.4135\n",
            "Epoch 11/40\n",
            "8126/8126 [==============================] - 0s 35us/step - loss: 0.4166 - val_loss: 0.4064\n",
            "Epoch 12/40\n",
            "8126/8126 [==============================] - 0s 34us/step - loss: 0.4085 - val_loss: 0.4024\n",
            "Epoch 13/40\n",
            "8126/8126 [==============================] - 0s 36us/step - loss: 0.4053 - val_loss: 0.3926\n",
            "Epoch 14/40\n",
            "8126/8126 [==============================] - 0s 35us/step - loss: 0.3998 - val_loss: 0.3874\n",
            "Epoch 15/40\n",
            "8126/8126 [==============================] - 0s 34us/step - loss: 0.3946 - val_loss: 0.3825\n",
            "Epoch 16/40\n",
            "8126/8126 [==============================] - 0s 35us/step - loss: 0.3901 - val_loss: 0.3768\n",
            "Epoch 17/40\n",
            "8126/8126 [==============================] - 0s 35us/step - loss: 0.3849 - val_loss: 0.3725\n",
            "Epoch 18/40\n",
            "8126/8126 [==============================] - 0s 34us/step - loss: 0.3827 - val_loss: 0.3678\n",
            "Epoch 19/40\n",
            "8126/8126 [==============================] - 0s 36us/step - loss: 0.3779 - val_loss: 0.3620\n",
            "Epoch 20/40\n",
            "8126/8126 [==============================] - 0s 35us/step - loss: 0.3714 - val_loss: 0.3636\n",
            "Epoch 21/40\n",
            "8126/8126 [==============================] - 0s 35us/step - loss: 0.3685 - val_loss: 0.3594\n",
            "Epoch 22/40\n",
            "8126/8126 [==============================] - 0s 34us/step - loss: 0.3651 - val_loss: 0.3545\n",
            "Epoch 23/40\n",
            "8126/8126 [==============================] - 0s 36us/step - loss: 0.3619 - val_loss: 0.3551\n",
            "Epoch 24/40\n",
            "8126/8126 [==============================] - 0s 36us/step - loss: 0.3612 - val_loss: 0.3524\n",
            "Epoch 25/40\n",
            "8126/8126 [==============================] - 0s 35us/step - loss: 0.3587 - val_loss: 0.3498\n",
            "Epoch 26/40\n",
            "8126/8126 [==============================] - 0s 35us/step - loss: 0.3559 - val_loss: 0.3478\n",
            "Epoch 27/40\n",
            "8126/8126 [==============================] - 0s 35us/step - loss: 0.3545 - val_loss: 0.3472\n",
            "Epoch 28/40\n",
            "8126/8126 [==============================] - 0s 35us/step - loss: 0.3527 - val_loss: 0.3445\n",
            "Epoch 29/40\n",
            "8126/8126 [==============================] - 0s 34us/step - loss: 0.3519 - val_loss: 0.3475\n",
            "Epoch 30/40\n",
            "8126/8126 [==============================] - 0s 34us/step - loss: 0.3493 - val_loss: 0.3427\n",
            "Epoch 31/40\n",
            "8126/8126 [==============================] - 0s 36us/step - loss: 0.3471 - val_loss: 0.3457\n",
            "Epoch 32/40\n",
            "8126/8126 [==============================] - 0s 35us/step - loss: 0.3474 - val_loss: 0.3419\n",
            "Epoch 33/40\n",
            "8126/8126 [==============================] - 0s 35us/step - loss: 0.3441 - val_loss: 0.3468\n",
            "Epoch 34/40\n",
            "8126/8126 [==============================] - 0s 34us/step - loss: 0.3438 - val_loss: 0.3424\n",
            "Epoch 35/40\n",
            "8126/8126 [==============================] - 0s 36us/step - loss: 0.3433 - val_loss: 0.3379\n",
            "Epoch 36/40\n",
            "8126/8126 [==============================] - 0s 34us/step - loss: 0.3409 - val_loss: 0.3397\n",
            "Epoch 37/40\n",
            "8126/8126 [==============================] - 0s 35us/step - loss: 0.3394 - val_loss: 0.3382\n",
            "Epoch 38/40\n",
            "8126/8126 [==============================] - 0s 36us/step - loss: 0.3386 - val_loss: 0.3379\n",
            "Epoch 39/40\n",
            "8126/8126 [==============================] - 0s 35us/step - loss: 0.3359 - val_loss: 0.3370\n",
            "Epoch 40/40\n",
            "8126/8126 [==============================] - 0s 36us/step - loss: 0.3368 - val_loss: 0.3354\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 50%|█████     | 18/36 [03:33<03:32, 11.81s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "{'activation1': <function elu at 0x7f4d33558e18>, 'first_hidden_layer': 30, 'optimizer': 'sgd', 'second_hidden_layer': 16}\n",
            "Train on 8126 samples, validate on 3870 samples\n",
            "Epoch 1/40\n",
            "8126/8126 [==============================] - 0s 40us/step - loss: 0.7387 - val_loss: 0.5012\n",
            "Epoch 2/40\n",
            "8126/8126 [==============================] - 0s 35us/step - loss: 0.5109 - val_loss: 0.4659\n",
            "Epoch 3/40\n",
            "8126/8126 [==============================] - 0s 35us/step - loss: 0.4756 - val_loss: 0.4574\n",
            "Epoch 4/40\n",
            "8126/8126 [==============================] - 0s 35us/step - loss: 0.4618 - val_loss: 0.4433\n",
            "Epoch 5/40\n",
            "8126/8126 [==============================] - 0s 35us/step - loss: 0.4517 - val_loss: 0.4526\n",
            "Epoch 6/40\n",
            "8126/8126 [==============================] - 0s 34us/step - loss: 0.4437 - val_loss: 0.4287\n",
            "Epoch 7/40\n",
            "8126/8126 [==============================] - 0s 33us/step - loss: 0.4369 - val_loss: 0.4337\n",
            "Epoch 8/40\n",
            "8126/8126 [==============================] - 0s 34us/step - loss: 0.4311 - val_loss: 0.4231\n",
            "Epoch 9/40\n",
            "8126/8126 [==============================] - 0s 33us/step - loss: 0.4246 - val_loss: 0.4218\n",
            "Epoch 10/40\n",
            "8126/8126 [==============================] - 0s 32us/step - loss: 0.4201 - val_loss: 0.4191\n",
            "Epoch 11/40\n",
            "8126/8126 [==============================] - 0s 35us/step - loss: 0.4204 - val_loss: 0.4082\n",
            "Epoch 12/40\n",
            "8126/8126 [==============================] - 0s 32us/step - loss: 0.4162 - val_loss: 0.4051\n",
            "Epoch 13/40\n",
            "8126/8126 [==============================] - 0s 34us/step - loss: 0.4098 - val_loss: 0.4708\n",
            "Epoch 14/40\n",
            "8126/8126 [==============================] - 0s 33us/step - loss: 0.4098 - val_loss: 0.4045\n",
            "Epoch 15/40\n",
            "8126/8126 [==============================] - 0s 34us/step - loss: 0.4056 - val_loss: 0.4039\n",
            "Epoch 16/40\n",
            "8126/8126 [==============================] - 0s 34us/step - loss: 0.4038 - val_loss: 0.3933\n",
            "Epoch 17/40\n",
            "8126/8126 [==============================] - 0s 33us/step - loss: 0.4010 - val_loss: 0.3993\n",
            "Epoch 18/40\n",
            "8126/8126 [==============================] - 0s 33us/step - loss: 0.4096 - val_loss: 0.3925\n",
            "Epoch 19/40\n",
            "8126/8126 [==============================] - 0s 37us/step - loss: 0.3992 - val_loss: 0.3889\n",
            "Epoch 20/40\n",
            "8126/8126 [==============================] - 0s 33us/step - loss: 0.3953 - val_loss: 0.4010\n",
            "Epoch 21/40\n",
            "8126/8126 [==============================] - 0s 33us/step - loss: 0.3937 - val_loss: 0.3891\n",
            "Epoch 22/40\n",
            "8126/8126 [==============================] - 0s 34us/step - loss: 0.3914 - val_loss: 0.4001\n",
            "Epoch 23/40\n",
            "8126/8126 [==============================] - 0s 33us/step - loss: 0.3909 - val_loss: 0.3936\n",
            "Epoch 24/40\n",
            "8126/8126 [==============================] - 0s 34us/step - loss: 0.3902 - val_loss: 0.3803\n",
            "Epoch 25/40\n",
            "8126/8126 [==============================] - 0s 32us/step - loss: 0.3883 - val_loss: 0.3880\n",
            "Epoch 26/40\n",
            "8126/8126 [==============================] - 0s 32us/step - loss: 0.3880 - val_loss: 0.3868\n",
            "Epoch 27/40\n",
            "8126/8126 [==============================] - 0s 35us/step - loss: 0.3852 - val_loss: 0.3854\n",
            "Epoch 28/40\n",
            "8126/8126 [==============================] - 0s 33us/step - loss: 0.3847 - val_loss: 0.3774\n",
            "Epoch 29/40\n",
            "8126/8126 [==============================] - 0s 33us/step - loss: 0.3835 - val_loss: 0.3805\n",
            "Epoch 30/40\n",
            "8126/8126 [==============================] - 0s 33us/step - loss: 0.3826 - val_loss: 0.3778\n",
            "Epoch 31/40\n",
            "8126/8126 [==============================] - 0s 33us/step - loss: 0.3814 - val_loss: 0.3759\n",
            "Epoch 32/40\n",
            "8126/8126 [==============================] - 0s 33us/step - loss: 0.3808 - val_loss: 0.3752\n",
            "Epoch 33/40\n",
            "8126/8126 [==============================] - 0s 34us/step - loss: 0.3793 - val_loss: 0.3787\n",
            "Epoch 34/40\n",
            "8126/8126 [==============================] - 0s 34us/step - loss: 0.3853 - val_loss: 0.3810\n",
            "Epoch 35/40\n",
            "8126/8126 [==============================] - 0s 33us/step - loss: 0.3763 - val_loss: 0.3726\n",
            "Epoch 36/40\n",
            "8126/8126 [==============================] - 0s 32us/step - loss: 0.3827 - val_loss: 0.3771\n",
            "Epoch 37/40\n",
            "8126/8126 [==============================] - 0s 34us/step - loss: 0.3769 - val_loss: 0.3874\n",
            "Epoch 38/40\n",
            "8126/8126 [==============================] - 0s 34us/step - loss: 0.3746 - val_loss: 0.3723\n",
            "Epoch 39/40\n",
            "8126/8126 [==============================] - 0s 32us/step - loss: 0.3746 - val_loss: 0.3659\n",
            "Epoch 40/40\n",
            "8126/8126 [==============================] - 0s 34us/step - loss: 0.3735 - val_loss: 0.3772\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 53%|█████▎    | 19/36 [03:44<03:18, 11.67s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "{'activation1': <function elu at 0x7f4d33558e18>, 'first_hidden_layer': 30, 'optimizer': 'sgd', 'second_hidden_layer': 12}\n",
            "Train on 8126 samples, validate on 3870 samples\n",
            "Epoch 1/40\n",
            "8126/8126 [==============================] - 0s 39us/step - loss: 0.7147 - val_loss: 0.4703\n",
            "Epoch 2/40\n",
            "8126/8126 [==============================] - 0s 32us/step - loss: 0.4816 - val_loss: 0.4500\n",
            "Epoch 3/40\n",
            "8126/8126 [==============================] - 0s 31us/step - loss: 0.4587 - val_loss: 0.4407\n",
            "Epoch 4/40\n",
            "8126/8126 [==============================] - 0s 32us/step - loss: 0.4469 - val_loss: 0.4312\n",
            "Epoch 5/40\n",
            "8126/8126 [==============================] - 0s 33us/step - loss: 0.4396 - val_loss: 0.4220\n",
            "Epoch 6/40\n",
            "8126/8126 [==============================] - 0s 34us/step - loss: 0.4336 - val_loss: 0.4217\n",
            "Epoch 7/40\n",
            "8126/8126 [==============================] - 0s 32us/step - loss: 0.4270 - val_loss: 0.4142\n",
            "Epoch 8/40\n",
            "8126/8126 [==============================] - 0s 33us/step - loss: 0.4235 - val_loss: 0.4161\n",
            "Epoch 9/40\n",
            "8126/8126 [==============================] - 0s 32us/step - loss: 0.4203 - val_loss: 0.4074\n",
            "Epoch 10/40\n",
            "8126/8126 [==============================] - 0s 35us/step - loss: 0.4159 - val_loss: 0.4163\n",
            "Epoch 11/40\n",
            "8126/8126 [==============================] - 0s 34us/step - loss: 0.4125 - val_loss: 0.4055\n",
            "Epoch 12/40\n",
            "8126/8126 [==============================] - 0s 33us/step - loss: 0.4109 - val_loss: 0.4029\n",
            "Epoch 13/40\n",
            "8126/8126 [==============================] - 0s 32us/step - loss: 0.4076 - val_loss: 0.3967\n",
            "Epoch 14/40\n",
            "8126/8126 [==============================] - 0s 32us/step - loss: 0.4049 - val_loss: 0.3994\n",
            "Epoch 15/40\n",
            "8126/8126 [==============================] - 0s 33us/step - loss: 0.4021 - val_loss: 0.3918\n",
            "Epoch 16/40\n",
            "8126/8126 [==============================] - 0s 36us/step - loss: 0.3990 - val_loss: 0.3906\n",
            "Epoch 17/40\n",
            "8126/8126 [==============================] - 0s 31us/step - loss: 0.3976 - val_loss: 0.3904\n",
            "Epoch 18/40\n",
            "8126/8126 [==============================] - 0s 32us/step - loss: 0.3964 - val_loss: 0.3885\n",
            "Epoch 19/40\n",
            "8126/8126 [==============================] - 0s 32us/step - loss: 0.3944 - val_loss: 0.3890\n",
            "Epoch 20/40\n",
            "8126/8126 [==============================] - 0s 33us/step - loss: 0.3921 - val_loss: 0.3875\n",
            "Epoch 21/40\n",
            "8126/8126 [==============================] - 0s 33us/step - loss: 0.3906 - val_loss: 0.3801\n",
            "Epoch 22/40\n",
            "8126/8126 [==============================] - 0s 34us/step - loss: 0.3891 - val_loss: 0.3828\n",
            "Epoch 23/40\n",
            "8126/8126 [==============================] - 0s 33us/step - loss: 0.3873 - val_loss: 0.3796\n",
            "Epoch 24/40\n",
            "8126/8126 [==============================] - 0s 33us/step - loss: 0.3855 - val_loss: 0.3774\n",
            "Epoch 25/40\n",
            "8126/8126 [==============================] - 0s 33us/step - loss: 0.3847 - val_loss: 0.3803\n",
            "Epoch 26/40\n",
            "8126/8126 [==============================] - 0s 33us/step - loss: 0.3841 - val_loss: 0.3764\n",
            "Epoch 27/40\n",
            "8126/8126 [==============================] - 0s 32us/step - loss: 0.3815 - val_loss: 0.3757\n",
            "Epoch 28/40\n",
            "8126/8126 [==============================] - 0s 32us/step - loss: 0.3801 - val_loss: 0.3856\n",
            "Epoch 29/40\n",
            "8126/8126 [==============================] - 0s 32us/step - loss: 0.3780 - val_loss: 0.3809\n",
            "Epoch 30/40\n",
            "8126/8126 [==============================] - 0s 32us/step - loss: 0.3784 - val_loss: 0.3849\n",
            "Epoch 31/40\n",
            "8126/8126 [==============================] - 0s 34us/step - loss: 0.3761 - val_loss: 0.3754\n",
            "Epoch 32/40\n",
            "8126/8126 [==============================] - 0s 33us/step - loss: 0.3746 - val_loss: 0.3705\n",
            "Epoch 33/40\n",
            "8126/8126 [==============================] - 0s 34us/step - loss: 0.3735 - val_loss: 0.3792\n",
            "Epoch 34/40\n",
            "8126/8126 [==============================] - 0s 32us/step - loss: 0.3693 - val_loss: 0.3797\n",
            "Epoch 35/40\n",
            "8126/8126 [==============================] - 0s 34us/step - loss: 0.3698 - val_loss: 0.3714\n",
            "Epoch 36/40\n",
            "8126/8126 [==============================] - 0s 37us/step - loss: 0.3695 - val_loss: 0.3677\n",
            "Epoch 37/40\n",
            "8126/8126 [==============================] - 0s 33us/step - loss: 0.3691 - val_loss: 0.3674\n",
            "Epoch 38/40\n",
            "8126/8126 [==============================] - 0s 34us/step - loss: 0.3676 - val_loss: 0.3656\n",
            "Epoch 39/40\n",
            "8126/8126 [==============================] - 0s 36us/step - loss: 0.3667 - val_loss: 0.3698\n",
            "Epoch 40/40\n",
            "8126/8126 [==============================] - 0s 32us/step - loss: 0.3664 - val_loss: 0.3708\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 56%|█████▌    | 20/36 [03:56<03:04, 11.51s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "{'activation1': <function elu at 0x7f4d33558e18>, 'first_hidden_layer': 30, 'optimizer': 'sgd', 'second_hidden_layer': 8}\n",
            "Train on 8126 samples, validate on 3870 samples\n",
            "Epoch 1/40\n",
            "8126/8126 [==============================] - 0s 38us/step - loss: 0.6768 - val_loss: 0.4866\n",
            "Epoch 2/40\n",
            "8126/8126 [==============================] - 0s 32us/step - loss: 0.4904 - val_loss: 0.4628\n",
            "Epoch 3/40\n",
            "8126/8126 [==============================] - 0s 32us/step - loss: 0.4674 - val_loss: 0.4534\n",
            "Epoch 4/40\n",
            "8126/8126 [==============================] - 0s 32us/step - loss: 0.4587 - val_loss: 0.4446\n",
            "Epoch 5/40\n",
            "8126/8126 [==============================] - 0s 32us/step - loss: 0.4485 - val_loss: 0.4372\n",
            "Epoch 6/40\n",
            "8126/8126 [==============================] - 0s 31us/step - loss: 0.4419 - val_loss: 0.4326\n",
            "Epoch 7/40\n",
            "8126/8126 [==============================] - 0s 34us/step - loss: 0.4339 - val_loss: 0.4194\n",
            "Epoch 8/40\n",
            "8126/8126 [==============================] - 0s 32us/step - loss: 0.4274 - val_loss: 0.4316\n",
            "Epoch 9/40\n",
            "8126/8126 [==============================] - 0s 31us/step - loss: 0.4224 - val_loss: 0.4102\n",
            "Epoch 10/40\n",
            "8126/8126 [==============================] - 0s 32us/step - loss: 0.4184 - val_loss: 0.4068\n",
            "Epoch 11/40\n",
            "8126/8126 [==============================] - 0s 33us/step - loss: 0.4152 - val_loss: 0.4136\n",
            "Epoch 12/40\n",
            "8126/8126 [==============================] - 0s 33us/step - loss: 0.4087 - val_loss: 0.4214\n",
            "Epoch 13/40\n",
            "8126/8126 [==============================] - 0s 31us/step - loss: 0.4069 - val_loss: 0.4030\n",
            "Epoch 14/40\n",
            "8126/8126 [==============================] - 0s 31us/step - loss: 0.4038 - val_loss: 0.3958\n",
            "Epoch 15/40\n",
            "8126/8126 [==============================] - 0s 32us/step - loss: 0.4008 - val_loss: 0.4003\n",
            "Epoch 16/40\n",
            "8126/8126 [==============================] - 0s 32us/step - loss: 0.3977 - val_loss: 0.3886\n",
            "Epoch 17/40\n",
            "8126/8126 [==============================] - 0s 31us/step - loss: 0.3967 - val_loss: 0.3874\n",
            "Epoch 18/40\n",
            "8126/8126 [==============================] - 0s 31us/step - loss: 0.3942 - val_loss: 0.3850\n",
            "Epoch 19/40\n",
            "8126/8126 [==============================] - 0s 32us/step - loss: 0.3922 - val_loss: 0.3852\n",
            "Epoch 20/40\n",
            "8126/8126 [==============================] - 0s 32us/step - loss: 0.3918 - val_loss: 0.3852\n",
            "Epoch 21/40\n",
            "8126/8126 [==============================] - 0s 32us/step - loss: 0.3904 - val_loss: 0.3829\n",
            "Epoch 22/40\n",
            "8126/8126 [==============================] - 0s 31us/step - loss: 0.3894 - val_loss: 0.3806\n",
            "Epoch 23/40\n",
            "8126/8126 [==============================] - 0s 32us/step - loss: 0.3859 - val_loss: 0.3834\n",
            "Epoch 24/40\n",
            "8126/8126 [==============================] - 0s 33us/step - loss: 0.3852 - val_loss: 0.3821\n",
            "Epoch 25/40\n",
            "8126/8126 [==============================] - 0s 31us/step - loss: 0.3839 - val_loss: 0.3781\n",
            "Epoch 26/40\n",
            "8126/8126 [==============================] - 0s 31us/step - loss: 0.3825 - val_loss: 0.3764\n",
            "Epoch 27/40\n",
            "8126/8126 [==============================] - 0s 32us/step - loss: 0.3823 - val_loss: 0.3838\n",
            "Epoch 28/40\n",
            "8126/8126 [==============================] - 0s 31us/step - loss: 0.3799 - val_loss: 0.3740\n",
            "Epoch 29/40\n",
            "8126/8126 [==============================] - 0s 31us/step - loss: 0.3770 - val_loss: 0.3839\n",
            "Epoch 30/40\n",
            "8126/8126 [==============================] - 0s 32us/step - loss: 0.3779 - val_loss: 0.3725\n",
            "Epoch 31/40\n",
            "8126/8126 [==============================] - 0s 32us/step - loss: 0.3767 - val_loss: 0.3878\n",
            "Epoch 32/40\n",
            "8126/8126 [==============================] - 0s 32us/step - loss: 0.3765 - val_loss: 0.3759\n",
            "Epoch 33/40\n",
            "8126/8126 [==============================] - 0s 33us/step - loss: 0.3755 - val_loss: 0.3753\n",
            "Epoch 34/40\n",
            "8126/8126 [==============================] - 0s 35us/step - loss: 0.3747 - val_loss: 0.3687\n",
            "Epoch 35/40\n",
            "8126/8126 [==============================] - 0s 32us/step - loss: 0.3718 - val_loss: 0.3695\n",
            "Epoch 36/40\n",
            "8126/8126 [==============================] - 0s 33us/step - loss: 0.3709 - val_loss: 0.3715\n",
            "Epoch 37/40\n",
            "8126/8126 [==============================] - 0s 32us/step - loss: 0.3712 - val_loss: 0.3708\n",
            "Epoch 38/40\n",
            "8126/8126 [==============================] - 0s 31us/step - loss: 0.3899 - val_loss: 0.3834\n",
            "Epoch 39/40\n",
            "8126/8126 [==============================] - 0s 30us/step - loss: 0.3725 - val_loss: 0.3696\n",
            "Epoch 40/40\n",
            "8126/8126 [==============================] - 0s 32us/step - loss: 0.3685 - val_loss: 0.3635\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 58%|█████▊    | 21/36 [04:06<02:49, 11.29s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "{'activation1': <function elu at 0x7f4d33558e18>, 'first_hidden_layer': 30, 'optimizer': 'adam', 'second_hidden_layer': 16}\n",
            "Train on 8126 samples, validate on 3870 samples\n",
            "Epoch 1/40\n",
            "8126/8126 [==============================] - 0s 46us/step - loss: 1.2906 - val_loss: 0.5801\n",
            "Epoch 2/40\n",
            "8126/8126 [==============================] - 0s 34us/step - loss: 0.5272 - val_loss: 0.4679\n",
            "Epoch 3/40\n",
            "8126/8126 [==============================] - 0s 34us/step - loss: 0.4708 - val_loss: 0.4545\n",
            "Epoch 4/40\n",
            "8126/8126 [==============================] - 0s 36us/step - loss: 0.4540 - val_loss: 0.4456\n",
            "Epoch 5/40\n",
            "8126/8126 [==============================] - 0s 35us/step - loss: 0.4419 - val_loss: 0.4235\n",
            "Epoch 6/40\n",
            "8126/8126 [==============================] - 0s 35us/step - loss: 0.4330 - val_loss: 0.4128\n",
            "Epoch 7/40\n",
            "8126/8126 [==============================] - 0s 35us/step - loss: 0.4267 - val_loss: 0.4064\n",
            "Epoch 8/40\n",
            "8126/8126 [==============================] - 0s 34us/step - loss: 0.4198 - val_loss: 0.4027\n",
            "Epoch 9/40\n",
            "8126/8126 [==============================] - 0s 35us/step - loss: 0.4106 - val_loss: 0.3960\n",
            "Epoch 10/40\n",
            "8126/8126 [==============================] - 0s 33us/step - loss: 0.4069 - val_loss: 0.4057\n",
            "Epoch 11/40\n",
            "8126/8126 [==============================] - 0s 35us/step - loss: 0.4031 - val_loss: 0.3938\n",
            "Epoch 12/40\n",
            "8126/8126 [==============================] - 0s 35us/step - loss: 0.4109 - val_loss: 0.3853\n",
            "Epoch 13/40\n",
            "8126/8126 [==============================] - 0s 35us/step - loss: 0.3972 - val_loss: 0.3822\n",
            "Epoch 14/40\n",
            "8126/8126 [==============================] - 0s 35us/step - loss: 0.3903 - val_loss: 0.3814\n",
            "Epoch 15/40\n",
            "8126/8126 [==============================] - 0s 34us/step - loss: 0.3869 - val_loss: 0.3758\n",
            "Epoch 16/40\n",
            "8126/8126 [==============================] - 0s 36us/step - loss: 0.3826 - val_loss: 0.3716\n",
            "Epoch 17/40\n",
            "8126/8126 [==============================] - 0s 34us/step - loss: 0.3811 - val_loss: 0.3698\n",
            "Epoch 18/40\n",
            "8126/8126 [==============================] - 0s 35us/step - loss: 0.3794 - val_loss: 0.3707\n",
            "Epoch 19/40\n",
            "8126/8126 [==============================] - 0s 34us/step - loss: 0.3787 - val_loss: 0.3680\n",
            "Epoch 20/40\n",
            "8126/8126 [==============================] - 0s 35us/step - loss: 0.3774 - val_loss: 0.3667\n",
            "Epoch 21/40\n",
            "8126/8126 [==============================] - 0s 34us/step - loss: 0.3716 - val_loss: 0.3628\n",
            "Epoch 22/40\n",
            "8126/8126 [==============================] - 0s 35us/step - loss: 0.3672 - val_loss: 0.3566\n",
            "Epoch 23/40\n",
            "8126/8126 [==============================] - 0s 37us/step - loss: 0.3655 - val_loss: 0.3517\n",
            "Epoch 24/40\n",
            "8126/8126 [==============================] - 0s 33us/step - loss: 0.3651 - val_loss: 0.3587\n",
            "Epoch 25/40\n",
            "8126/8126 [==============================] - 0s 36us/step - loss: 0.3638 - val_loss: 0.3539\n",
            "Epoch 26/40\n",
            "8126/8126 [==============================] - 0s 34us/step - loss: 0.3598 - val_loss: 0.3509\n",
            "Epoch 27/40\n",
            "8126/8126 [==============================] - 0s 34us/step - loss: 0.3582 - val_loss: 0.3466\n",
            "Epoch 28/40\n",
            "8126/8126 [==============================] - 0s 33us/step - loss: 0.3579 - val_loss: 0.3534\n",
            "Epoch 29/40\n",
            "8126/8126 [==============================] - 0s 37us/step - loss: 0.3563 - val_loss: 0.3595\n",
            "Epoch 30/40\n",
            "8126/8126 [==============================] - 0s 36us/step - loss: 0.3509 - val_loss: 0.3447\n",
            "Epoch 31/40\n",
            "8126/8126 [==============================] - 0s 34us/step - loss: 0.3495 - val_loss: 0.3468\n",
            "Epoch 32/40\n",
            "8126/8126 [==============================] - 0s 34us/step - loss: 0.3489 - val_loss: 0.3399\n",
            "Epoch 33/40\n",
            "8126/8126 [==============================] - 0s 36us/step - loss: 0.3571 - val_loss: 0.3484\n",
            "Epoch 34/40\n",
            "8126/8126 [==============================] - 0s 34us/step - loss: 0.3458 - val_loss: 0.3378\n",
            "Epoch 35/40\n",
            "8126/8126 [==============================] - 0s 35us/step - loss: 0.3415 - val_loss: 0.3337\n",
            "Epoch 36/40\n",
            "8126/8126 [==============================] - 0s 35us/step - loss: 0.3412 - val_loss: 0.3351\n",
            "Epoch 37/40\n",
            "8126/8126 [==============================] - 0s 35us/step - loss: 0.3385 - val_loss: 0.3327\n",
            "Epoch 38/40\n",
            "8126/8126 [==============================] - 0s 35us/step - loss: 0.3388 - val_loss: 0.3419\n",
            "Epoch 39/40\n",
            "8126/8126 [==============================] - 0s 34us/step - loss: 0.3372 - val_loss: 0.3338\n",
            "Epoch 40/40\n",
            "8126/8126 [==============================] - 0s 35us/step - loss: 0.3340 - val_loss: 0.3286\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 61%|██████    | 22/36 [04:18<02:40, 11.47s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "{'activation1': <function elu at 0x7f4d33558e18>, 'first_hidden_layer': 30, 'optimizer': 'adam', 'second_hidden_layer': 12}\n",
            "Train on 8126 samples, validate on 3870 samples\n",
            "Epoch 1/40\n",
            "8126/8126 [==============================] - 0s 47us/step - loss: 1.3685 - val_loss: 0.5393\n",
            "Epoch 2/40\n",
            "8126/8126 [==============================] - 0s 38us/step - loss: 0.5017 - val_loss: 0.4604\n",
            "Epoch 3/40\n",
            "8126/8126 [==============================] - 0s 35us/step - loss: 0.4562 - val_loss: 0.4423\n",
            "Epoch 4/40\n",
            "8126/8126 [==============================] - 0s 37us/step - loss: 0.4403 - val_loss: 0.4404\n",
            "Epoch 5/40\n",
            "8126/8126 [==============================] - 0s 36us/step - loss: 0.4322 - val_loss: 0.4208\n",
            "Epoch 6/40\n",
            "8126/8126 [==============================] - 0s 36us/step - loss: 0.4245 - val_loss: 0.4177\n",
            "Epoch 7/40\n",
            "8126/8126 [==============================] - 0s 36us/step - loss: 0.4197 - val_loss: 0.4079\n",
            "Epoch 8/40\n",
            "8126/8126 [==============================] - 0s 36us/step - loss: 0.4127 - val_loss: 0.4139\n",
            "Epoch 9/40\n",
            "8126/8126 [==============================] - 0s 37us/step - loss: 0.4101 - val_loss: 0.4008\n",
            "Epoch 10/40\n",
            "8126/8126 [==============================] - 0s 36us/step - loss: 0.4050 - val_loss: 0.3963\n",
            "Epoch 11/40\n",
            "8126/8126 [==============================] - 0s 37us/step - loss: 0.4004 - val_loss: 0.3900\n",
            "Epoch 12/40\n",
            "8126/8126 [==============================] - 0s 37us/step - loss: 0.3972 - val_loss: 0.3856\n",
            "Epoch 13/40\n",
            "8126/8126 [==============================] - 0s 36us/step - loss: 0.3918 - val_loss: 0.3839\n",
            "Epoch 14/40\n",
            "8126/8126 [==============================] - 0s 36us/step - loss: 0.3895 - val_loss: 0.3822\n",
            "Epoch 15/40\n",
            "8126/8126 [==============================] - 0s 37us/step - loss: 0.3848 - val_loss: 0.3793\n",
            "Epoch 16/40\n",
            "8126/8126 [==============================] - 0s 37us/step - loss: 0.3852 - val_loss: 0.3741\n",
            "Epoch 17/40\n",
            "8126/8126 [==============================] - 0s 35us/step - loss: 0.3815 - val_loss: 0.3787\n",
            "Epoch 18/40\n",
            "8126/8126 [==============================] - 0s 37us/step - loss: 0.3785 - val_loss: 0.3710\n",
            "Epoch 19/40\n",
            "8126/8126 [==============================] - 0s 37us/step - loss: 0.3779 - val_loss: 0.3698\n",
            "Epoch 20/40\n",
            "8126/8126 [==============================] - 0s 36us/step - loss: 0.3747 - val_loss: 0.3651\n",
            "Epoch 21/40\n",
            "8126/8126 [==============================] - 0s 37us/step - loss: 0.3722 - val_loss: 0.3696\n",
            "Epoch 22/40\n",
            "8126/8126 [==============================] - 0s 39us/step - loss: 0.3706 - val_loss: 0.3642\n",
            "Epoch 23/40\n",
            "8126/8126 [==============================] - 0s 36us/step - loss: 0.3703 - val_loss: 0.3671\n",
            "Epoch 24/40\n",
            "8126/8126 [==============================] - 0s 36us/step - loss: 0.3677 - val_loss: 0.3652\n",
            "Epoch 25/40\n",
            "8126/8126 [==============================] - 0s 37us/step - loss: 0.3659 - val_loss: 0.3658\n",
            "Epoch 26/40\n",
            "8126/8126 [==============================] - 0s 37us/step - loss: 0.3649 - val_loss: 0.3583\n",
            "Epoch 27/40\n",
            "8126/8126 [==============================] - 0s 36us/step - loss: 0.3630 - val_loss: 0.3580\n",
            "Epoch 28/40\n",
            "8126/8126 [==============================] - 0s 38us/step - loss: 0.3614 - val_loss: 0.3518\n",
            "Epoch 29/40\n",
            "8126/8126 [==============================] - 0s 38us/step - loss: 0.3586 - val_loss: 0.3545\n",
            "Epoch 30/40\n",
            "8126/8126 [==============================] - 0s 36us/step - loss: 0.3590 - val_loss: 0.3514\n",
            "Epoch 31/40\n",
            "8126/8126 [==============================] - 0s 39us/step - loss: 0.3557 - val_loss: 0.3535\n",
            "Epoch 32/40\n",
            "8126/8126 [==============================] - 0s 37us/step - loss: 0.3549 - val_loss: 0.3528\n",
            "Epoch 33/40\n",
            "8126/8126 [==============================] - 0s 37us/step - loss: 0.3528 - val_loss: 0.3598\n",
            "Epoch 34/40\n",
            "8126/8126 [==============================] - 0s 36us/step - loss: 0.3508 - val_loss: 0.3562\n",
            "Epoch 35/40\n",
            "8126/8126 [==============================] - 0s 38us/step - loss: 0.3498 - val_loss: 0.3468\n",
            "Epoch 36/40\n",
            "8126/8126 [==============================] - 0s 37us/step - loss: 0.3472 - val_loss: 0.3422\n",
            "Epoch 37/40\n",
            "8126/8126 [==============================] - 0s 38us/step - loss: 0.3487 - val_loss: 0.3445\n",
            "Epoch 38/40\n",
            "8126/8126 [==============================] - 0s 37us/step - loss: 0.3469 - val_loss: 0.3394\n",
            "Epoch 39/40\n",
            "8126/8126 [==============================] - 0s 38us/step - loss: 0.3449 - val_loss: 0.3459\n",
            "Epoch 40/40\n",
            "8126/8126 [==============================] - 0s 37us/step - loss: 0.3413 - val_loss: 0.3418\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 64%|██████▍   | 23/36 [04:31<02:33, 11.80s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "{'activation1': <function elu at 0x7f4d33558e18>, 'first_hidden_layer': 30, 'optimizer': 'adam', 'second_hidden_layer': 8}\n",
            "Train on 8126 samples, validate on 3870 samples\n",
            "Epoch 1/40\n",
            "8126/8126 [==============================] - 0s 47us/step - loss: 1.3130 - val_loss: 0.5392\n",
            "Epoch 2/40\n",
            "8126/8126 [==============================] - 0s 36us/step - loss: 0.5191 - val_loss: 0.4866\n",
            "Epoch 3/40\n",
            "8126/8126 [==============================] - 0s 37us/step - loss: 0.4732 - val_loss: 0.4593\n",
            "Epoch 4/40\n",
            "8126/8126 [==============================] - 0s 37us/step - loss: 0.4556 - val_loss: 0.4469\n",
            "Epoch 5/40\n",
            "8126/8126 [==============================] - 0s 36us/step - loss: 0.4466 - val_loss: 0.4351\n",
            "Epoch 6/40\n",
            "8126/8126 [==============================] - 0s 37us/step - loss: 0.4375 - val_loss: 0.4297\n",
            "Epoch 7/40\n",
            "8126/8126 [==============================] - 0s 36us/step - loss: 0.4304 - val_loss: 0.4276\n",
            "Epoch 8/40\n",
            "8126/8126 [==============================] - 0s 37us/step - loss: 0.4245 - val_loss: 0.4247\n",
            "Epoch 9/40\n",
            "8126/8126 [==============================] - 0s 36us/step - loss: 0.4219 - val_loss: 0.4268\n",
            "Epoch 10/40\n",
            "8126/8126 [==============================] - 0s 37us/step - loss: 0.4159 - val_loss: 0.4072\n",
            "Epoch 11/40\n",
            "8126/8126 [==============================] - 0s 35us/step - loss: 0.4120 - val_loss: 0.4063\n",
            "Epoch 12/40\n",
            "8126/8126 [==============================] - 0s 37us/step - loss: 0.4081 - val_loss: 0.4070\n",
            "Epoch 13/40\n",
            "8126/8126 [==============================] - 0s 37us/step - loss: 0.4067 - val_loss: 0.4022\n",
            "Epoch 14/40\n",
            "8126/8126 [==============================] - 0s 38us/step - loss: 0.4044 - val_loss: 0.3993\n",
            "Epoch 15/40\n",
            "8126/8126 [==============================] - 0s 36us/step - loss: 0.3995 - val_loss: 0.3927\n",
            "Epoch 16/40\n",
            "8126/8126 [==============================] - 0s 38us/step - loss: 0.3978 - val_loss: 0.3914\n",
            "Epoch 17/40\n",
            "8126/8126 [==============================] - 0s 36us/step - loss: 0.3947 - val_loss: 0.3867\n",
            "Epoch 18/40\n",
            "8126/8126 [==============================] - 0s 37us/step - loss: 0.3937 - val_loss: 0.3854\n",
            "Epoch 19/40\n",
            "8126/8126 [==============================] - 0s 39us/step - loss: 0.3909 - val_loss: 0.3885\n",
            "Epoch 20/40\n",
            "8126/8126 [==============================] - 0s 36us/step - loss: 0.3877 - val_loss: 0.3824\n",
            "Epoch 21/40\n",
            "8126/8126 [==============================] - 0s 37us/step - loss: 0.3842 - val_loss: 0.3841\n",
            "Epoch 22/40\n",
            "8126/8126 [==============================] - 0s 37us/step - loss: 0.3840 - val_loss: 0.3795\n",
            "Epoch 23/40\n",
            "8126/8126 [==============================] - 0s 37us/step - loss: 0.3811 - val_loss: 0.3826\n",
            "Epoch 24/40\n",
            "8126/8126 [==============================] - 0s 37us/step - loss: 0.3804 - val_loss: 0.3722\n",
            "Epoch 25/40\n",
            "8126/8126 [==============================] - 0s 38us/step - loss: 0.3790 - val_loss: 0.3691\n",
            "Epoch 26/40\n",
            "8126/8126 [==============================] - 0s 37us/step - loss: 0.3759 - val_loss: 0.3762\n",
            "Epoch 27/40\n",
            "8126/8126 [==============================] - 0s 36us/step - loss: 0.3740 - val_loss: 0.3674\n",
            "Epoch 28/40\n",
            "8126/8126 [==============================] - 0s 41us/step - loss: 0.3707 - val_loss: 0.3668\n",
            "Epoch 29/40\n",
            "8126/8126 [==============================] - 0s 40us/step - loss: 0.3705 - val_loss: 0.3651\n",
            "Epoch 30/40\n",
            "8126/8126 [==============================] - 0s 39us/step - loss: 0.3661 - val_loss: 0.3652\n",
            "Epoch 31/40\n",
            "8126/8126 [==============================] - 0s 40us/step - loss: 0.3681 - val_loss: 0.3644\n",
            "Epoch 32/40\n",
            "8126/8126 [==============================] - 0s 40us/step - loss: 0.3663 - val_loss: 0.3746\n",
            "Epoch 33/40\n",
            "8126/8126 [==============================] - 0s 39us/step - loss: 0.3653 - val_loss: 0.3636\n",
            "Epoch 34/40\n",
            "8126/8126 [==============================] - 0s 40us/step - loss: 0.3626 - val_loss: 0.3627\n",
            "Epoch 35/40\n",
            "8126/8126 [==============================] - 0s 39us/step - loss: 0.3605 - val_loss: 0.3578\n",
            "Epoch 36/40\n",
            "8126/8126 [==============================] - 0s 38us/step - loss: 0.3595 - val_loss: 0.3582\n",
            "Epoch 37/40\n",
            "8126/8126 [==============================] - 0s 40us/step - loss: 0.3602 - val_loss: 0.3565\n",
            "Epoch 38/40\n",
            "8126/8126 [==============================] - 0s 38us/step - loss: 0.3565 - val_loss: 0.3557\n",
            "Epoch 39/40\n",
            "8126/8126 [==============================] - 0s 39us/step - loss: 0.3566 - val_loss: 0.3591\n",
            "Epoch 40/40\n",
            "8126/8126 [==============================] - 0s 38us/step - loss: 0.3555 - val_loss: 0.3596\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 67%|██████▋   | 24/36 [04:44<02:25, 12.11s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "{'activation1': <function elu at 0x7f4d33558e18>, 'first_hidden_layer': 16, 'optimizer': 'sgd', 'second_hidden_layer': 16}\n",
            "Train on 8126 samples, validate on 3870 samples\n",
            "Epoch 1/40\n",
            "8126/8126 [==============================] - 0s 43us/step - loss: 1.1235 - val_loss: 0.5304\n",
            "Epoch 2/40\n",
            "8126/8126 [==============================] - 0s 32us/step - loss: 0.7073 - val_loss: 0.4924\n",
            "Epoch 3/40\n",
            "8126/8126 [==============================] - 0s 33us/step - loss: 0.4936 - val_loss: 0.4719\n",
            "Epoch 4/40\n",
            "8126/8126 [==============================] - 0s 33us/step - loss: 0.4793 - val_loss: 0.4668\n",
            "Epoch 5/40\n",
            "8126/8126 [==============================] - 0s 32us/step - loss: 0.4690 - val_loss: 0.4603\n",
            "Epoch 6/40\n",
            "8126/8126 [==============================] - 0s 33us/step - loss: 0.4598 - val_loss: 0.4459\n",
            "Epoch 7/40\n",
            "8126/8126 [==============================] - 0s 32us/step - loss: 0.4517 - val_loss: 0.4435\n",
            "Epoch 8/40\n",
            "8126/8126 [==============================] - 0s 32us/step - loss: 0.4450 - val_loss: 0.4344\n",
            "Epoch 9/40\n",
            "8126/8126 [==============================] - 0s 31us/step - loss: 0.4399 - val_loss: 0.4372\n",
            "Epoch 10/40\n",
            "8126/8126 [==============================] - 0s 32us/step - loss: 0.4349 - val_loss: 0.4283\n",
            "Epoch 11/40\n",
            "8126/8126 [==============================] - 0s 32us/step - loss: 0.4304 - val_loss: 0.4253\n",
            "Epoch 12/40\n",
            "8126/8126 [==============================] - 0s 31us/step - loss: 0.4277 - val_loss: 0.4207\n",
            "Epoch 13/40\n",
            "8126/8126 [==============================] - 0s 32us/step - loss: 0.4245 - val_loss: 0.4239\n",
            "Epoch 14/40\n",
            "8126/8126 [==============================] - 0s 31us/step - loss: 0.4225 - val_loss: 0.4144\n",
            "Epoch 15/40\n",
            "8126/8126 [==============================] - 0s 33us/step - loss: 0.4202 - val_loss: 0.4135\n",
            "Epoch 16/40\n",
            "8126/8126 [==============================] - 0s 31us/step - loss: 0.4187 - val_loss: 0.4129\n",
            "Epoch 17/40\n",
            "8126/8126 [==============================] - 0s 31us/step - loss: 0.4167 - val_loss: 0.4107\n",
            "Epoch 18/40\n",
            "8126/8126 [==============================] - 0s 30us/step - loss: 0.4143 - val_loss: 0.4166\n",
            "Epoch 19/40\n",
            "8126/8126 [==============================] - 0s 32us/step - loss: 0.4135 - val_loss: 0.4089\n",
            "Epoch 20/40\n",
            "8126/8126 [==============================] - 0s 32us/step - loss: 0.4106 - val_loss: 0.4037\n",
            "Epoch 21/40\n",
            "8126/8126 [==============================] - 0s 32us/step - loss: 0.4098 - val_loss: 0.4027\n",
            "Epoch 22/40\n",
            "8126/8126 [==============================] - 0s 31us/step - loss: 0.4093 - val_loss: 0.4060\n",
            "Epoch 23/40\n",
            "8126/8126 [==============================] - 0s 31us/step - loss: 0.4068 - val_loss: 0.4131\n",
            "Epoch 24/40\n",
            "8126/8126 [==============================] - 0s 33us/step - loss: 0.4064 - val_loss: 0.4032\n",
            "Epoch 25/40\n",
            "8126/8126 [==============================] - 0s 34us/step - loss: 0.4047 - val_loss: 0.4033\n",
            "Epoch 26/40\n",
            "8126/8126 [==============================] - 0s 32us/step - loss: 0.4039 - val_loss: 0.3972\n",
            "Epoch 27/40\n",
            "8126/8126 [==============================] - 0s 31us/step - loss: 0.4016 - val_loss: 0.3983\n",
            "Epoch 28/40\n",
            "8126/8126 [==============================] - 0s 31us/step - loss: 0.4004 - val_loss: 0.4005\n",
            "Epoch 29/40\n",
            "8126/8126 [==============================] - 0s 32us/step - loss: 0.3993 - val_loss: 0.3974\n",
            "Epoch 30/40\n",
            "8126/8126 [==============================] - 0s 31us/step - loss: 0.3985 - val_loss: 0.3924\n",
            "Epoch 31/40\n",
            "8126/8126 [==============================] - 0s 32us/step - loss: 0.3969 - val_loss: 0.3926\n",
            "Epoch 32/40\n",
            "8126/8126 [==============================] - 0s 31us/step - loss: 0.3965 - val_loss: 0.3955\n",
            "Epoch 33/40\n",
            "8126/8126 [==============================] - 0s 31us/step - loss: 0.3942 - val_loss: 0.3914\n",
            "Epoch 34/40\n",
            "8126/8126 [==============================] - 0s 31us/step - loss: 0.3935 - val_loss: 0.3911\n",
            "Epoch 35/40\n",
            "8126/8126 [==============================] - 0s 31us/step - loss: 0.3930 - val_loss: 0.3950\n",
            "Epoch 36/40\n",
            "8126/8126 [==============================] - 0s 31us/step - loss: 0.3922 - val_loss: 0.3858\n",
            "Epoch 37/40\n",
            "8126/8126 [==============================] - 0s 30us/step - loss: 0.3919 - val_loss: 0.3863\n",
            "Epoch 38/40\n",
            "8126/8126 [==============================] - 0s 31us/step - loss: 0.3902 - val_loss: 0.3851\n",
            "Epoch 39/40\n",
            "8126/8126 [==============================] - 0s 31us/step - loss: 0.3900 - val_loss: 0.3875\n",
            "Epoch 40/40\n",
            "8126/8126 [==============================] - 0s 33us/step - loss: 0.3879 - val_loss: 0.3881\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 69%|██████▉   | 25/36 [04:54<02:08, 11.70s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "{'activation1': <function elu at 0x7f4d33558e18>, 'first_hidden_layer': 16, 'optimizer': 'sgd', 'second_hidden_layer': 12}\n",
            "Train on 8126 samples, validate on 3870 samples\n",
            "Epoch 1/40\n",
            "8126/8126 [==============================] - 0s 39us/step - loss: 0.7223 - val_loss: 0.5097\n",
            "Epoch 2/40\n",
            "8126/8126 [==============================] - 0s 33us/step - loss: 0.5392 - val_loss: 0.4702\n",
            "Epoch 3/40\n",
            "8126/8126 [==============================] - 0s 32us/step - loss: 0.5257 - val_loss: 0.4835\n",
            "Epoch 4/40\n",
            "8126/8126 [==============================] - 0s 35us/step - loss: 0.5056 - val_loss: 0.4497\n",
            "Epoch 5/40\n",
            "8126/8126 [==============================] - 0s 33us/step - loss: 0.4641 - val_loss: 0.4378\n",
            "Epoch 6/40\n",
            "8126/8126 [==============================] - 0s 32us/step - loss: 0.4519 - val_loss: 0.4318\n",
            "Epoch 7/40\n",
            "8126/8126 [==============================] - 0s 33us/step - loss: 0.4425 - val_loss: 0.4261\n",
            "Epoch 8/40\n",
            "8126/8126 [==============================] - 0s 33us/step - loss: 0.4358 - val_loss: 0.4255\n",
            "Epoch 9/40\n",
            "8126/8126 [==============================] - 0s 32us/step - loss: 0.4309 - val_loss: 0.4249\n",
            "Epoch 10/40\n",
            "8126/8126 [==============================] - 0s 33us/step - loss: 0.4244 - val_loss: 0.4166\n",
            "Epoch 11/40\n",
            "8126/8126 [==============================] - 0s 32us/step - loss: 0.4200 - val_loss: 0.4167\n",
            "Epoch 12/40\n",
            "8126/8126 [==============================] - 0s 33us/step - loss: 0.4164 - val_loss: 0.4045\n",
            "Epoch 13/40\n",
            "8126/8126 [==============================] - 0s 32us/step - loss: 0.4107 - val_loss: 0.4044\n",
            "Epoch 14/40\n",
            "8126/8126 [==============================] - 0s 34us/step - loss: 0.4090 - val_loss: 0.3978\n",
            "Epoch 15/40\n",
            "8126/8126 [==============================] - 0s 33us/step - loss: 0.4041 - val_loss: 0.3969\n",
            "Epoch 16/40\n",
            "8126/8126 [==============================] - 0s 34us/step - loss: 0.4014 - val_loss: 0.3945\n",
            "Epoch 17/40\n",
            "8126/8126 [==============================] - 0s 33us/step - loss: 0.3992 - val_loss: 0.3899\n",
            "Epoch 18/40\n",
            "8126/8126 [==============================] - 0s 33us/step - loss: 0.3955 - val_loss: 0.3886\n",
            "Epoch 19/40\n",
            "8126/8126 [==============================] - 0s 32us/step - loss: 0.3937 - val_loss: 0.4005\n",
            "Epoch 20/40\n",
            "8126/8126 [==============================] - 0s 35us/step - loss: 0.3911 - val_loss: 0.3911\n",
            "Epoch 21/40\n",
            "8126/8126 [==============================] - 0s 34us/step - loss: 0.3911 - val_loss: 0.3816\n",
            "Epoch 22/40\n",
            "8126/8126 [==============================] - 0s 33us/step - loss: 0.3896 - val_loss: 0.3813\n",
            "Epoch 23/40\n",
            "8126/8126 [==============================] - 0s 32us/step - loss: 0.3873 - val_loss: 0.3866\n",
            "Epoch 24/40\n",
            "8126/8126 [==============================] - 0s 33us/step - loss: 0.3852 - val_loss: 0.3853\n",
            "Epoch 25/40\n",
            "8126/8126 [==============================] - 0s 32us/step - loss: 0.3834 - val_loss: 0.3754\n",
            "Epoch 26/40\n",
            "8126/8126 [==============================] - 0s 32us/step - loss: 0.3828 - val_loss: 0.3815\n",
            "Epoch 27/40\n",
            "8126/8126 [==============================] - 0s 33us/step - loss: 0.3813 - val_loss: 0.3780\n",
            "Epoch 28/40\n",
            "8126/8126 [==============================] - 0s 33us/step - loss: 0.3808 - val_loss: 0.3740\n",
            "Epoch 29/40\n",
            "8126/8126 [==============================] - 0s 33us/step - loss: 0.3802 - val_loss: 0.3741\n",
            "Epoch 30/40\n",
            "8126/8126 [==============================] - 0s 33us/step - loss: 0.3784 - val_loss: 0.3723\n",
            "Epoch 31/40\n",
            "8126/8126 [==============================] - 0s 32us/step - loss: 0.3781 - val_loss: 0.3763\n",
            "Epoch 32/40\n",
            "8126/8126 [==============================] - 0s 32us/step - loss: 0.3781 - val_loss: 0.3741\n",
            "Epoch 33/40\n",
            "8126/8126 [==============================] - 0s 33us/step - loss: 0.3749 - val_loss: 0.3680\n",
            "Epoch 34/40\n",
            "8126/8126 [==============================] - 0s 32us/step - loss: 0.3753 - val_loss: 0.3761\n",
            "Epoch 35/40\n",
            "8126/8126 [==============================] - 0s 32us/step - loss: 0.3750 - val_loss: 0.3692\n",
            "Epoch 36/40\n",
            "8126/8126 [==============================] - 0s 35us/step - loss: 0.3752 - val_loss: 0.3690\n",
            "Epoch 37/40\n",
            "8126/8126 [==============================] - 0s 32us/step - loss: 0.3722 - val_loss: 0.3678\n",
            "Epoch 38/40\n",
            "8126/8126 [==============================] - 0s 33us/step - loss: 0.3711 - val_loss: 0.3740\n",
            "Epoch 39/40\n",
            "8126/8126 [==============================] - 0s 35us/step - loss: 0.3707 - val_loss: 0.3675\n",
            "Epoch 40/40\n",
            "8126/8126 [==============================] - 0s 31us/step - loss: 0.3694 - val_loss: 0.3647\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 72%|███████▏  | 26/36 [05:06<01:55, 11.53s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "{'activation1': <function elu at 0x7f4d33558e18>, 'first_hidden_layer': 16, 'optimizer': 'sgd', 'second_hidden_layer': 8}\n",
            "Train on 8126 samples, validate on 3870 samples\n",
            "Epoch 1/40\n",
            "8126/8126 [==============================] - 0s 39us/step - loss: 1.1685 - val_loss: 0.5074\n",
            "Epoch 2/40\n",
            "8126/8126 [==============================] - 0s 32us/step - loss: 0.4975 - val_loss: 0.4444\n",
            "Epoch 3/40\n",
            "8126/8126 [==============================] - 0s 30us/step - loss: 0.4513 - val_loss: 0.4284\n",
            "Epoch 4/40\n",
            "8126/8126 [==============================] - 0s 31us/step - loss: 0.4390 - val_loss: 0.4214\n",
            "Epoch 5/40\n",
            "8126/8126 [==============================] - 0s 33us/step - loss: 0.4285 - val_loss: 0.4141\n",
            "Epoch 6/40\n",
            "8126/8126 [==============================] - 0s 30us/step - loss: 0.4314 - val_loss: 0.4079\n",
            "Epoch 7/40\n",
            "8126/8126 [==============================] - 0s 31us/step - loss: 0.4192 - val_loss: 0.4050\n",
            "Epoch 8/40\n",
            "8126/8126 [==============================] - 0s 31us/step - loss: 0.4118 - val_loss: 0.3986\n",
            "Epoch 9/40\n",
            "8126/8126 [==============================] - 0s 32us/step - loss: 0.4075 - val_loss: 0.3925\n",
            "Epoch 10/40\n",
            "8126/8126 [==============================] - 0s 31us/step - loss: 0.4032 - val_loss: 0.3955\n",
            "Epoch 11/40\n",
            "8126/8126 [==============================] - 0s 31us/step - loss: 0.4004 - val_loss: 0.3916\n",
            "Epoch 12/40\n",
            "8126/8126 [==============================] - 0s 31us/step - loss: 0.3977 - val_loss: 0.3856\n",
            "Epoch 13/40\n",
            "8126/8126 [==============================] - 0s 32us/step - loss: 0.3954 - val_loss: 0.3865\n",
            "Epoch 14/40\n",
            "8126/8126 [==============================] - 0s 31us/step - loss: 0.3931 - val_loss: 0.3829\n",
            "Epoch 15/40\n",
            "8126/8126 [==============================] - 0s 31us/step - loss: 0.3903 - val_loss: 0.3785\n",
            "Epoch 16/40\n",
            "8126/8126 [==============================] - 0s 31us/step - loss: 0.3886 - val_loss: 0.3793\n",
            "Epoch 17/40\n",
            "8126/8126 [==============================] - 0s 31us/step - loss: 0.3863 - val_loss: 0.3774\n",
            "Epoch 18/40\n",
            "8126/8126 [==============================] - 0s 30us/step - loss: 0.3852 - val_loss: 0.3752\n",
            "Epoch 19/40\n",
            "8126/8126 [==============================] - 0s 32us/step - loss: 0.3817 - val_loss: 0.3706\n",
            "Epoch 20/40\n",
            "8126/8126 [==============================] - 0s 31us/step - loss: 0.3805 - val_loss: 0.3675\n",
            "Epoch 21/40\n",
            "8126/8126 [==============================] - 0s 31us/step - loss: 0.3790 - val_loss: 0.3677\n",
            "Epoch 22/40\n",
            "8126/8126 [==============================] - 0s 31us/step - loss: 0.3761 - val_loss: 0.3690\n",
            "Epoch 23/40\n",
            "8126/8126 [==============================] - 0s 31us/step - loss: 0.3747 - val_loss: 0.3699\n",
            "Epoch 24/40\n",
            "8126/8126 [==============================] - 0s 31us/step - loss: 0.3733 - val_loss: 0.3687\n",
            "Epoch 25/40\n",
            "8126/8126 [==============================] - 0s 32us/step - loss: 0.3725 - val_loss: 0.3614\n",
            "Epoch 26/40\n",
            "8126/8126 [==============================] - 0s 33us/step - loss: 0.3804 - val_loss: 0.3734\n",
            "Epoch 27/40\n",
            "8126/8126 [==============================] - 0s 32us/step - loss: 0.3697 - val_loss: 0.3594\n",
            "Epoch 28/40\n",
            "8126/8126 [==============================] - 0s 33us/step - loss: 0.3684 - val_loss: 0.3619\n",
            "Epoch 29/40\n",
            "8126/8126 [==============================] - 0s 31us/step - loss: 0.3737 - val_loss: 0.3616\n",
            "Epoch 30/40\n",
            "8126/8126 [==============================] - 0s 35us/step - loss: 0.3651 - val_loss: 0.3648\n",
            "Epoch 31/40\n",
            "8126/8126 [==============================] - 0s 32us/step - loss: 0.3654 - val_loss: 0.3567\n",
            "Epoch 32/40\n",
            "8126/8126 [==============================] - 0s 32us/step - loss: 0.3641 - val_loss: 0.3565\n",
            "Epoch 33/40\n",
            "8126/8126 [==============================] - 0s 32us/step - loss: 0.3727 - val_loss: 0.3631\n",
            "Epoch 34/40\n",
            "8126/8126 [==============================] - 0s 32us/step - loss: 0.3635 - val_loss: 0.3535\n",
            "Epoch 35/40\n",
            "8126/8126 [==============================] - 0s 32us/step - loss: 0.3614 - val_loss: 0.3567\n",
            "Epoch 36/40\n",
            "8126/8126 [==============================] - 0s 33us/step - loss: 0.3608 - val_loss: 0.3539\n",
            "Epoch 37/40\n",
            "8126/8126 [==============================] - 0s 32us/step - loss: 0.3598 - val_loss: 0.3537\n",
            "Epoch 38/40\n",
            "8126/8126 [==============================] - 0s 32us/step - loss: 0.3590 - val_loss: 0.3519\n",
            "Epoch 39/40\n",
            "8126/8126 [==============================] - 0s 32us/step - loss: 0.3686 - val_loss: 0.3532\n",
            "Epoch 40/40\n",
            "8126/8126 [==============================] - 0s 35us/step - loss: 0.3574 - val_loss: 0.3520\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 75%|███████▌  | 27/36 [05:16<01:41, 11.29s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "{'activation1': <function elu at 0x7f4d33558e18>, 'first_hidden_layer': 16, 'optimizer': 'adam', 'second_hidden_layer': 16}\n",
            "Train on 8126 samples, validate on 3870 samples\n",
            "Epoch 1/40\n",
            "8126/8126 [==============================] - 0s 47us/step - loss: 1.7900 - val_loss: 0.6789\n",
            "Epoch 2/40\n",
            "8126/8126 [==============================] - 0s 36us/step - loss: 0.6735 - val_loss: 0.5328\n",
            "Epoch 3/40\n",
            "8126/8126 [==============================] - 0s 36us/step - loss: 0.5197 - val_loss: 0.4795\n",
            "Epoch 4/40\n",
            "8126/8126 [==============================] - 0s 36us/step - loss: 0.4845 - val_loss: 0.4616\n",
            "Epoch 5/40\n",
            "8126/8126 [==============================] - 0s 40us/step - loss: 0.4659 - val_loss: 0.4495\n",
            "Epoch 6/40\n",
            "8126/8126 [==============================] - 0s 35us/step - loss: 0.4523 - val_loss: 0.4416\n",
            "Epoch 7/40\n",
            "8126/8126 [==============================] - 0s 34us/step - loss: 0.4425 - val_loss: 0.4342\n",
            "Epoch 8/40\n",
            "8126/8126 [==============================] - 0s 33us/step - loss: 0.4345 - val_loss: 0.4236\n",
            "Epoch 9/40\n",
            "8126/8126 [==============================] - 0s 35us/step - loss: 0.4279 - val_loss: 0.4189\n",
            "Epoch 10/40\n",
            "8126/8126 [==============================] - 0s 37us/step - loss: 0.4223 - val_loss: 0.4155\n",
            "Epoch 11/40\n",
            "8126/8126 [==============================] - 0s 34us/step - loss: 0.4159 - val_loss: 0.4099\n",
            "Epoch 12/40\n",
            "8126/8126 [==============================] - 0s 34us/step - loss: 0.4134 - val_loss: 0.4068\n",
            "Epoch 13/40\n",
            "8126/8126 [==============================] - 0s 35us/step - loss: 0.4079 - val_loss: 0.4063\n",
            "Epoch 14/40\n",
            "8126/8126 [==============================] - 0s 33us/step - loss: 0.4046 - val_loss: 0.3979\n",
            "Epoch 15/40\n",
            "8126/8126 [==============================] - 0s 34us/step - loss: 0.4016 - val_loss: 0.3933\n",
            "Epoch 16/40\n",
            "8126/8126 [==============================] - 0s 35us/step - loss: 0.3996 - val_loss: 0.3941\n",
            "Epoch 17/40\n",
            "8126/8126 [==============================] - 0s 35us/step - loss: 0.3945 - val_loss: 0.3872\n",
            "Epoch 18/40\n",
            "8126/8126 [==============================] - 0s 34us/step - loss: 0.3912 - val_loss: 0.3887\n",
            "Epoch 19/40\n",
            "8126/8126 [==============================] - 0s 35us/step - loss: 0.3887 - val_loss: 0.3846\n",
            "Epoch 20/40\n",
            "8126/8126 [==============================] - 0s 35us/step - loss: 0.3862 - val_loss: 0.3820\n",
            "Epoch 21/40\n",
            "8126/8126 [==============================] - 0s 34us/step - loss: 0.3830 - val_loss: 0.3760\n",
            "Epoch 22/40\n",
            "8126/8126 [==============================] - 0s 35us/step - loss: 0.3823 - val_loss: 0.3746\n",
            "Epoch 23/40\n",
            "8126/8126 [==============================] - 0s 36us/step - loss: 0.3877 - val_loss: 0.3732\n",
            "Epoch 24/40\n",
            "8126/8126 [==============================] - 0s 34us/step - loss: 0.3790 - val_loss: 0.3742\n",
            "Epoch 25/40\n",
            "8126/8126 [==============================] - 0s 35us/step - loss: 0.3757 - val_loss: 0.3682\n",
            "Epoch 26/40\n",
            "8126/8126 [==============================] - 0s 32us/step - loss: 0.3730 - val_loss: 0.3706\n",
            "Epoch 27/40\n",
            "8126/8126 [==============================] - 0s 34us/step - loss: 0.3715 - val_loss: 0.3635\n",
            "Epoch 28/40\n",
            "8126/8126 [==============================] - 0s 34us/step - loss: 0.3702 - val_loss: 0.3600\n",
            "Epoch 29/40\n",
            "8126/8126 [==============================] - 0s 33us/step - loss: 0.3664 - val_loss: 0.3608\n",
            "Epoch 30/40\n",
            "8126/8126 [==============================] - 0s 35us/step - loss: 0.3662 - val_loss: 0.3570\n",
            "Epoch 31/40\n",
            "8126/8126 [==============================] - 0s 34us/step - loss: 0.3669 - val_loss: 0.3599\n",
            "Epoch 32/40\n",
            "8126/8126 [==============================] - 0s 34us/step - loss: 0.3641 - val_loss: 0.3561\n",
            "Epoch 33/40\n",
            "8126/8126 [==============================] - 0s 34us/step - loss: 0.3611 - val_loss: 0.3539\n",
            "Epoch 34/40\n",
            "8126/8126 [==============================] - 0s 37us/step - loss: 0.3602 - val_loss: 0.3500\n",
            "Epoch 35/40\n",
            "8126/8126 [==============================] - 0s 34us/step - loss: 0.3592 - val_loss: 0.3491\n",
            "Epoch 36/40\n",
            "8126/8126 [==============================] - 0s 33us/step - loss: 0.3579 - val_loss: 0.3498\n",
            "Epoch 37/40\n",
            "8126/8126 [==============================] - 0s 35us/step - loss: 0.3564 - val_loss: 0.3569\n",
            "Epoch 38/40\n",
            "8126/8126 [==============================] - 0s 35us/step - loss: 0.3589 - val_loss: 0.3531\n",
            "Epoch 39/40\n",
            "8126/8126 [==============================] - 0s 36us/step - loss: 0.3671 - val_loss: 0.3478\n",
            "Epoch 40/40\n",
            "8126/8126 [==============================] - 0s 34us/step - loss: 0.3536 - val_loss: 0.3471\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 78%|███████▊  | 28/36 [05:28<01:31, 11.48s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "{'activation1': <function elu at 0x7f4d33558e18>, 'first_hidden_layer': 16, 'optimizer': 'adam', 'second_hidden_layer': 12}\n",
            "Train on 8126 samples, validate on 3870 samples\n",
            "Epoch 1/40\n",
            "8126/8126 [==============================] - 0s 46us/step - loss: 1.9977 - val_loss: 0.5915\n",
            "Epoch 2/40\n",
            "8126/8126 [==============================] - 0s 35us/step - loss: 0.5407 - val_loss: 0.4990\n",
            "Epoch 3/40\n",
            "8126/8126 [==============================] - 0s 37us/step - loss: 0.4942 - val_loss: 0.4679\n",
            "Epoch 4/40\n",
            "8126/8126 [==============================] - 0s 37us/step - loss: 0.4728 - val_loss: 0.4539\n",
            "Epoch 5/40\n",
            "8126/8126 [==============================] - 0s 35us/step - loss: 0.4602 - val_loss: 0.4486\n",
            "Epoch 6/40\n",
            "8126/8126 [==============================] - 0s 37us/step - loss: 0.4552 - val_loss: 0.4370\n",
            "Epoch 7/40\n",
            "8126/8126 [==============================] - 0s 36us/step - loss: 0.4485 - val_loss: 0.4326\n",
            "Epoch 8/40\n",
            "8126/8126 [==============================] - 0s 36us/step - loss: 0.4421 - val_loss: 0.4252\n",
            "Epoch 9/40\n",
            "8126/8126 [==============================] - 0s 36us/step - loss: 0.4352 - val_loss: 0.4249\n",
            "Epoch 10/40\n",
            "8126/8126 [==============================] - 0s 37us/step - loss: 0.4324 - val_loss: 0.4184\n",
            "Epoch 11/40\n",
            "8126/8126 [==============================] - 0s 36us/step - loss: 0.4273 - val_loss: 0.4120\n",
            "Epoch 12/40\n",
            "8126/8126 [==============================] - 0s 37us/step - loss: 0.4254 - val_loss: 0.4158\n",
            "Epoch 13/40\n",
            "8126/8126 [==============================] - 0s 36us/step - loss: 0.4179 - val_loss: 0.4094\n",
            "Epoch 14/40\n",
            "8126/8126 [==============================] - 0s 37us/step - loss: 0.4166 - val_loss: 0.4036\n",
            "Epoch 15/40\n",
            "8126/8126 [==============================] - 0s 36us/step - loss: 0.4125 - val_loss: 0.4012\n",
            "Epoch 16/40\n",
            "8126/8126 [==============================] - 0s 36us/step - loss: 0.4097 - val_loss: 0.3957\n",
            "Epoch 17/40\n",
            "8126/8126 [==============================] - 0s 36us/step - loss: 0.4046 - val_loss: 0.3920\n",
            "Epoch 18/40\n",
            "8126/8126 [==============================] - 0s 36us/step - loss: 0.3990 - val_loss: 0.3943\n",
            "Epoch 19/40\n",
            "8126/8126 [==============================] - 0s 37us/step - loss: 0.3974 - val_loss: 0.3851\n",
            "Epoch 20/40\n",
            "8126/8126 [==============================] - 0s 37us/step - loss: 0.3957 - val_loss: 0.3806\n",
            "Epoch 21/40\n",
            "8126/8126 [==============================] - 0s 37us/step - loss: 0.3887 - val_loss: 0.3801\n",
            "Epoch 22/40\n",
            "8126/8126 [==============================] - 0s 37us/step - loss: 0.3859 - val_loss: 0.3789\n",
            "Epoch 23/40\n",
            "8126/8126 [==============================] - 0s 37us/step - loss: 0.3864 - val_loss: 0.3821\n",
            "Epoch 24/40\n",
            "8126/8126 [==============================] - 0s 36us/step - loss: 0.3812 - val_loss: 0.3725\n",
            "Epoch 25/40\n",
            "8126/8126 [==============================] - 0s 35us/step - loss: 0.3788 - val_loss: 0.3709\n",
            "Epoch 26/40\n",
            "8126/8126 [==============================] - 0s 37us/step - loss: 0.3756 - val_loss: 0.3644\n",
            "Epoch 27/40\n",
            "8126/8126 [==============================] - 0s 38us/step - loss: 0.3744 - val_loss: 0.3647\n",
            "Epoch 28/40\n",
            "8126/8126 [==============================] - 0s 39us/step - loss: 0.3711 - val_loss: 0.3657\n",
            "Epoch 29/40\n",
            "8126/8126 [==============================] - 0s 36us/step - loss: 0.3682 - val_loss: 0.3598\n",
            "Epoch 30/40\n",
            "8126/8126 [==============================] - 0s 36us/step - loss: 0.3669 - val_loss: 0.3567\n",
            "Epoch 31/40\n",
            "8126/8126 [==============================] - 0s 36us/step - loss: 0.3636 - val_loss: 0.3540\n",
            "Epoch 32/40\n",
            "8126/8126 [==============================] - 0s 37us/step - loss: 0.3610 - val_loss: 0.3523\n",
            "Epoch 33/40\n",
            "8126/8126 [==============================] - 0s 43us/step - loss: 0.3596 - val_loss: 0.3537\n",
            "Epoch 34/40\n",
            "8126/8126 [==============================] - 0s 38us/step - loss: 0.3586 - val_loss: 0.3487\n",
            "Epoch 35/40\n",
            "8126/8126 [==============================] - 0s 38us/step - loss: 0.3549 - val_loss: 0.3510\n",
            "Epoch 36/40\n",
            "8126/8126 [==============================] - 0s 37us/step - loss: 0.3556 - val_loss: 0.3499\n",
            "Epoch 37/40\n",
            "8126/8126 [==============================] - 0s 37us/step - loss: 0.3515 - val_loss: 0.3497\n",
            "Epoch 38/40\n",
            "8126/8126 [==============================] - 0s 39us/step - loss: 0.3510 - val_loss: 0.3499\n",
            "Epoch 39/40\n",
            "8126/8126 [==============================] - 0s 45us/step - loss: 0.3482 - val_loss: 0.3418\n",
            "Epoch 40/40\n",
            "8126/8126 [==============================] - 0s 47us/step - loss: 0.3479 - val_loss: 0.3459\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 81%|████████  | 29/36 [05:41<01:22, 11.85s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "{'activation1': <function elu at 0x7f4d33558e18>, 'first_hidden_layer': 16, 'optimizer': 'adam', 'second_hidden_layer': 8}\n",
            "Train on 8126 samples, validate on 3870 samples\n",
            "Epoch 1/40\n",
            "8126/8126 [==============================] - 0s 50us/step - loss: 1.9793 - val_loss: 0.5977\n",
            "Epoch 2/40\n",
            "8126/8126 [==============================] - 0s 37us/step - loss: 0.5876 - val_loss: 0.5027\n",
            "Epoch 3/40\n",
            "8126/8126 [==============================] - 0s 35us/step - loss: 0.5002 - val_loss: 0.4631\n",
            "Epoch 4/40\n",
            "8126/8126 [==============================] - 0s 37us/step - loss: 0.4695 - val_loss: 0.4425\n",
            "Epoch 5/40\n",
            "8126/8126 [==============================] - 0s 38us/step - loss: 0.4535 - val_loss: 0.4290\n",
            "Epoch 6/40\n",
            "8126/8126 [==============================] - 0s 37us/step - loss: 0.4419 - val_loss: 0.4237\n",
            "Epoch 7/40\n",
            "8126/8126 [==============================] - 0s 40us/step - loss: 0.4334 - val_loss: 0.4153\n",
            "Epoch 8/40\n",
            "8126/8126 [==============================] - 0s 37us/step - loss: 0.4278 - val_loss: 0.4104\n",
            "Epoch 9/40\n",
            "8126/8126 [==============================] - 0s 38us/step - loss: 0.4241 - val_loss: 0.4062\n",
            "Epoch 10/40\n",
            "8126/8126 [==============================] - 0s 36us/step - loss: 0.4189 - val_loss: 0.4001\n",
            "Epoch 11/40\n",
            "8126/8126 [==============================] - 0s 37us/step - loss: 0.4142 - val_loss: 0.4020\n",
            "Epoch 12/40\n",
            "8126/8126 [==============================] - 0s 38us/step - loss: 0.4123 - val_loss: 0.3971\n",
            "Epoch 13/40\n",
            "8126/8126 [==============================] - 0s 35us/step - loss: 0.4051 - val_loss: 0.3965\n",
            "Epoch 14/40\n",
            "8126/8126 [==============================] - 0s 36us/step - loss: 0.4083 - val_loss: 0.3877\n",
            "Epoch 15/40\n",
            "8126/8126 [==============================] - 0s 36us/step - loss: 0.4001 - val_loss: 0.3846\n",
            "Epoch 16/40\n",
            "8126/8126 [==============================] - 0s 37us/step - loss: 0.3969 - val_loss: 0.3848\n",
            "Epoch 17/40\n",
            "8126/8126 [==============================] - 0s 37us/step - loss: 0.3981 - val_loss: 0.3807\n",
            "Epoch 18/40\n",
            "8126/8126 [==============================] - 0s 38us/step - loss: 0.3986 - val_loss: 0.3812\n",
            "Epoch 19/40\n",
            "8126/8126 [==============================] - 0s 36us/step - loss: 0.3892 - val_loss: 0.3744\n",
            "Epoch 20/40\n",
            "8126/8126 [==============================] - 0s 35us/step - loss: 0.3898 - val_loss: 0.3728\n",
            "Epoch 21/40\n",
            "8126/8126 [==============================] - 0s 37us/step - loss: 0.3860 - val_loss: 0.3744\n",
            "Epoch 22/40\n",
            "8126/8126 [==============================] - 0s 36us/step - loss: 0.3880 - val_loss: 0.3715\n",
            "Epoch 23/40\n",
            "8126/8126 [==============================] - 0s 35us/step - loss: 0.3814 - val_loss: 0.3701\n",
            "Epoch 24/40\n",
            "8126/8126 [==============================] - 0s 41us/step - loss: 0.3797 - val_loss: 0.3655\n",
            "Epoch 25/40\n",
            "8126/8126 [==============================] - 0s 42us/step - loss: 0.3850 - val_loss: 0.3645\n",
            "Epoch 26/40\n",
            "8126/8126 [==============================] - 0s 42us/step - loss: 0.3787 - val_loss: 0.3660\n",
            "Epoch 27/40\n",
            "8126/8126 [==============================] - 0s 43us/step - loss: 0.3789 - val_loss: 0.3629\n",
            "Epoch 28/40\n",
            "8126/8126 [==============================] - 0s 40us/step - loss: 0.3750 - val_loss: 0.3593\n",
            "Epoch 29/40\n",
            "8126/8126 [==============================] - 0s 38us/step - loss: 0.3729 - val_loss: 0.3630\n",
            "Epoch 30/40\n",
            "8126/8126 [==============================] - 0s 41us/step - loss: 0.3667 - val_loss: 0.3573\n",
            "Epoch 31/40\n",
            "8126/8126 [==============================] - 0s 39us/step - loss: 0.3688 - val_loss: 0.3559\n",
            "Epoch 32/40\n",
            "8126/8126 [==============================] - 0s 37us/step - loss: 0.3674 - val_loss: 0.3549\n",
            "Epoch 33/40\n",
            "8126/8126 [==============================] - 0s 36us/step - loss: 0.3677 - val_loss: 0.3569\n",
            "Epoch 34/40\n",
            "8126/8126 [==============================] - 0s 42us/step - loss: 0.3704 - val_loss: 0.3571\n",
            "Epoch 35/40\n",
            "8126/8126 [==============================] - 0s 37us/step - loss: 0.3643 - val_loss: 0.3527\n",
            "Epoch 36/40\n",
            "8126/8126 [==============================] - 0s 37us/step - loss: 0.3608 - val_loss: 0.3522\n",
            "Epoch 37/40\n",
            "8126/8126 [==============================] - 0s 38us/step - loss: 0.3617 - val_loss: 0.3489\n",
            "Epoch 38/40\n",
            "8126/8126 [==============================] - 0s 43us/step - loss: 0.3623 - val_loss: 0.3477\n",
            "Epoch 39/40\n",
            "8126/8126 [==============================] - 0s 43us/step - loss: 0.3579 - val_loss: 0.3526\n",
            "Epoch 40/40\n",
            "8126/8126 [==============================] - 0s 39us/step - loss: 0.3579 - val_loss: 0.3477\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 83%|████████▎ | 30/36 [05:54<01:13, 12.22s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "{'activation1': <function elu at 0x7f4d33558e18>, 'first_hidden_layer': 8, 'optimizer': 'sgd', 'second_hidden_layer': 16}\n",
            "Train on 8126 samples, validate on 3870 samples\n",
            "Epoch 1/40\n",
            "8126/8126 [==============================] - 0s 43us/step - loss: 0.8684 - val_loss: 0.5751\n",
            "Epoch 2/40\n",
            "8126/8126 [==============================] - 0s 38us/step - loss: 0.5598 - val_loss: 0.5215\n",
            "Epoch 3/40\n",
            "8126/8126 [==============================] - 0s 38us/step - loss: 0.5200 - val_loss: 0.4986\n",
            "Epoch 4/40\n",
            "8126/8126 [==============================] - 0s 37us/step - loss: 0.5034 - val_loss: 0.4817\n",
            "Epoch 5/40\n",
            "8126/8126 [==============================] - 0s 37us/step - loss: 0.4911 - val_loss: 0.4842\n",
            "Epoch 6/40\n",
            "8126/8126 [==============================] - 0s 37us/step - loss: 0.4824 - val_loss: 0.4657\n",
            "Epoch 7/40\n",
            "8126/8126 [==============================] - 0s 35us/step - loss: 0.4752 - val_loss: 0.4594\n",
            "Epoch 8/40\n",
            "8126/8126 [==============================] - 0s 37us/step - loss: 0.4697 - val_loss: 0.4597\n",
            "Epoch 9/40\n",
            "8126/8126 [==============================] - 0s 38us/step - loss: 0.4628 - val_loss: 0.4539\n",
            "Epoch 10/40\n",
            "8126/8126 [==============================] - 0s 34us/step - loss: 0.4577 - val_loss: 0.4512\n",
            "Epoch 11/40\n",
            "8126/8126 [==============================] - 0s 32us/step - loss: 0.4533 - val_loss: 0.4424\n",
            "Epoch 12/40\n",
            "8126/8126 [==============================] - 0s 34us/step - loss: 0.4478 - val_loss: 0.4329\n",
            "Epoch 13/40\n",
            "8126/8126 [==============================] - 0s 34us/step - loss: 0.4455 - val_loss: 0.4333\n",
            "Epoch 14/40\n",
            "8126/8126 [==============================] - 0s 33us/step - loss: 0.4420 - val_loss: 0.4270\n",
            "Epoch 15/40\n",
            "8126/8126 [==============================] - 0s 33us/step - loss: 0.4367 - val_loss: 0.4261\n",
            "Epoch 16/40\n",
            "8126/8126 [==============================] - 0s 36us/step - loss: 0.4354 - val_loss: 0.4291\n",
            "Epoch 17/40\n",
            "8126/8126 [==============================] - 0s 34us/step - loss: 0.4314 - val_loss: 0.4190\n",
            "Epoch 18/40\n",
            "8126/8126 [==============================] - 0s 34us/step - loss: 0.4282 - val_loss: 0.4192\n",
            "Epoch 19/40\n",
            "8126/8126 [==============================] - 0s 34us/step - loss: 0.4265 - val_loss: 0.4271\n",
            "Epoch 20/40\n",
            "8126/8126 [==============================] - 0s 35us/step - loss: 0.4250 - val_loss: 0.4153\n",
            "Epoch 21/40\n",
            "8126/8126 [==============================] - 0s 33us/step - loss: 0.4214 - val_loss: 0.4176\n",
            "Epoch 22/40\n",
            "8126/8126 [==============================] - 0s 34us/step - loss: 0.4208 - val_loss: 0.4063\n",
            "Epoch 23/40\n",
            "8126/8126 [==============================] - 0s 33us/step - loss: 0.4169 - val_loss: 0.4125\n",
            "Epoch 24/40\n",
            "8126/8126 [==============================] - 0s 33us/step - loss: 0.4169 - val_loss: 0.4059\n",
            "Epoch 25/40\n",
            "8126/8126 [==============================] - 0s 33us/step - loss: 0.4148 - val_loss: 0.4042\n",
            "Epoch 26/40\n",
            "8126/8126 [==============================] - 0s 33us/step - loss: 0.4121 - val_loss: 0.4003\n",
            "Epoch 27/40\n",
            "8126/8126 [==============================] - 0s 34us/step - loss: 0.4114 - val_loss: 0.4008\n",
            "Epoch 28/40\n",
            "8126/8126 [==============================] - 0s 34us/step - loss: 0.4087 - val_loss: 0.3999\n",
            "Epoch 29/40\n",
            "8126/8126 [==============================] - 0s 34us/step - loss: 0.4092 - val_loss: 0.3979\n",
            "Epoch 30/40\n",
            "8126/8126 [==============================] - 0s 35us/step - loss: 0.4076 - val_loss: 0.3976\n",
            "Epoch 31/40\n",
            "8126/8126 [==============================] - 0s 38us/step - loss: 0.4053 - val_loss: 0.3905\n",
            "Epoch 32/40\n",
            "8126/8126 [==============================] - 0s 36us/step - loss: 0.4034 - val_loss: 0.3916\n",
            "Epoch 33/40\n",
            "8126/8126 [==============================] - 0s 36us/step - loss: 0.4014 - val_loss: 0.3936\n",
            "Epoch 34/40\n",
            "8126/8126 [==============================] - 0s 39us/step - loss: 0.3994 - val_loss: 0.3953\n",
            "Epoch 35/40\n",
            "8126/8126 [==============================] - 0s 38us/step - loss: 0.4006 - val_loss: 0.3875\n",
            "Epoch 36/40\n",
            "8126/8126 [==============================] - 0s 38us/step - loss: 0.3972 - val_loss: 0.3873\n",
            "Epoch 37/40\n",
            "8126/8126 [==============================] - 0s 38us/step - loss: 0.3969 - val_loss: 0.3885\n",
            "Epoch 38/40\n",
            "8126/8126 [==============================] - 0s 39us/step - loss: 0.3954 - val_loss: 0.3841\n",
            "Epoch 39/40\n",
            "8126/8126 [==============================] - 0s 33us/step - loss: 0.3940 - val_loss: 0.3824\n",
            "Epoch 40/40\n",
            "8126/8126 [==============================] - 0s 33us/step - loss: 0.3933 - val_loss: 0.3833\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 86%|████████▌ | 31/36 [06:06<01:00, 12.13s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "{'activation1': <function elu at 0x7f4d33558e18>, 'first_hidden_layer': 8, 'optimizer': 'sgd', 'second_hidden_layer': 12}\n",
            "Train on 8126 samples, validate on 3870 samples\n",
            "Epoch 1/40\n",
            "8126/8126 [==============================] - 0s 40us/step - loss: 1.2318 - val_loss: 0.5243\n",
            "Epoch 2/40\n",
            "8126/8126 [==============================] - 0s 34us/step - loss: nan - val_loss: nan\n",
            "Epoch 3/40\n",
            "8126/8126 [==============================] - 0s 37us/step - loss: nan - val_loss: nan\n",
            "Epoch 4/40\n",
            "8126/8126 [==============================] - 0s 35us/step - loss: nan - val_loss: nan\n",
            "Epoch 5/40\n",
            "8126/8126 [==============================] - 0s 33us/step - loss: nan - val_loss: nan\n",
            "Epoch 6/40\n",
            "8126/8126 [==============================] - 0s 33us/step - loss: nan - val_loss: nan\n",
            "Epoch 7/40\n",
            "8126/8126 [==============================] - 0s 37us/step - loss: nan - val_loss: nan\n",
            "Epoch 8/40\n",
            "8126/8126 [==============================] - 0s 33us/step - loss: nan - val_loss: nan\n",
            "Epoch 9/40\n",
            "8126/8126 [==============================] - 0s 33us/step - loss: nan - val_loss: nan\n",
            "Epoch 10/40\n",
            "8126/8126 [==============================] - 0s 34us/step - loss: nan - val_loss: nan\n",
            "Epoch 11/40\n",
            "8126/8126 [==============================] - 0s 37us/step - loss: nan - val_loss: nan\n",
            "Epoch 12/40\n",
            "8126/8126 [==============================] - 0s 34us/step - loss: nan - val_loss: nan\n",
            "Epoch 13/40\n",
            "8126/8126 [==============================] - 0s 32us/step - loss: nan - val_loss: nan\n",
            "Epoch 14/40\n",
            "8126/8126 [==============================] - 0s 34us/step - loss: nan - val_loss: nan\n",
            "Epoch 15/40\n",
            "8126/8126 [==============================] - 0s 36us/step - loss: nan - val_loss: nan\n",
            "Epoch 16/40\n",
            "8126/8126 [==============================] - 0s 33us/step - loss: nan - val_loss: nan\n",
            "Epoch 17/40\n",
            "8126/8126 [==============================] - 0s 32us/step - loss: nan - val_loss: nan\n",
            "Epoch 18/40\n",
            "8126/8126 [==============================] - 0s 36us/step - loss: nan - val_loss: nan\n",
            "Epoch 19/40\n",
            "8126/8126 [==============================] - 0s 33us/step - loss: nan - val_loss: nan\n",
            "Epoch 20/40\n",
            "8126/8126 [==============================] - 0s 34us/step - loss: nan - val_loss: nan\n",
            "Epoch 21/40\n",
            "8126/8126 [==============================] - 0s 33us/step - loss: nan - val_loss: nan\n",
            "Epoch 22/40\n",
            "8126/8126 [==============================] - 0s 33us/step - loss: nan - val_loss: nan\n",
            "Epoch 23/40\n",
            "8126/8126 [==============================] - 0s 35us/step - loss: nan - val_loss: nan\n",
            "Epoch 24/40\n",
            "8126/8126 [==============================] - 0s 37us/step - loss: nan - val_loss: nan\n",
            "Epoch 25/40\n",
            "8126/8126 [==============================] - 0s 36us/step - loss: nan - val_loss: nan\n",
            "Epoch 26/40\n",
            "8126/8126 [==============================] - 0s 33us/step - loss: nan - val_loss: nan\n",
            "Epoch 27/40\n",
            "8126/8126 [==============================] - 0s 32us/step - loss: nan - val_loss: nan\n",
            "Epoch 28/40\n",
            "8126/8126 [==============================] - 0s 35us/step - loss: nan - val_loss: nan\n",
            "Epoch 29/40\n",
            "8126/8126 [==============================] - 0s 36us/step - loss: nan - val_loss: nan\n",
            "Epoch 30/40\n",
            "8126/8126 [==============================] - 0s 35us/step - loss: nan - val_loss: nan\n",
            "Epoch 31/40\n",
            "8126/8126 [==============================] - 0s 35us/step - loss: nan - val_loss: nan\n",
            "Epoch 32/40\n",
            "8126/8126 [==============================] - 0s 36us/step - loss: nan - val_loss: nan\n",
            "Epoch 33/40\n",
            "8126/8126 [==============================] - 0s 35us/step - loss: nan - val_loss: nan\n",
            "Epoch 34/40\n",
            "8126/8126 [==============================] - 0s 33us/step - loss: nan - val_loss: nan\n",
            "Epoch 35/40\n",
            "8126/8126 [==============================] - 0s 37us/step - loss: nan - val_loss: nan\n",
            "Epoch 36/40\n",
            "8126/8126 [==============================] - 0s 41us/step - loss: nan - val_loss: nan\n",
            "Epoch 37/40\n",
            "8126/8126 [==============================] - 0s 39us/step - loss: nan - val_loss: nan\n",
            "Epoch 38/40\n",
            "8126/8126 [==============================] - 0s 39us/step - loss: nan - val_loss: nan\n",
            "Epoch 39/40\n",
            "8126/8126 [==============================] - 0s 42us/step - loss: nan - val_loss: nan\n",
            "Epoch 40/40\n",
            "8126/8126 [==============================] - 0s 37us/step - loss: nan - val_loss: nan\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 89%|████████▉ | 32/36 [06:18<00:48, 12.04s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "{'activation1': <function elu at 0x7f4d33558e18>, 'first_hidden_layer': 8, 'optimizer': 'sgd', 'second_hidden_layer': 8}\n",
            "Train on 8126 samples, validate on 3870 samples\n",
            "Epoch 1/40\n",
            "8126/8126 [==============================] - 0s 44us/step - loss: 1.2956 - val_loss: 0.5708\n",
            "Epoch 2/40\n",
            "8126/8126 [==============================] - 0s 38us/step - loss: 0.5431 - val_loss: 0.5058\n",
            "Epoch 3/40\n",
            "8126/8126 [==============================] - 0s 39us/step - loss: 0.4981 - val_loss: 0.4809\n",
            "Epoch 4/40\n",
            "8126/8126 [==============================] - 0s 39us/step - loss: 0.4718 - val_loss: 0.4623\n",
            "Epoch 5/40\n",
            "8126/8126 [==============================] - 0s 39us/step - loss: 0.4529 - val_loss: 0.4372\n",
            "Epoch 6/40\n",
            "8126/8126 [==============================] - 0s 39us/step - loss: 0.4399 - val_loss: 0.4278\n",
            "Epoch 7/40\n",
            "8126/8126 [==============================] - 0s 40us/step - loss: 0.4307 - val_loss: 0.4257\n",
            "Epoch 8/40\n",
            "8126/8126 [==============================] - 0s 39us/step - loss: 0.4267 - val_loss: 0.4162\n",
            "Epoch 9/40\n",
            "8126/8126 [==============================] - 0s 37us/step - loss: 0.4220 - val_loss: 0.4114\n",
            "Epoch 10/40\n",
            "8126/8126 [==============================] - 0s 38us/step - loss: 0.4189 - val_loss: 0.4132\n",
            "Epoch 11/40\n",
            "8126/8126 [==============================] - 0s 41us/step - loss: 0.4160 - val_loss: 0.4074\n",
            "Epoch 12/40\n",
            "8126/8126 [==============================] - 0s 43us/step - loss: 0.4147 - val_loss: 0.4158\n",
            "Epoch 13/40\n",
            "8126/8126 [==============================] - 0s 41us/step - loss: 0.4114 - val_loss: 0.4109\n",
            "Epoch 14/40\n",
            "8126/8126 [==============================] - 0s 42us/step - loss: 0.4096 - val_loss: 0.4128\n",
            "Epoch 15/40\n",
            "8126/8126 [==============================] - 0s 39us/step - loss: 0.4070 - val_loss: 0.4028\n",
            "Epoch 16/40\n",
            "8126/8126 [==============================] - 0s 40us/step - loss: 0.4058 - val_loss: 0.4106\n",
            "Epoch 17/40\n",
            "8126/8126 [==============================] - 0s 42us/step - loss: 0.4037 - val_loss: 0.3951\n",
            "Epoch 18/40\n",
            "8126/8126 [==============================] - 0s 39us/step - loss: 0.4004 - val_loss: 0.3946\n",
            "Epoch 19/40\n",
            "8126/8126 [==============================] - 0s 41us/step - loss: 0.3993 - val_loss: 0.3962\n",
            "Epoch 20/40\n",
            "8126/8126 [==============================] - 0s 41us/step - loss: 0.3975 - val_loss: 0.4047\n",
            "Epoch 21/40\n",
            "8126/8126 [==============================] - 0s 40us/step - loss: 0.3966 - val_loss: 0.3925\n",
            "Epoch 22/40\n",
            "8126/8126 [==============================] - 0s 40us/step - loss: 0.3949 - val_loss: 0.3910\n",
            "Epoch 23/40\n",
            "8126/8126 [==============================] - 0s 41us/step - loss: 0.3943 - val_loss: 0.3889\n",
            "Epoch 24/40\n",
            "8126/8126 [==============================] - 0s 39us/step - loss: 0.3911 - val_loss: 0.3998\n",
            "Epoch 25/40\n",
            "8126/8126 [==============================] - 0s 40us/step - loss: 0.3935 - val_loss: 0.3840\n",
            "Epoch 26/40\n",
            "8126/8126 [==============================] - 0s 41us/step - loss: 0.3895 - val_loss: 0.3899\n",
            "Epoch 27/40\n",
            "8126/8126 [==============================] - 0s 39us/step - loss: 0.3887 - val_loss: 0.3832\n",
            "Epoch 28/40\n",
            "8126/8126 [==============================] - 0s 40us/step - loss: 0.3874 - val_loss: 0.3906\n",
            "Epoch 29/40\n",
            "8126/8126 [==============================] - 0s 42us/step - loss: 0.3867 - val_loss: 0.3930\n",
            "Epoch 30/40\n",
            "8126/8126 [==============================] - 0s 40us/step - loss: 0.3852 - val_loss: 0.3890\n",
            "Epoch 31/40\n",
            "8126/8126 [==============================] - 0s 40us/step - loss: 0.3856 - val_loss: 0.3824\n",
            "Epoch 32/40\n",
            "8126/8126 [==============================] - 0s 42us/step - loss: 0.3846 - val_loss: 0.3790\n",
            "Epoch 33/40\n",
            "8126/8126 [==============================] - 0s 40us/step - loss: 0.3832 - val_loss: 0.3794\n",
            "Epoch 34/40\n",
            "8126/8126 [==============================] - 0s 41us/step - loss: 0.3834 - val_loss: 0.3794\n",
            "Epoch 35/40\n",
            "8126/8126 [==============================] - 0s 41us/step - loss: 0.3818 - val_loss: 0.3829\n",
            "Epoch 36/40\n",
            "8126/8126 [==============================] - 0s 41us/step - loss: 0.3801 - val_loss: 0.3775\n",
            "Epoch 37/40\n",
            "8126/8126 [==============================] - 0s 41us/step - loss: 0.3815 - val_loss: 0.3876\n",
            "Epoch 38/40\n",
            "8126/8126 [==============================] - 0s 41us/step - loss: 0.3796 - val_loss: 0.3906\n",
            "Epoch 39/40\n",
            "8126/8126 [==============================] - 0s 41us/step - loss: 0.3795 - val_loss: 0.3848\n",
            "Epoch 40/40\n",
            "8126/8126 [==============================] - 0s 41us/step - loss: 0.3798 - val_loss: 0.3801\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 92%|█████████▏| 33/36 [06:31<00:37, 12.47s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "{'activation1': <function elu at 0x7f4d33558e18>, 'first_hidden_layer': 8, 'optimizer': 'adam', 'second_hidden_layer': 16}\n",
            "Train on 8126 samples, validate on 3870 samples\n",
            "Epoch 1/40\n",
            "8126/8126 [==============================] - 0s 54us/step - loss: 1.8649 - val_loss: 0.6026\n",
            "Epoch 2/40\n",
            "8126/8126 [==============================] - 0s 41us/step - loss: 0.5762 - val_loss: 0.5089\n",
            "Epoch 3/40\n",
            "8126/8126 [==============================] - 0s 42us/step - loss: 0.5055 - val_loss: 0.4762\n",
            "Epoch 4/40\n",
            "8126/8126 [==============================] - 0s 40us/step - loss: 0.4800 - val_loss: 0.4644\n",
            "Epoch 5/40\n",
            "8126/8126 [==============================] - 0s 42us/step - loss: 0.4687 - val_loss: 0.4526\n",
            "Epoch 6/40\n",
            "8126/8126 [==============================] - 0s 40us/step - loss: 0.4575 - val_loss: 0.4434\n",
            "Epoch 7/40\n",
            "8126/8126 [==============================] - 0s 41us/step - loss: 0.4508 - val_loss: 0.4375\n",
            "Epoch 8/40\n",
            "8126/8126 [==============================] - 0s 41us/step - loss: 0.4439 - val_loss: 0.4314\n",
            "Epoch 9/40\n",
            "8126/8126 [==============================] - 0s 42us/step - loss: 0.4383 - val_loss: 0.4249\n",
            "Epoch 10/40\n",
            "8126/8126 [==============================] - 0s 40us/step - loss: 0.4320 - val_loss: 0.4180\n",
            "Epoch 11/40\n",
            "8126/8126 [==============================] - 0s 43us/step - loss: 0.4278 - val_loss: 0.4154\n",
            "Epoch 12/40\n",
            "8126/8126 [==============================] - 0s 41us/step - loss: 0.4203 - val_loss: 0.4063\n",
            "Epoch 13/40\n",
            "8126/8126 [==============================] - 0s 41us/step - loss: 0.4164 - val_loss: 0.4074\n",
            "Epoch 14/40\n",
            "8126/8126 [==============================] - 0s 42us/step - loss: 0.4122 - val_loss: 0.4001\n",
            "Epoch 15/40\n",
            "8126/8126 [==============================] - 0s 41us/step - loss: 0.4082 - val_loss: 0.3988\n",
            "Epoch 16/40\n",
            "8126/8126 [==============================] - 0s 42us/step - loss: 0.4047 - val_loss: 0.3941\n",
            "Epoch 17/40\n",
            "8126/8126 [==============================] - 0s 44us/step - loss: 0.4058 - val_loss: 0.3924\n",
            "Epoch 18/40\n",
            "8126/8126 [==============================] - 0s 41us/step - loss: 0.4002 - val_loss: 0.3917\n",
            "Epoch 19/40\n",
            "8126/8126 [==============================] - 0s 43us/step - loss: 0.3971 - val_loss: 0.3878\n",
            "Epoch 20/40\n",
            "8126/8126 [==============================] - 0s 40us/step - loss: 0.3968 - val_loss: 0.3836\n",
            "Epoch 21/40\n",
            "8126/8126 [==============================] - 0s 41us/step - loss: 0.3943 - val_loss: 0.3845\n",
            "Epoch 22/40\n",
            "8126/8126 [==============================] - 0s 40us/step - loss: 0.3916 - val_loss: 0.3842\n",
            "Epoch 23/40\n",
            "8126/8126 [==============================] - 0s 39us/step - loss: 0.3913 - val_loss: 0.3791\n",
            "Epoch 24/40\n",
            "8126/8126 [==============================] - 0s 41us/step - loss: 0.3854 - val_loss: 0.3771\n",
            "Epoch 25/40\n",
            "8126/8126 [==============================] - 0s 40us/step - loss: 0.3889 - val_loss: 0.3781\n",
            "Epoch 26/40\n",
            "8126/8126 [==============================] - 0s 39us/step - loss: 0.3875 - val_loss: 0.3795\n",
            "Epoch 27/40\n",
            "8126/8126 [==============================] - 0s 42us/step - loss: 0.3802 - val_loss: 0.3767\n",
            "Epoch 28/40\n",
            "8126/8126 [==============================] - 0s 38us/step - loss: 0.3784 - val_loss: 0.3715\n",
            "Epoch 29/40\n",
            "8126/8126 [==============================] - 0s 38us/step - loss: 0.3781 - val_loss: 0.3685\n",
            "Epoch 30/40\n",
            "8126/8126 [==============================] - 0s 40us/step - loss: 0.3761 - val_loss: 0.3682\n",
            "Epoch 31/40\n",
            "8126/8126 [==============================] - 0s 39us/step - loss: 0.3734 - val_loss: 0.3649\n",
            "Epoch 32/40\n",
            "8126/8126 [==============================] - 0s 39us/step - loss: 0.3715 - val_loss: 0.3647\n",
            "Epoch 33/40\n",
            "8126/8126 [==============================] - 0s 39us/step - loss: 0.3712 - val_loss: 0.3638\n",
            "Epoch 34/40\n",
            "8126/8126 [==============================] - 0s 38us/step - loss: 0.3685 - val_loss: 0.3659\n",
            "Epoch 35/40\n",
            "8126/8126 [==============================] - 0s 37us/step - loss: 0.3721 - val_loss: 0.3608\n",
            "Epoch 36/40\n",
            "8126/8126 [==============================] - 0s 39us/step - loss: 0.3681 - val_loss: 0.3595\n",
            "Epoch 37/40\n",
            "8126/8126 [==============================] - 0s 38us/step - loss: 0.3637 - val_loss: 0.3592\n",
            "Epoch 38/40\n",
            "8126/8126 [==============================] - 0s 38us/step - loss: 0.3635 - val_loss: 0.3591\n",
            "Epoch 39/40\n",
            "8126/8126 [==============================] - 0s 40us/step - loss: 0.3645 - val_loss: 0.3625\n",
            "Epoch 40/40\n",
            "8126/8126 [==============================] - 0s 37us/step - loss: 0.3624 - val_loss: 0.3607\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 94%|█████████▍| 34/36 [06:45<00:25, 12.86s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "{'activation1': <function elu at 0x7f4d33558e18>, 'first_hidden_layer': 8, 'optimizer': 'adam', 'second_hidden_layer': 12}\n",
            "Train on 8126 samples, validate on 3870 samples\n",
            "Epoch 1/40\n",
            "8126/8126 [==============================] - 0s 51us/step - loss: 2.6535 - val_loss: 0.6071\n",
            "Epoch 2/40\n",
            "8126/8126 [==============================] - 0s 38us/step - loss: 0.5626 - val_loss: 0.5236\n",
            "Epoch 3/40\n",
            "8126/8126 [==============================] - 0s 39us/step - loss: 0.5187 - val_loss: 0.4964\n",
            "Epoch 4/40\n",
            "8126/8126 [==============================] - 0s 42us/step - loss: 0.4991 - val_loss: 0.4837\n",
            "Epoch 5/40\n",
            "8126/8126 [==============================] - 0s 39us/step - loss: 0.4873 - val_loss: 0.4725\n",
            "Epoch 6/40\n",
            "8126/8126 [==============================] - 0s 39us/step - loss: 0.4789 - val_loss: 0.4657\n",
            "Epoch 7/40\n",
            "8126/8126 [==============================] - 0s 41us/step - loss: 0.4719 - val_loss: 0.4555\n",
            "Epoch 8/40\n",
            "8126/8126 [==============================] - 0s 40us/step - loss: 0.4636 - val_loss: 0.4470\n",
            "Epoch 9/40\n",
            "8126/8126 [==============================] - 0s 38us/step - loss: 0.4571 - val_loss: 0.4395\n",
            "Epoch 10/40\n",
            "8126/8126 [==============================] - 0s 41us/step - loss: 0.4514 - val_loss: 0.4338\n",
            "Epoch 11/40\n",
            "8126/8126 [==============================] - 0s 39us/step - loss: 0.4440 - val_loss: 0.4268\n",
            "Epoch 12/40\n",
            "8126/8126 [==============================] - 0s 38us/step - loss: 0.4366 - val_loss: 0.4229\n",
            "Epoch 13/40\n",
            "8126/8126 [==============================] - 0s 39us/step - loss: 0.4317 - val_loss: 0.4149\n",
            "Epoch 14/40\n",
            "8126/8126 [==============================] - 0s 39us/step - loss: 0.4256 - val_loss: 0.4098\n",
            "Epoch 15/40\n",
            "8126/8126 [==============================] - 0s 38us/step - loss: 0.4207 - val_loss: 0.4092\n",
            "Epoch 16/40\n",
            "8126/8126 [==============================] - 0s 38us/step - loss: 0.4177 - val_loss: 0.4013\n",
            "Epoch 17/40\n",
            "8126/8126 [==============================] - 0s 46us/step - loss: 0.4141 - val_loss: 0.4003\n",
            "Epoch 18/40\n",
            "8126/8126 [==============================] - 0s 42us/step - loss: 0.4101 - val_loss: 0.3977\n",
            "Epoch 19/40\n",
            "8126/8126 [==============================] - 0s 45us/step - loss: 0.4070 - val_loss: 0.3991\n",
            "Epoch 20/40\n",
            "8126/8126 [==============================] - 0s 43us/step - loss: 0.4050 - val_loss: 0.3957\n",
            "Epoch 21/40\n",
            "8126/8126 [==============================] - 0s 43us/step - loss: 0.4027 - val_loss: 0.3901\n",
            "Epoch 22/40\n",
            "8126/8126 [==============================] - 0s 44us/step - loss: 0.4000 - val_loss: 0.3915\n",
            "Epoch 23/40\n",
            "8126/8126 [==============================] - 0s 43us/step - loss: 0.3976 - val_loss: 0.3873\n",
            "Epoch 24/40\n",
            "8126/8126 [==============================] - 0s 40us/step - loss: 0.3950 - val_loss: 0.3816\n",
            "Epoch 25/40\n",
            "8126/8126 [==============================] - 0s 45us/step - loss: 0.3935 - val_loss: 0.3842\n",
            "Epoch 26/40\n",
            "8126/8126 [==============================] - 0s 42us/step - loss: 0.3913 - val_loss: 0.3764\n",
            "Epoch 27/40\n",
            "8126/8126 [==============================] - 0s 43us/step - loss: 0.3898 - val_loss: 0.3760\n",
            "Epoch 28/40\n",
            "8126/8126 [==============================] - 0s 42us/step - loss: 0.3878 - val_loss: 0.3757\n",
            "Epoch 29/40\n",
            "8126/8126 [==============================] - 0s 44us/step - loss: 0.3847 - val_loss: 0.3778\n",
            "Epoch 30/40\n",
            "8126/8126 [==============================] - 0s 43us/step - loss: 0.3832 - val_loss: 0.3704\n",
            "Epoch 31/40\n",
            "8126/8126 [==============================] - 0s 44us/step - loss: 0.3817 - val_loss: 0.3701\n",
            "Epoch 32/40\n",
            "8126/8126 [==============================] - 0s 43us/step - loss: 0.3794 - val_loss: 0.3734\n",
            "Epoch 33/40\n",
            "8126/8126 [==============================] - 0s 44us/step - loss: 0.3798 - val_loss: 0.3669\n",
            "Epoch 34/40\n",
            "8126/8126 [==============================] - 0s 45us/step - loss: 0.3761 - val_loss: 0.3665\n",
            "Epoch 35/40\n",
            "8126/8126 [==============================] - 0s 42us/step - loss: 0.3758 - val_loss: 0.3640\n",
            "Epoch 36/40\n",
            "8126/8126 [==============================] - 0s 40us/step - loss: 0.3757 - val_loss: 0.3648\n",
            "Epoch 37/40\n",
            "8126/8126 [==============================] - 0s 46us/step - loss: 0.3729 - val_loss: 0.3629\n",
            "Epoch 38/40\n",
            "8126/8126 [==============================] - 0s 43us/step - loss: 0.3727 - val_loss: 0.3629\n",
            "Epoch 39/40\n",
            "8126/8126 [==============================] - 0s 40us/step - loss: 0.3705 - val_loss: 0.3604\n",
            "Epoch 40/40\n",
            "8126/8126 [==============================] - 0s 43us/step - loss: 0.3705 - val_loss: 0.3606\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 97%|█████████▋| 35/36 [06:59<00:13, 13.25s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "{'activation1': <function elu at 0x7f4d33558e18>, 'first_hidden_layer': 8, 'optimizer': 'adam', 'second_hidden_layer': 8}\n",
            "Train on 8126 samples, validate on 3870 samples\n",
            "Epoch 1/40\n",
            "8126/8126 [==============================] - 0s 54us/step - loss: 3.2998 - val_loss: 1.2507\n",
            "Epoch 2/40\n",
            "8126/8126 [==============================] - 0s 40us/step - loss: 0.8618 - val_loss: 0.6910\n",
            "Epoch 3/40\n",
            "8126/8126 [==============================] - 0s 42us/step - loss: 0.6447 - val_loss: 0.5810\n",
            "Epoch 4/40\n",
            "8126/8126 [==============================] - 0s 43us/step - loss: 0.5574 - val_loss: 0.5231\n",
            "Epoch 5/40\n",
            "8126/8126 [==============================] - 0s 41us/step - loss: 0.5155 - val_loss: 0.4947\n",
            "Epoch 6/40\n",
            "8126/8126 [==============================] - 0s 41us/step - loss: 0.4907 - val_loss: 0.4764\n",
            "Epoch 7/40\n",
            "8126/8126 [==============================] - 0s 44us/step - loss: 0.4757 - val_loss: 0.4598\n",
            "Epoch 8/40\n",
            "8126/8126 [==============================] - 0s 41us/step - loss: 0.4638 - val_loss: 0.4510\n",
            "Epoch 9/40\n",
            "8126/8126 [==============================] - 0s 42us/step - loss: 0.4546 - val_loss: 0.4437\n",
            "Epoch 10/40\n",
            "8126/8126 [==============================] - 0s 42us/step - loss: 0.4450 - val_loss: 0.4371\n",
            "Epoch 11/40\n",
            "8126/8126 [==============================] - 0s 42us/step - loss: 0.4395 - val_loss: 0.4299\n",
            "Epoch 12/40\n",
            "8126/8126 [==============================] - 0s 41us/step - loss: 0.4323 - val_loss: 0.4313\n",
            "Epoch 13/40\n",
            "8126/8126 [==============================] - 0s 43us/step - loss: 0.4293 - val_loss: 0.4193\n",
            "Epoch 14/40\n",
            "8126/8126 [==============================] - 0s 42us/step - loss: 0.4261 - val_loss: 0.4157\n",
            "Epoch 15/40\n",
            "8126/8126 [==============================] - 0s 40us/step - loss: 0.4220 - val_loss: 0.4098\n",
            "Epoch 16/40\n",
            "8126/8126 [==============================] - 0s 44us/step - loss: 0.4180 - val_loss: 0.4062\n",
            "Epoch 17/40\n",
            "8126/8126 [==============================] - 0s 42us/step - loss: 0.4168 - val_loss: 0.4038\n",
            "Epoch 18/40\n",
            "8126/8126 [==============================] - 0s 42us/step - loss: 0.4123 - val_loss: 0.4040\n",
            "Epoch 19/40\n",
            "8126/8126 [==============================] - 0s 42us/step - loss: 0.4101 - val_loss: 0.4006\n",
            "Epoch 20/40\n",
            "8126/8126 [==============================] - 0s 42us/step - loss: 0.4077 - val_loss: 0.4029\n",
            "Epoch 21/40\n",
            "8126/8126 [==============================] - 0s 39us/step - loss: 0.4055 - val_loss: 0.3978\n",
            "Epoch 22/40\n",
            "8126/8126 [==============================] - 0s 41us/step - loss: 0.4027 - val_loss: 0.3979\n",
            "Epoch 23/40\n",
            "8126/8126 [==============================] - 0s 43us/step - loss: 0.4014 - val_loss: 0.3903\n",
            "Epoch 24/40\n",
            "8126/8126 [==============================] - 0s 43us/step - loss: 0.4003 - val_loss: 0.3887\n",
            "Epoch 25/40\n",
            "8126/8126 [==============================] - 0s 45us/step - loss: 0.3964 - val_loss: 0.3894\n",
            "Epoch 26/40\n",
            "8126/8126 [==============================] - 0s 41us/step - loss: 0.3943 - val_loss: 0.3883\n",
            "Epoch 27/40\n",
            "8126/8126 [==============================] - 0s 42us/step - loss: 0.3930 - val_loss: 0.3883\n",
            "Epoch 28/40\n",
            "8126/8126 [==============================] - 0s 44us/step - loss: 0.3917 - val_loss: 0.3823\n",
            "Epoch 29/40\n",
            "8126/8126 [==============================] - 0s 42us/step - loss: 0.3898 - val_loss: 0.3846\n",
            "Epoch 30/40\n",
            "8126/8126 [==============================] - 0s 42us/step - loss: 0.3873 - val_loss: 0.3802\n",
            "Epoch 31/40\n",
            "8126/8126 [==============================] - 0s 44us/step - loss: 0.3876 - val_loss: 0.3795\n",
            "Epoch 32/40\n",
            "8126/8126 [==============================] - 0s 43us/step - loss: 0.3842 - val_loss: 0.3773\n",
            "Epoch 33/40\n",
            "8126/8126 [==============================] - 0s 44us/step - loss: 0.3838 - val_loss: 0.3776\n",
            "Epoch 34/40\n",
            "8126/8126 [==============================] - 0s 42us/step - loss: 0.3860 - val_loss: 0.3793\n",
            "Epoch 35/40\n",
            "8126/8126 [==============================] - 0s 43us/step - loss: 0.3834 - val_loss: 0.3751\n",
            "Epoch 36/40\n",
            "8126/8126 [==============================] - 0s 43us/step - loss: 0.3801 - val_loss: 0.3744\n",
            "Epoch 37/40\n",
            "8126/8126 [==============================] - 0s 42us/step - loss: 0.3777 - val_loss: 0.3759\n",
            "Epoch 38/40\n",
            "8126/8126 [==============================] - 0s 44us/step - loss: 0.3758 - val_loss: 0.3712\n",
            "Epoch 39/40\n",
            "8126/8126 [==============================] - 0s 43us/step - loss: 0.3741 - val_loss: 0.3743\n",
            "Epoch 40/40\n",
            "8126/8126 [==============================] - 0s 43us/step - loss: 0.3733 - val_loss: 0.3708\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 36/36 [07:14<00:00, 12.06s/it]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "M3jGw4cDiZG5",
        "colab": {}
      },
      "source": [
        "from talos import Evaluate\n",
        "\n",
        "e=Evaluate(h)\n",
        "evaluation = e.evaluate(x_test, \n",
        "                        y_test,\n",
        "                        folds=5,\n",
        "                        metric='val_loss',\n",
        "                        task='continuous')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "04y4pfTxmc5D",
        "outputId": "ff8049e0-50cb-4fc0-c704-3e6e07fcb69e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "#this will save the data, weights and the model in a zip file\n",
        "from talos import Deploy\n",
        "m = Deploy(h, \"model_11\", metric=\"val_loss\", asc=True)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Deploy package model_11 have been saved.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "-vRBUdayDBxS",
        "colab": {}
      },
      "source": [
        "# we can restore the model form the zip file above using this command\n",
        "from talos import Restore\n",
        "\n",
        "m = Restore('model_11.zip')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "D37WFJQrQXYR",
        "outputId": "d555f274-7fa4-4de2-e7fd-cda1ae173df3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "m"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<talos.commands.restore.Restore at 0x7f4d314b19b0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "pTI290lFQYuX",
        "outputId": "04689d0f-f5bc-4f7a-9e1d-84ae23a14e4d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 386
        }
      },
      "source": [
        "m.details"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>experiment_name</td>\n",
              "      <td>e1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>random_method</td>\n",
              "      <td>uniform_mersenne</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>reduction_method</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>reduction_interval</td>\n",
              "      <td>50</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>reduction_window</td>\n",
              "      <td>20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>reduction_threshold</td>\n",
              "      <td>0.2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>reduction_metric</td>\n",
              "      <td>val_loss</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>complete_time</td>\n",
              "      <td>06/15/20/17:34</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>x_shape</td>\n",
              "      <td>(11610, 8)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>y_shape</td>\n",
              "      <td>(11610,)</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                      0                 1\n",
              "0                   NaN                 0\n",
              "1       experiment_name                e1\n",
              "2         random_method  uniform_mersenne\n",
              "3      reduction_method               NaN\n",
              "4    reduction_interval                50\n",
              "5      reduction_window                20\n",
              "6   reduction_threshold               0.2\n",
              "7      reduction_metric          val_loss\n",
              "8         complete_time    06/15/20/17:34\n",
              "9               x_shape        (11610, 8)\n",
              "10              y_shape          (11610,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ujE8ScsNQfX-",
        "outputId": "bf7b69a1-e003-414e-bd13-201647fcd82e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "m.results"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>round_epochs</th>\n",
              "      <th>val_loss</th>\n",
              "      <th>loss</th>\n",
              "      <th>activation1</th>\n",
              "      <th>first_hidden_layer</th>\n",
              "      <th>optimizer</th>\n",
              "      <th>second_hidden_layer</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>40</td>\n",
              "      <td>0.338181</td>\n",
              "      <td>0.307128</td>\n",
              "      <td>&lt;function relu at 0x7f4d33213598&gt;</td>\n",
              "      <td>30</td>\n",
              "      <td>sgd</td>\n",
              "      <td>16</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>40</td>\n",
              "      <td>0.322961</td>\n",
              "      <td>0.311235</td>\n",
              "      <td>&lt;function relu at 0x7f4d33213598&gt;</td>\n",
              "      <td>30</td>\n",
              "      <td>sgd</td>\n",
              "      <td>12</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>40</td>\n",
              "      <td>0.321220</td>\n",
              "      <td>0.314951</td>\n",
              "      <td>&lt;function relu at 0x7f4d33213598&gt;</td>\n",
              "      <td>30</td>\n",
              "      <td>sgd</td>\n",
              "      <td>8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>40</td>\n",
              "      <td>0.309001</td>\n",
              "      <td>0.285462</td>\n",
              "      <td>&lt;function relu at 0x7f4d33213598&gt;</td>\n",
              "      <td>30</td>\n",
              "      <td>adam</td>\n",
              "      <td>16</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>40</td>\n",
              "      <td>0.325237</td>\n",
              "      <td>0.300794</td>\n",
              "      <td>&lt;function relu at 0x7f4d33213598&gt;</td>\n",
              "      <td>30</td>\n",
              "      <td>adam</td>\n",
              "      <td>12</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>40</td>\n",
              "      <td>0.306926</td>\n",
              "      <td>0.331452</td>\n",
              "      <td>&lt;function relu at 0x7f4d33213598&gt;</td>\n",
              "      <td>30</td>\n",
              "      <td>adam</td>\n",
              "      <td>8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>40</td>\n",
              "      <td>0.345412</td>\n",
              "      <td>0.341193</td>\n",
              "      <td>&lt;function relu at 0x7f4d33213598&gt;</td>\n",
              "      <td>16</td>\n",
              "      <td>sgd</td>\n",
              "      <td>16</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>40</td>\n",
              "      <td>0.341044</td>\n",
              "      <td>0.337599</td>\n",
              "      <td>&lt;function relu at 0x7f4d33213598&gt;</td>\n",
              "      <td>16</td>\n",
              "      <td>sgd</td>\n",
              "      <td>12</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>40</td>\n",
              "      <td>0.335905</td>\n",
              "      <td>0.335150</td>\n",
              "      <td>&lt;function relu at 0x7f4d33213598&gt;</td>\n",
              "      <td>16</td>\n",
              "      <td>sgd</td>\n",
              "      <td>8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>40</td>\n",
              "      <td>0.308876</td>\n",
              "      <td>0.302960</td>\n",
              "      <td>&lt;function relu at 0x7f4d33213598&gt;</td>\n",
              "      <td>16</td>\n",
              "      <td>adam</td>\n",
              "      <td>16</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>40</td>\n",
              "      <td>0.322732</td>\n",
              "      <td>0.322188</td>\n",
              "      <td>&lt;function relu at 0x7f4d33213598&gt;</td>\n",
              "      <td>16</td>\n",
              "      <td>adam</td>\n",
              "      <td>12</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>40</td>\n",
              "      <td>0.319617</td>\n",
              "      <td>0.314642</td>\n",
              "      <td>&lt;function relu at 0x7f4d33213598&gt;</td>\n",
              "      <td>16</td>\n",
              "      <td>adam</td>\n",
              "      <td>8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>40</td>\n",
              "      <td>0.374070</td>\n",
              "      <td>0.370865</td>\n",
              "      <td>&lt;function relu at 0x7f4d33213598&gt;</td>\n",
              "      <td>8</td>\n",
              "      <td>sgd</td>\n",
              "      <td>16</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>40</td>\n",
              "      <td>0.346464</td>\n",
              "      <td>0.343892</td>\n",
              "      <td>&lt;function relu at 0x7f4d33213598&gt;</td>\n",
              "      <td>8</td>\n",
              "      <td>sgd</td>\n",
              "      <td>12</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>40</td>\n",
              "      <td>0.364906</td>\n",
              "      <td>0.358779</td>\n",
              "      <td>&lt;function relu at 0x7f4d33213598&gt;</td>\n",
              "      <td>8</td>\n",
              "      <td>sgd</td>\n",
              "      <td>8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>40</td>\n",
              "      <td>0.327299</td>\n",
              "      <td>0.333033</td>\n",
              "      <td>&lt;function relu at 0x7f4d33213598&gt;</td>\n",
              "      <td>8</td>\n",
              "      <td>adam</td>\n",
              "      <td>16</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>40</td>\n",
              "      <td>0.363958</td>\n",
              "      <td>0.363132</td>\n",
              "      <td>&lt;function relu at 0x7f4d33213598&gt;</td>\n",
              "      <td>8</td>\n",
              "      <td>adam</td>\n",
              "      <td>12</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>40</td>\n",
              "      <td>0.335401</td>\n",
              "      <td>0.336758</td>\n",
              "      <td>&lt;function relu at 0x7f4d33213598&gt;</td>\n",
              "      <td>8</td>\n",
              "      <td>adam</td>\n",
              "      <td>8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>40</td>\n",
              "      <td>0.377218</td>\n",
              "      <td>0.373521</td>\n",
              "      <td>&lt;function elu at 0x7f4d33558e18&gt;</td>\n",
              "      <td>30</td>\n",
              "      <td>sgd</td>\n",
              "      <td>16</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>40</td>\n",
              "      <td>0.370792</td>\n",
              "      <td>0.366380</td>\n",
              "      <td>&lt;function elu at 0x7f4d33558e18&gt;</td>\n",
              "      <td>30</td>\n",
              "      <td>sgd</td>\n",
              "      <td>12</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>40</td>\n",
              "      <td>0.363478</td>\n",
              "      <td>0.368450</td>\n",
              "      <td>&lt;function elu at 0x7f4d33558e18&gt;</td>\n",
              "      <td>30</td>\n",
              "      <td>sgd</td>\n",
              "      <td>8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>40</td>\n",
              "      <td>0.328598</td>\n",
              "      <td>0.333971</td>\n",
              "      <td>&lt;function elu at 0x7f4d33558e18&gt;</td>\n",
              "      <td>30</td>\n",
              "      <td>adam</td>\n",
              "      <td>16</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>40</td>\n",
              "      <td>0.341764</td>\n",
              "      <td>0.341290</td>\n",
              "      <td>&lt;function elu at 0x7f4d33558e18&gt;</td>\n",
              "      <td>30</td>\n",
              "      <td>adam</td>\n",
              "      <td>12</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>40</td>\n",
              "      <td>0.359580</td>\n",
              "      <td>0.355503</td>\n",
              "      <td>&lt;function elu at 0x7f4d33558e18&gt;</td>\n",
              "      <td>30</td>\n",
              "      <td>adam</td>\n",
              "      <td>8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>40</td>\n",
              "      <td>0.388120</td>\n",
              "      <td>0.387923</td>\n",
              "      <td>&lt;function elu at 0x7f4d33558e18&gt;</td>\n",
              "      <td>16</td>\n",
              "      <td>sgd</td>\n",
              "      <td>16</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>40</td>\n",
              "      <td>0.364671</td>\n",
              "      <td>0.369400</td>\n",
              "      <td>&lt;function elu at 0x7f4d33558e18&gt;</td>\n",
              "      <td>16</td>\n",
              "      <td>sgd</td>\n",
              "      <td>12</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>40</td>\n",
              "      <td>0.351962</td>\n",
              "      <td>0.357402</td>\n",
              "      <td>&lt;function elu at 0x7f4d33558e18&gt;</td>\n",
              "      <td>16</td>\n",
              "      <td>sgd</td>\n",
              "      <td>8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>40</td>\n",
              "      <td>0.347051</td>\n",
              "      <td>0.353555</td>\n",
              "      <td>&lt;function elu at 0x7f4d33558e18&gt;</td>\n",
              "      <td>16</td>\n",
              "      <td>adam</td>\n",
              "      <td>16</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>40</td>\n",
              "      <td>0.345934</td>\n",
              "      <td>0.347887</td>\n",
              "      <td>&lt;function elu at 0x7f4d33558e18&gt;</td>\n",
              "      <td>16</td>\n",
              "      <td>adam</td>\n",
              "      <td>12</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>40</td>\n",
              "      <td>0.347707</td>\n",
              "      <td>0.357856</td>\n",
              "      <td>&lt;function elu at 0x7f4d33558e18&gt;</td>\n",
              "      <td>16</td>\n",
              "      <td>adam</td>\n",
              "      <td>8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>40</td>\n",
              "      <td>0.383323</td>\n",
              "      <td>0.393347</td>\n",
              "      <td>&lt;function elu at 0x7f4d33558e18&gt;</td>\n",
              "      <td>8</td>\n",
              "      <td>sgd</td>\n",
              "      <td>16</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>40</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>&lt;function elu at 0x7f4d33558e18&gt;</td>\n",
              "      <td>8</td>\n",
              "      <td>sgd</td>\n",
              "      <td>12</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>40</td>\n",
              "      <td>0.380103</td>\n",
              "      <td>0.379838</td>\n",
              "      <td>&lt;function elu at 0x7f4d33558e18&gt;</td>\n",
              "      <td>8</td>\n",
              "      <td>sgd</td>\n",
              "      <td>8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>40</td>\n",
              "      <td>0.360746</td>\n",
              "      <td>0.362357</td>\n",
              "      <td>&lt;function elu at 0x7f4d33558e18&gt;</td>\n",
              "      <td>8</td>\n",
              "      <td>adam</td>\n",
              "      <td>16</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>40</td>\n",
              "      <td>0.360552</td>\n",
              "      <td>0.370501</td>\n",
              "      <td>&lt;function elu at 0x7f4d33558e18&gt;</td>\n",
              "      <td>8</td>\n",
              "      <td>adam</td>\n",
              "      <td>12</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>40</td>\n",
              "      <td>0.370765</td>\n",
              "      <td>0.373322</td>\n",
              "      <td>&lt;function elu at 0x7f4d33558e18&gt;</td>\n",
              "      <td>8</td>\n",
              "      <td>adam</td>\n",
              "      <td>8</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    round_epochs  val_loss  ...  optimizer second_hidden_layer\n",
              "0             40  0.338181  ...        sgd                  16\n",
              "1             40  0.322961  ...        sgd                  12\n",
              "2             40  0.321220  ...        sgd                   8\n",
              "3             40  0.309001  ...       adam                  16\n",
              "4             40  0.325237  ...       adam                  12\n",
              "5             40  0.306926  ...       adam                   8\n",
              "6             40  0.345412  ...        sgd                  16\n",
              "7             40  0.341044  ...        sgd                  12\n",
              "8             40  0.335905  ...        sgd                   8\n",
              "9             40  0.308876  ...       adam                  16\n",
              "10            40  0.322732  ...       adam                  12\n",
              "11            40  0.319617  ...       adam                   8\n",
              "12            40  0.374070  ...        sgd                  16\n",
              "13            40  0.346464  ...        sgd                  12\n",
              "14            40  0.364906  ...        sgd                   8\n",
              "15            40  0.327299  ...       adam                  16\n",
              "16            40  0.363958  ...       adam                  12\n",
              "17            40  0.335401  ...       adam                   8\n",
              "18            40  0.377218  ...        sgd                  16\n",
              "19            40  0.370792  ...        sgd                  12\n",
              "20            40  0.363478  ...        sgd                   8\n",
              "21            40  0.328598  ...       adam                  16\n",
              "22            40  0.341764  ...       adam                  12\n",
              "23            40  0.359580  ...       adam                   8\n",
              "24            40  0.388120  ...        sgd                  16\n",
              "25            40  0.364671  ...        sgd                  12\n",
              "26            40  0.351962  ...        sgd                   8\n",
              "27            40  0.347051  ...       adam                  16\n",
              "28            40  0.345934  ...       adam                  12\n",
              "29            40  0.347707  ...       adam                   8\n",
              "30            40  0.383323  ...        sgd                  16\n",
              "31            40       NaN  ...        sgd                  12\n",
              "32            40  0.380103  ...        sgd                   8\n",
              "33            40  0.360746  ...       adam                  16\n",
              "34            40  0.360552  ...       adam                  12\n",
              "35            40  0.370765  ...       adam                   8\n",
              "\n",
              "[36 rows x 7 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "w_DYqwKkQn1H",
        "outputId": "f1569c02-6de1-49e1-b71b-84a49fed9492",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        }
      },
      "source": [
        "m.params"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'activation1': [<function tensorflow.python.keras.activations.relu>,\n",
              "  <function tensorflow.python.keras.activations.elu>],\n",
              " 'first_hidden_layer': [30, 16, 8],\n",
              " 'optimizer': ['sgd', 'adam'],\n",
              " 'second_hidden_layer': [16, 12, 8]}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "f6WfH9Reaqf5",
        "colab": {}
      },
      "source": [
        "r = talos.Reporting(h)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "0RtEfyjUbbaW",
        "outputId": "40127d67-8b74-467c-d51f-e36b1a3cd530",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "r.data.sort_values('val_loss')"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>round_epochs</th>\n",
              "      <th>val_loss</th>\n",
              "      <th>loss</th>\n",
              "      <th>activation1</th>\n",
              "      <th>first_hidden_layer</th>\n",
              "      <th>optimizer</th>\n",
              "      <th>second_hidden_layer</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>40</td>\n",
              "      <td>0.306926</td>\n",
              "      <td>0.331452</td>\n",
              "      <td>&lt;function relu at 0x7f4d33213598&gt;</td>\n",
              "      <td>30</td>\n",
              "      <td>adam</td>\n",
              "      <td>8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>40</td>\n",
              "      <td>0.308876</td>\n",
              "      <td>0.302960</td>\n",
              "      <td>&lt;function relu at 0x7f4d33213598&gt;</td>\n",
              "      <td>16</td>\n",
              "      <td>adam</td>\n",
              "      <td>16</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>40</td>\n",
              "      <td>0.309001</td>\n",
              "      <td>0.285462</td>\n",
              "      <td>&lt;function relu at 0x7f4d33213598&gt;</td>\n",
              "      <td>30</td>\n",
              "      <td>adam</td>\n",
              "      <td>16</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>40</td>\n",
              "      <td>0.319617</td>\n",
              "      <td>0.314642</td>\n",
              "      <td>&lt;function relu at 0x7f4d33213598&gt;</td>\n",
              "      <td>16</td>\n",
              "      <td>adam</td>\n",
              "      <td>8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>40</td>\n",
              "      <td>0.321220</td>\n",
              "      <td>0.314951</td>\n",
              "      <td>&lt;function relu at 0x7f4d33213598&gt;</td>\n",
              "      <td>30</td>\n",
              "      <td>sgd</td>\n",
              "      <td>8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>40</td>\n",
              "      <td>0.322732</td>\n",
              "      <td>0.322188</td>\n",
              "      <td>&lt;function relu at 0x7f4d33213598&gt;</td>\n",
              "      <td>16</td>\n",
              "      <td>adam</td>\n",
              "      <td>12</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>40</td>\n",
              "      <td>0.322961</td>\n",
              "      <td>0.311235</td>\n",
              "      <td>&lt;function relu at 0x7f4d33213598&gt;</td>\n",
              "      <td>30</td>\n",
              "      <td>sgd</td>\n",
              "      <td>12</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>40</td>\n",
              "      <td>0.325237</td>\n",
              "      <td>0.300794</td>\n",
              "      <td>&lt;function relu at 0x7f4d33213598&gt;</td>\n",
              "      <td>30</td>\n",
              "      <td>adam</td>\n",
              "      <td>12</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>40</td>\n",
              "      <td>0.327299</td>\n",
              "      <td>0.333033</td>\n",
              "      <td>&lt;function relu at 0x7f4d33213598&gt;</td>\n",
              "      <td>8</td>\n",
              "      <td>adam</td>\n",
              "      <td>16</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>40</td>\n",
              "      <td>0.328598</td>\n",
              "      <td>0.333971</td>\n",
              "      <td>&lt;function elu at 0x7f4d33558e18&gt;</td>\n",
              "      <td>30</td>\n",
              "      <td>adam</td>\n",
              "      <td>16</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>40</td>\n",
              "      <td>0.335401</td>\n",
              "      <td>0.336758</td>\n",
              "      <td>&lt;function relu at 0x7f4d33213598&gt;</td>\n",
              "      <td>8</td>\n",
              "      <td>adam</td>\n",
              "      <td>8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>40</td>\n",
              "      <td>0.335905</td>\n",
              "      <td>0.335150</td>\n",
              "      <td>&lt;function relu at 0x7f4d33213598&gt;</td>\n",
              "      <td>16</td>\n",
              "      <td>sgd</td>\n",
              "      <td>8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>40</td>\n",
              "      <td>0.338181</td>\n",
              "      <td>0.307128</td>\n",
              "      <td>&lt;function relu at 0x7f4d33213598&gt;</td>\n",
              "      <td>30</td>\n",
              "      <td>sgd</td>\n",
              "      <td>16</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>40</td>\n",
              "      <td>0.341044</td>\n",
              "      <td>0.337599</td>\n",
              "      <td>&lt;function relu at 0x7f4d33213598&gt;</td>\n",
              "      <td>16</td>\n",
              "      <td>sgd</td>\n",
              "      <td>12</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>40</td>\n",
              "      <td>0.341764</td>\n",
              "      <td>0.341290</td>\n",
              "      <td>&lt;function elu at 0x7f4d33558e18&gt;</td>\n",
              "      <td>30</td>\n",
              "      <td>adam</td>\n",
              "      <td>12</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>40</td>\n",
              "      <td>0.345412</td>\n",
              "      <td>0.341193</td>\n",
              "      <td>&lt;function relu at 0x7f4d33213598&gt;</td>\n",
              "      <td>16</td>\n",
              "      <td>sgd</td>\n",
              "      <td>16</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>40</td>\n",
              "      <td>0.345934</td>\n",
              "      <td>0.347887</td>\n",
              "      <td>&lt;function elu at 0x7f4d33558e18&gt;</td>\n",
              "      <td>16</td>\n",
              "      <td>adam</td>\n",
              "      <td>12</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>40</td>\n",
              "      <td>0.346464</td>\n",
              "      <td>0.343892</td>\n",
              "      <td>&lt;function relu at 0x7f4d33213598&gt;</td>\n",
              "      <td>8</td>\n",
              "      <td>sgd</td>\n",
              "      <td>12</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>40</td>\n",
              "      <td>0.347051</td>\n",
              "      <td>0.353555</td>\n",
              "      <td>&lt;function elu at 0x7f4d33558e18&gt;</td>\n",
              "      <td>16</td>\n",
              "      <td>adam</td>\n",
              "      <td>16</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>40</td>\n",
              "      <td>0.347707</td>\n",
              "      <td>0.357856</td>\n",
              "      <td>&lt;function elu at 0x7f4d33558e18&gt;</td>\n",
              "      <td>16</td>\n",
              "      <td>adam</td>\n",
              "      <td>8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>40</td>\n",
              "      <td>0.351962</td>\n",
              "      <td>0.357402</td>\n",
              "      <td>&lt;function elu at 0x7f4d33558e18&gt;</td>\n",
              "      <td>16</td>\n",
              "      <td>sgd</td>\n",
              "      <td>8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>40</td>\n",
              "      <td>0.359580</td>\n",
              "      <td>0.355503</td>\n",
              "      <td>&lt;function elu at 0x7f4d33558e18&gt;</td>\n",
              "      <td>30</td>\n",
              "      <td>adam</td>\n",
              "      <td>8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>40</td>\n",
              "      <td>0.360552</td>\n",
              "      <td>0.370501</td>\n",
              "      <td>&lt;function elu at 0x7f4d33558e18&gt;</td>\n",
              "      <td>8</td>\n",
              "      <td>adam</td>\n",
              "      <td>12</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>40</td>\n",
              "      <td>0.360746</td>\n",
              "      <td>0.362357</td>\n",
              "      <td>&lt;function elu at 0x7f4d33558e18&gt;</td>\n",
              "      <td>8</td>\n",
              "      <td>adam</td>\n",
              "      <td>16</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>40</td>\n",
              "      <td>0.363478</td>\n",
              "      <td>0.368450</td>\n",
              "      <td>&lt;function elu at 0x7f4d33558e18&gt;</td>\n",
              "      <td>30</td>\n",
              "      <td>sgd</td>\n",
              "      <td>8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>40</td>\n",
              "      <td>0.363958</td>\n",
              "      <td>0.363132</td>\n",
              "      <td>&lt;function relu at 0x7f4d33213598&gt;</td>\n",
              "      <td>8</td>\n",
              "      <td>adam</td>\n",
              "      <td>12</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>40</td>\n",
              "      <td>0.364671</td>\n",
              "      <td>0.369400</td>\n",
              "      <td>&lt;function elu at 0x7f4d33558e18&gt;</td>\n",
              "      <td>16</td>\n",
              "      <td>sgd</td>\n",
              "      <td>12</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>40</td>\n",
              "      <td>0.364906</td>\n",
              "      <td>0.358779</td>\n",
              "      <td>&lt;function relu at 0x7f4d33213598&gt;</td>\n",
              "      <td>8</td>\n",
              "      <td>sgd</td>\n",
              "      <td>8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>40</td>\n",
              "      <td>0.370765</td>\n",
              "      <td>0.373322</td>\n",
              "      <td>&lt;function elu at 0x7f4d33558e18&gt;</td>\n",
              "      <td>8</td>\n",
              "      <td>adam</td>\n",
              "      <td>8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>40</td>\n",
              "      <td>0.370792</td>\n",
              "      <td>0.366380</td>\n",
              "      <td>&lt;function elu at 0x7f4d33558e18&gt;</td>\n",
              "      <td>30</td>\n",
              "      <td>sgd</td>\n",
              "      <td>12</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>40</td>\n",
              "      <td>0.374070</td>\n",
              "      <td>0.370865</td>\n",
              "      <td>&lt;function relu at 0x7f4d33213598&gt;</td>\n",
              "      <td>8</td>\n",
              "      <td>sgd</td>\n",
              "      <td>16</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>40</td>\n",
              "      <td>0.377218</td>\n",
              "      <td>0.373521</td>\n",
              "      <td>&lt;function elu at 0x7f4d33558e18&gt;</td>\n",
              "      <td>30</td>\n",
              "      <td>sgd</td>\n",
              "      <td>16</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>40</td>\n",
              "      <td>0.380103</td>\n",
              "      <td>0.379838</td>\n",
              "      <td>&lt;function elu at 0x7f4d33558e18&gt;</td>\n",
              "      <td>8</td>\n",
              "      <td>sgd</td>\n",
              "      <td>8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>40</td>\n",
              "      <td>0.383323</td>\n",
              "      <td>0.393347</td>\n",
              "      <td>&lt;function elu at 0x7f4d33558e18&gt;</td>\n",
              "      <td>8</td>\n",
              "      <td>sgd</td>\n",
              "      <td>16</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>40</td>\n",
              "      <td>0.388120</td>\n",
              "      <td>0.387923</td>\n",
              "      <td>&lt;function elu at 0x7f4d33558e18&gt;</td>\n",
              "      <td>16</td>\n",
              "      <td>sgd</td>\n",
              "      <td>16</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>40</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>&lt;function elu at 0x7f4d33558e18&gt;</td>\n",
              "      <td>8</td>\n",
              "      <td>sgd</td>\n",
              "      <td>12</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    round_epochs  val_loss  ...  optimizer second_hidden_layer\n",
              "5             40  0.306926  ...       adam                   8\n",
              "9             40  0.308876  ...       adam                  16\n",
              "3             40  0.309001  ...       adam                  16\n",
              "11            40  0.319617  ...       adam                   8\n",
              "2             40  0.321220  ...        sgd                   8\n",
              "10            40  0.322732  ...       adam                  12\n",
              "1             40  0.322961  ...        sgd                  12\n",
              "4             40  0.325237  ...       adam                  12\n",
              "15            40  0.327299  ...       adam                  16\n",
              "21            40  0.328598  ...       adam                  16\n",
              "17            40  0.335401  ...       adam                   8\n",
              "8             40  0.335905  ...        sgd                   8\n",
              "0             40  0.338181  ...        sgd                  16\n",
              "7             40  0.341044  ...        sgd                  12\n",
              "22            40  0.341764  ...       adam                  12\n",
              "6             40  0.345412  ...        sgd                  16\n",
              "28            40  0.345934  ...       adam                  12\n",
              "13            40  0.346464  ...        sgd                  12\n",
              "27            40  0.347051  ...       adam                  16\n",
              "29            40  0.347707  ...       adam                   8\n",
              "26            40  0.351962  ...        sgd                   8\n",
              "23            40  0.359580  ...       adam                   8\n",
              "34            40  0.360552  ...       adam                  12\n",
              "33            40  0.360746  ...       adam                  16\n",
              "20            40  0.363478  ...        sgd                   8\n",
              "16            40  0.363958  ...       adam                  12\n",
              "25            40  0.364671  ...        sgd                  12\n",
              "14            40  0.364906  ...        sgd                   8\n",
              "35            40  0.370765  ...       adam                   8\n",
              "19            40  0.370792  ...        sgd                  12\n",
              "12            40  0.374070  ...        sgd                  16\n",
              "18            40  0.377218  ...        sgd                  16\n",
              "32            40  0.380103  ...        sgd                   8\n",
              "30            40  0.383323  ...        sgd                  16\n",
              "24            40  0.388120  ...        sgd                  16\n",
              "31            40       NaN  ...        sgd                  12\n",
              "\n",
              "[36 rows x 7 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "-hVQtuwLbgZw",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "1EpZyg3kbrX_",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}